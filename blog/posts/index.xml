<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
    <channel>
        <title>Posts on Kelly Shortridge</title>
        <link>//swagitda.com/blog/posts/</link>
        <description>Recent content in Posts on Kelly Shortridge</description>
        <generator>Hugo -- gohugo.io</generator>
        <language>en-us</language>
        <lastBuildDate>Sun, 05 May 2019 08:30:09 -0400</lastBuildDate>
        <atom:link href="//swagitda.com/blog/posts/index.xml" rel="self" type="application/rss+xml" />
        
        <item>
            <title>Darth Jar Jar: a Model for Infosec Innovation</title>
            <link>//swagitda.com/blog/posts/darth-jar-jar-model-infosec-innovation/</link>
            <pubDate>Sun, 05 May 2019 08:30:09 -0400</pubDate>
            
            <guid>//swagitda.com/blog/posts/darth-jar-jar-model-infosec-innovation/</guid>
            <description>Despite its seeming absurdity and superficiality, the theory of Darth Jar Jar can serve as a poignant parable for security innovation. For those unfamiliar, the notion of a Darth Jar Jar springs from a meticulously researched fan theory from the Star Wars subreddit. While I do not wish to spoil the breathtaking beauty of the fleshed-out theory, the underlying legend is one of an ostensibly bumbling fool who, in reality, is an insidious puppet master full of cunning and strength.</description>
            <content type="html"><![CDATA[

<p><img src="/blog/img/darth-jar-jar.jpg" alt="Image of Darth Jar Jar" /></p>

<p>Despite its seeming absurdity and superficiality, the theory of Darth Jar Jar can serve as a poignant parable for security innovation. For those unfamiliar, the notion of a Darth Jar Jar springs from a <a href="https://www.reddit.com/r/StarWars/comments/3qvj6w/theory_jar_jar_binks_was_a_trained_force_user/">meticulously researched fan theory</a> from the Star Wars subreddit. While I do not wish to spoil the breathtaking beauty of the fleshed-out theory, the underlying legend is one of an ostensibly bumbling fool who, in reality, is an insidious puppet master full of cunning and strength.</p>

<p>What lessons from Darth Jar Jar can we glean and apply to information security? I argue there are a few innovative strategies one can postulate on both the offense and defense sides, which I will explore within this post.</p>

<hr />

<h2 id="darth-jar-jar-for-offense">Darth Jar Jar for Offense</h2>

<p>For an attacker to fully become one with Darth Jar Jar, they must be content playing a fool. Just as Darth Jar Jar appeared to be clumsy to lead to his underestimation, attackers should likewise appear to be sloppy to lead to their underestimation by defenders. By conducting purposefully noisy and messy operations in one part of a system, attackers can direct defenders’ attention away from the attack that is truly transpiring right under their cocksure noses.</p>

<p>There already exist public examples of attackers embracing the spirit of Darth Jar Jar. One of my favorites is the misdirection employed by <a href="https://krebsonsecurity.com/2013/02/ddos-attack-on-bank-hid-900000-cyberheist/">attackers back in 2013</a>, in which they conducted a DDoS to cover their tracks when exfiltrating funds out of corporate accounts. A further three banks were hit by <a href="https://www.cnet.com/news/cybercrooks-use-ddos-attacks-to-mask-theft-of-banks-millions/">the same approach later that year</a> – a low-powered DDoS attack that captured defenders’ attention, while fraudulent money transfers happened concurrently.</p>

<p>Additionally, the Nigerian prince scam is arguably a long-term ploy very much in the vein of Darth Jar Jar. Scammers specifically use poor spelling and grammatical errors to weed out victims who would be less trusting. Those who are not deterred by lacking language are more likely to believe in the tale the scammer is spinning – and thus more willingly depart with their money. The clumsiness is intentional to lure precisely the right victims into the attacker’s maw.</p>

<p>I collected and brainstormed a few additional offense strategies for attackers who seek to replicate the brilliance of Darth Jar Jar:</p>

<h3 id="meesa-so-noisy">Meesa so noisy!</h3>

<p>One option is to flood EDR systems with noise so that alert fatigue sets in on the part of the defender, letting subtler attacks slip through. The goal is to create noise for events that will receive a higher priority flag within the tool, such as purposefully clumsy kernel exploits – no team with a hundred critical-priority alerts (often called &ldquo;P0s,&rdquo; for priority-zero) would clear them in time to catch your sneakier attack.</p>

<p>Even cleverer would be to throw sloppy attacks over a period of time, while inserting the exact same quieter traffic that you plan to leverage in your real attack, so that the machine learning systems begin baselining it in. After all, Darth Jar Jar maintained his foolish façade for more than just one day!</p>

<h3 id="eicar-car-binks">EICAR-CAR Binks</h3>

<p>A common strategy in security detection is the notion of “alert on the first bad thing, then stop processing” so that there is not a flood of alerts. Thusly, an attacker could send EICAR in the same data stream or file as a real attack so that defenders see the EICAR alert and dismiss it. Then, the darker, sneakier underlying attack will go unnoticed – just as Darth Jar Jar used rambling babbling during the entirety of <em>The Phantom Menace</em> to obscure his devious speech that directly led to the dissolution of democracy.</p>

<h3 id="ex-squeeze-the-data-outta-the-network">Ex-squeeze the data outta the network</h3>

<p>During the rescue of Queen Amidala, Darth Jar Jar proved to be a master of diverting the attention of his enemies to take them by surprise. In the realm of critical infrastructure, an attacker could send a large volume of fake location update requests from SIMs, which would certainly attract the attention of defenders at any telecom provider.</p>

<p>While defenders remain distracted and scrambling, the attacker could then infiltrate the internal telecom network by plugging into <a href="https://en.wikipedia.org/wiki/ENodeB">an eNodeB</a> and moving horizontally to compromise elements such as <a href="https://en.wikipedia.org/wiki/System_Architecture_Evolution#MME_(Mobility_Management_Entity)">the MME</a> or even <a href="https://medium.com/@AlepoTech/home-subscriber-server-hss-82470d3f332">the HSS</a> (think: the master user database). Darth Jar Jar would absolutely approve of deflecting defender attention to outside of their own network to conceal the true threat from within.</p>

<hr />

<h2 id="darth-jar-jar-for-defense">Darth Jar Jar for Defense</h2>

<p>Just as attackers must play the fool to follow the wisdom of Darth Jar Jar, so must defenders. Luckily, many defenders are accidentally foolish, making purposeful foolishness all the more likely to lead attackers to underestimate defenders. While I usually strongly advise against #yolosec strategies, pretending to deploy a #yolosec strategy can perfectly replicate the sinister subterfuge of Darth Jar Jar.</p>

<p>There are fewer public examples in the realm of defense that highlight the gloriousness of a Darth Jar Jar approach. The closest might be the use of honeypots or canarytokens, such as a webserver that appears to be configured in a thoroughly #yolosec manner, creating irresistible temptation for attackers to explore them – and thus alert defenders to the fact that someone is very intrigued by their data.</p>

<p>In ths vein, there are a few Darth Jar Jar-esque defense strategies I collected and brainstormed:</p>

<h3 id="oh-no-yousa-connection-is-slow">Oh, no! Yousa connection is slow</h3>

<p>Once you detect attackers within your network, begin throttling their connections to satellite-link speeds at random. The goal is to make it seem like an unstable system to test how the attacker reacts and alters their strategy – just as Darth Jar Jar “clumsily” handled a booma during the Battle of Grassy Plains to take down an armored assault tank.</p>

<h3 id="where-wesa-executing">Where wesa executing?</h3>

<p>Darth Jar Jar never revealed to his Jedi companions that they were succumbing to his duplicity. Likewise, defenders can avoid revealing that they have not only caught attackers, but are bamboozling them.</p>

<p>For instance, when defenders catch an attacker attempting to pwn a production instance, they can migrate that instance out of production, setting up all the same network connectivity so no change is perceived by the attacker. Then, defenders can begin logging and monitoring everything for later learning (and to inform broader investigation) while spinning up a replacement instance in production to avoid downtime.</p>

<h3 id="mmm-dissen-loverly-data">Mmm, dissen loverly data</h3>

<p>Just as Darth Jar Jar tricked the Jedi into travelling through the core of a planet to make himself indispensable, defenders can trick attackers into going down a heavily monitored path to make it inevitable that they will be caught. For example, defenders can sprinkle cleartext AWS credentials in places that lead to alert traps. Or, defenders can “accidentally” reveal credentials attackers would need to access a seemingly sensitive system – when in reality, there is no valuable data residing in the system, just deep monitoring present.</p>

<hr />

<p>Ultimately, no matter which side of the infosec game on which you fight, I hope that you will adopt the following mantra to let the dark side flow through you, elevating your #basic strategy to one of subtle manipulation and insidious deception – “What would Darth Jar Jar do?”</p>

<p><img src="/blog/img/darth-jar-jar.gif" alt="Gif of Darth Jar Jar peeking through Vader's helmet" /></p>
]]></content>
        </item>
        
        <item>
            <title>My Reflections on the 2019 RSA Conference</title>
            <link>//swagitda.com/blog/posts/my-reflections-on-rsac-2019/</link>
            <pubDate>Wed, 13 Mar 2019 21:28:01 -0400</pubDate>
            
            <guid>//swagitda.com/blog/posts/my-reflections-on-rsac-2019/</guid>
            <description>Artist’s rendition of how I felt by Thursday morning of the con
Reflecting on my existential crisis during RSAC, I tried to distill what exactly was so troublesome about the conference. The expo floor was less two separate halls, as per years prior, and more like Mordor, with a befouled sprawl connecting Minas Morgul and the Black Gate — but instead of orcs and Uruk-hai, vendors crammed the hallways that used to serve as open breathing space.</description>
            <content type="html"><![CDATA[

<p><img src="/blog/img/bad-cyberart-12.jpeg" alt="Very cyber-y skull" /><em>Artist’s rendition of how I felt by Thursday morning of the con</em></p>

<p>Reflecting on my existential crisis during RSAC, I tried to distill what exactly was so troublesome about the conference. The expo floor was less two separate halls, as per years prior, and more like Mordor, with a befouled sprawl connecting Minas Morgul and the Black Gate — but instead of orcs and Uruk-hai, vendors crammed the hallways that used to serve as open breathing space.</p>

<p><figure style="float:right; max-width:40%; padding-left: 10px">
    <img src="/blog/img/infosec-buzzword-bingo.jpg" alt="Kelly Shortridge holding their infosec buzzword bingo card">
    <figcaption>The RSAC Innovation Sandbox’s “natural” lighting (featuring me and <a href="/blog/posts/infosec-startup-buzzword-bingo-2019-edition">my buzzword bingo card</a>)<figcaption>
</figure>
The color of RSAC itself is royal purple — fitting as hosts of this ostentatious banquet — with a splash of cyber-turquoise. The vendors left an abstract expressionist mark bursting with oceanic blues, imperial reds, dramatic gunmetal, and the occasional accent of atomic tangerine. Artfully abstract globes, honeycombs, and glowing orbs were juxtaposed with elegant waves and gently curved lines, mixing familiar shapes with the futuristic mouthfeel of “cyber.”</p>

<p>There was a dizzying sense of perpetual motion, like a spinning top just barely staying upright. Quiet was seemingly anathema — the pestilential cacophony of canned speeches and whizbangs and desperate pleas for attention was unavoidable.</p>

<p>In this vainglorious feast by the information security industry in dedication to itself, experiencing an existential crisis is perhaps an inevitability. The ultimate purpose of information security — ensuring an organization can thrive despite digital risks — was obscured beneath the thick layers of cheap pens dimly glinting, XL men’s t-shirts boldly proclaiming trivialities, and the hollow promises of soothing your fears one badge scan at a time.</p>

<p>But this alone can be rationalized away and not lead to a crisis of faith in the importance of our collective work. These criticisms are true for most trade shows; they are inherently loud and sales-y. What makes the RSA Conference burrow into the brain like a parasite greedily devouring all hope is that the diabolical song and dance is not helping beneficial solutions fall into the right hands.</p>

<p>RSAC is largely successful due to network effects — because everyone seemingly attends, everyone else attends — making it a good place to catch up with a lot of people at once. This reality makes its issues even more problematic; the four that personally drove my distress are:</p>

<ol>
<li><a href="#fud">Hyperbolization of FUD</a></li>
<li><a href="#disconnect">Disconnect between products &amp; personas</a></li>
<li><a href="#personas">Misunderstanding of personas</a></li>
<li><a href="#blocker">Promotion of security as a blocker vs. a compromiser</a></li>
</ol>

<hr />

<h2 id="a-name-fud-a-hyperbolization-of-fud"><a name="fud"></a>Hyperbolization of FUD</h2>

<p>In a banquet of distasteful and inane catch phrases and bullet points, one gave me acid reflux like no other—it exemplifies what I am calling the hyperbolization of fear, uncertainty, and doubt (“FUD”). A large EDR vendor’s advertisements around town roughly stated, <em>“It takes a lifetime to build your career, and 5 seconds to lose it.”</em></p>

<p>There was no shortage of FUD around threats, but this tagline directly stokes the fear that someone’s professional life will be over if they let an attacker slip by. It’s tasteless and mostly incorrect, and I find it appalling to imply that someone’s life will be ruined if they don’t buy your product. I also find it lazy; if you can’t sell your product without scaring people to such a degree, perhaps you should make your product inherently matter more to them.</p>

<p>Part of the issue might be that the industry is generally poor at realistic threat modelling, making these doom and gloom buzzphrases compelling. While it’s too much to hope that all vendors could help their customers threat model responsibly, I believe it’s a reasonable expectation to not aim directly at personal fears and not make up scenarios that lack precedence in history or logic. If a vendor must create dystopian sci-fi to justify use of their product, they are doing it all wrong.</p>

<h2 id="a-name-disconnect-a-disconnect-between-products-personas"><a name="disconnect"></a>Disconnect between products &amp; personas</h2>

<p>My first supposition with regards to the persona problem was a disconnect <a href="https://blog.hubspot.com/marketing/buyer-persona-definition-under-100-sr">between buyer</a> and <a href="https://blog.hubspot.com/marketing/buyer-persona-definition-under-100-sr">user personas</a>. Specifically, that narratives around usability seemed to be keeping the buyer (CISO) in mind, rather than the people who would actually use it (individual contributors (“ICs”). Upon reflection, I think it’s worse than this — that there is a disconnect between the buyer personas and the vendors’ target audience.</p>

<p>By this I mean that the marketing tactics, highlighted characteristics, even the words used all feel targeted towards other sales and marketing professionals, or perhaps venture capitalists (“VCs”). During the conference, I spoke with a large swath of practitioners ranging from CISOs to managers to senior ICs to more junior ICs, and none of them found the booths enlightening or particularly worthy of their attention.</p>

<p>This begets the question — for whom is all this marketing? My prior assumption was that vendors would target the buyer persona (generally the CISO or director/manager-level) at the very least, since they are the people spending the budget. Ideally vendors would also target ICs, who are the ones actually using the product, and whose thumbs up is generally required by the buyer.</p>

<p>But given <em>none</em> of those personas felt addressed by vendors at the RSAC expo hall, who did? To be perfectly frank, I’m not entirely convinced of the answer, but my hunch is that:</p>

<ol>
<li>VCs are dictating marketing/value propositions too much, particularly given they are generally disconnected from customer viewpoints</li>
<li>Marketing people are generally too disconnected from customers &amp; don’t really understand the relevant personas</li>
<li>Infosec is generally horrendously bad at understanding the spectrum of relevant user personas</li>
</ol>

<h2 id="a-name-personas-a-misunderstanding-of-personas"><a name="personas"></a>Misunderstanding of personas</h2>

<p>Point #3 above leads to this observation, which is that vendors overall seem to egregiously misunderstand the personas for which they are allegedly building their products.</p>

<p>As I discussed in <a href="https://techcrunch.com/2019/02/13/the-infosec-reckoning-has-arrived/">my TechCrunch article</a>, vendors are building tools and determining the specific problem being solved after the fact — often leading to the need to convince customers that the vendor-invented problem is relevant to their organization. This trend of focusing on tech rather than customer problems extends, I think, to vendor-invented personas, as well.</p>

<p>If you were judging solely based on the RSAC vendor floor this year, you might think the most important user persona in infosec is “SecOps.” There’s little differentiation visible in this “persona” between a SOC analyst straight out of school analyzing low-priority events vs. a SecOps engineer who writes automation scripts for things like <a href="https://slack.engineering/distributed-security-alerting-c89414c992d6">distributed alerting</a> — not to mention a SecOps manager vs a SOC manager and the chromatic variation around all of these titles. And, is their purview <a href="https://medium.com/@sroberts/introduction-to-dfir-d35d5de4c180">just DFIR</a> or other responsibilities, too?</p>

<p>This isn’t to say generalized personas aren’t useful — but I’m wondering if vendors are now treating SecOps as the “person who deals with security events.” If so, then most people in infosec are kind of SecOps depending on how you define “event,” which would thus render SecOps a meaningless term given how different everyone’s workflows are across the range of roles.</p>

<p>Additionally, there were impressively lazy attempts at catering to the DevOps and so-called SecDevOps personas. Security vendors seem to think that making their product available via API means they “integrate with DevOps workflows,” which is either amazingly apathetic or oblivious.</p>

<p>There was also lip service paid to working with existing CI/CD pipelines, but then security vendors still expect DevOps people to go through 20 additional hoops, not realizing that they simply won’t. For instance, please don’t claim you are DevOps-friendly while asking them to install a kernel module on every machine they actively use.</p>

<p>This leads to my assumption (and into my next point) that security doesn’t realize how immaterial they are relative to a team (DevOps) that can justifiably argue that they support revenue-generating activities. And, what’s worse, vendors provide little support for their customers to fight for security’s inclusion in the conversation.</p>

<p>It as if vendors expect that by saying “we secure the development lifecycle!”, security practitioners can convince the organization to install their product. This notion of a security team having the keys to an organizational steam roller is, of course, pure fantasy.</p>

<h2 id="a-name-blocker-a-promotion-of-security-as-a-blocker-vs-a-compromiser"><a name="blocker"></a>Promotion of security as a blocker vs. a compromiser</h2>

<p>Finally, I think infosec is largely ignoring an important emerging shift, which I believe is inevitable — the transition from security as the “no people” to enablers of the business. Enterprise security only matters to the extent that it is helping preserve the company’s ongoing operations. Anything beyond that feels largely like intellectual vanity to me (this is precisely why I’ve been evangelizing the notion for <a href="/blog/posts/security-as-a-product/">a few years now</a>).</p>

<p>How does this play into RSAC? I argue that the most successful security vendors of late are about removing barriers, in contrast to many security people believing barriers are worthwhile aids in their infosec crusade (and I also suspect that creating organizational barriers provides significant vocational fulfillment among a chunk of security professionals).</p>

<p>To wit, zScaler consolidated old products and made it easier for the organization to access resources without the tedious VPN dance. Okta likewise streamlined access and made a lot of enterprise workflows simpler. Both are now trading at delectably rich multiples (both above 20x <a href="https://www.investopedia.com/terms/e/ev-revenue-multiple.asp">EV/Revenue</a>) — a victory even beyond the fact that they are some of the few information security startups to IPO in recent years.</p>

<p>Ultimately, anyone can say “no” to something —but just saying “no” isn’t actually solving a problem. Figuring out a compromise, like preserving or even improving UX while still ensuring an organization’s security, is a hard problem — the type of problem which should be most intellectually fulfilling.</p>

<p>Security will keep being dismissed by other parts of the organization until it lets go of finding fulfillment through security righteousness and instead finds fulfillment by finding an elegant way to enable the business while still reducing its risk.</p>

<p>One of my favorite examples to cite is John “Four” Flynn and <a href="https://www.youtube.com/watch?v=pY4FBGI7bHM">the tale of implementing 2FA on SSH at Facebook</a>. The security team conducted product-like user research interviews across engineering teams to figure out how SSH was being used — essentially mapping user workflows. The security team’s goal was to add 2FA to SSH, but instead of ramming it through like petulant ayatollahs, they sought to understand the user perspective and determine a solution that would not add friction for engineers.</p>

<p>It was a hard problem, but with a huge payoff; now the engineering teams can trust that security isn’t just trying to make their lives more difficult for sport or quasi-religious fervor. This means the relationship can be more like a <em>partnership</em>, rather than a political battle — the sort of battle security practitioners allegedly detest (likely because they tend to lose it).</p>

<hr />

<h2 id="conclusion">Conclusion</h2>

<p>These issues are not engendered by one constituent of the industry — they just all happen to notably bubble up like a tar pit burping methane in the harsh fluorescent spotlight of RSAC. It is the failure of culture among practitioners, vendors failing to understand their customers’ needs, founders going for cash grabs, and VCs encouraging the noxious sleaze and bamboozlery.</p>

<p>All of this comes, perhaps, from a reticence towards doing the harder, right thing and instead taking the easier path — the one that harvests confusion and fear to seize success that mediocrity could not earn otherwise.</p>

<p><a href="https://swagitda.com/speaking/index.html">I speak a lot</a> about incentives, and, in fairness, there is an abundance of incentives rewarding this mediocrity, but far fewer promoting the more difficult path. CISOs don’t want to speak out against vendors and, in fact, benefit when they are seen as essential in navigating not just the threat landscape, but the vendor landscape as well. Vendors can still find decent M&amp;A exits that make founders and investors happy enough, even if they sell solely within their professional networks and don’t ever find true product/market fit.</p>

<p>And if VCs make enough money from this buffoonery, why should they bother to understand the customer personas better to inform their investment choices? Many VCs have CISO friends, but I know from private conversations that these CISO friends often view the VCs as useful fools who can help them gain advisory or board positions.</p>

<p>If it sounds hopeless, well… I’m not sure it isn’t. But I refuse to <a href="/blog/posts/red-pill-of-resilience-infosec/">give into nihlism</a>. There are absolutely those of us who are fighting to change things, but there is a lot of entrenched power which benefits from keeping these dynamics the way they are in information security.</p>

<p>It will take those willing to risk their power to speak out in the truest form of thought leadership there is. Speaking into a warm and fuzzy echo chamber isn’t thought leadership; bravely challenging the status quo, armed with evidence, is.</p>

<p>We can keep the corporate-friendly cerulean and navy aesthetic, the blustering loudness, and yes, even the free pens, but we need to dedicate these efforts not to the thrill of our industry finally being considered “cool,” but to solving hard problems and protecting organizations in the way they need.</p>
]]></content>
        </item>
        
        <item>
            <title>InfoSec Startup Buzzword Bingo: 2019 Edition</title>
            <link>//swagitda.com/blog/posts/infosec-buzzword-bingo-2019-edition/</link>
            <pubDate>Wed, 27 Feb 2019 16:57:23 -0400</pubDate>
            
            <guid>//swagitda.com/blog/posts/infosec-buzzword-bingo-2019-edition/</guid>
            <description>This is the third edition of my Infosec Buzzword Bingo, just in time for 2019’s RSA Conference (RSAC). Rather than relying on my keenly tuned snake oil spidey senses to generate the words populating the bingo card, I took a more data-driven approach this year.
I surveyed 100 companies’ websites[1], the vast majority of which are exhibiting at RSAC and possess VC funding. I did not include any of the large security vendors[2], who probably could populate their own bingo cards across their mastodonian websites.</description>
            <content type="html"><![CDATA[

<p>This is the <a href="https://twitter.com/swagitda_/status/912494974779973632">third</a> <a href="https://twitter.com/swagitda_/status/967566556262694912">edition</a> of my Infosec Buzzword Bingo, just in time for 2019’s RSA Conference (RSAC). Rather than relying on my keenly tuned snake oil spidey senses to generate the words populating the bingo card, I took a more data-driven approach this year.</p>

<p>I surveyed 100 companies’ websites<a name="back-1"></a><a href="#cite-1">[1]</a>, the vast majority of which are exhibiting at RSAC and possess VC funding. I did not include any of the large security vendors<a name="back-2"></a><a href="#cite-2">[2]</a>, who probably could populate their own bingo cards across their mastodonian websites.</p>

<p>The idea is to take this with you to RSAC or any other vendor halls at information security conferences this year to see how many times you can win bingo at a single vendor booth! For more fun, <a href="https://github.com/swagitda/infosec-buzzword-bingo/blob/master/cyber-taglines.py">try out my Cyber Tagline Generator script</a> to create your own maniacally terrible buzzword salad (<a href="https://twitter.com/swagitda_/">then @ it to me on Twitter</a>).</p>

<p>Without further introduction, here’s the bingo card in all its glory — read on if you want more analysis on the stats:
<img src="/blog/img/infosec-startup-bingo-2019.png" alt="Infosec Startup Buzzword Bingo card for 2019" /></p>

<p>The top word by far this year was <strong><em>automated</em></strong> and its variants — nearly three quarters of all companies used it on their sites in one way or another (e.g. <strong><em>automatically</em></strong>, <strong><em>automation</em></strong>, <strong><em>automates</em></strong>, etc.). There were a few repeats from prior bingo cards, perhaps proving my natural acuity for sensing the buzziest buzzwords. The <a href="https://github.com/swagitda/infosec-buzzword-bingo/blob/master/buzzword-bingo.md">following table</a> of the top 25 buzzwords (the ones on the bingo card) includes the number of companies who cited the buzzword, along with whether the buzzword was on prior bingo cards:
<script src="https://gist.github.com/swagitda/7a55615c8b7889b92a3edaae7e8462e2.js"></script></p>

<h2 id="which-buzzwords-are-on-the-rise">Which buzzwords are on the rise?</h2>

<p>Let’s start with the words on the bingo card itself. You can’t just enable security people anymore, you must <strong><em>empower</em></strong> them. Seemingly a reaction to CISOs through SecOps analysts complaining about the complexity of security tools, <strong><em>simple</em></strong>, <strong><em>simplifies</em></strong>, and <strong><em>simplified</em></strong> are being used to (proactively ;) assuage concerns.</p>

<p>Allegedly, security professionals now want to <strong><em>discover</em></strong> things, and I suppose with most data lakes being more akin to data swamps, the predilection for adventure that <strong><em>discovery</em></strong> implies is required. And, taking a page from the DevOps world, <strong><em>orchestration</em></strong> is peppered around enough now to create a veritable symphony of infosec startups <strong><em>orchestrating</em></strong> away.</p>

<p>Not quite making it to the bingo card, but still heating up, is <strong><em>collaborate</em></strong> and <strong><em>collaboration</em></strong>. Who knew that infosec teams wanted to work together? And if your security product isn’t <strong><em>optimized</em></strong>, what are you even doing? Note that you do not have to say for what your product is <strong><em>optimizing</em></strong>, just that it is, in fact, <strong><em>optimized</em></strong>.</p>

<p>Finally, solutions for <strong><em>runtime</em></strong> security are growing, which basically is just saying it doesn’t break the computer as it is computing. The fact that this assurance must be stated at all says more about the infosec vendor situation than perhaps <a href="https://techcrunch.com/2019/02/13/the-infosec-reckoning-has-arrived/">even a long thought piece</a> can.</p>

<h2 id="which-buzzwords-are-starting-to-fall">Which buzzwords are starting to fall?</h2>

<p><strong><em>Hunting</em></strong> / <strong><em>hunt</em></strong>, most frequently used with “threat” before it, just barely missed making the Infosec Buzzword Bingo card again this year. I anticipate that it’ll fall even further by next year as the category morphs into SIEM 2.0. While <strong><em>behavioral</em></strong> is still holding strong, <strong><em>anomalies</em></strong> is beginning to wane — it’s much sexier to say you have <strong><em>behavior-based machine learning</em></strong> or <strong><em>AI</em></strong> instead.</p>

<p>As far as threats, they are notably less <strong><em>sophisticated</em></strong>, and less found on the <strong><em>dark</em></strong> web or in <strong><em>IoT</em></strong> devices. You def don’t want to talk about your product as a <strong><em>single-pane-of-glass</em></strong> anymore (try <strong><em>intuitive</em></strong> instead, which is on the rise). And, <strong><em>cloud-based</em></strong> is becoming less relevant as most security solutions move to a SaaSy model. If anything, companies should now specify when they <em>aren’t</em> cloud-based.</p>

<h2 id="which-buzzwords-are-the-weirdest">Which buzzwords are the weirdest?</h2>

<p>Most of the weirdo-words are only used in a handful of companies’ marketing spiels, thankfully. But, a full seven companies used <strong><em>real-world</em></strong>, which, I mean — as far as I know, we aren’t trying to secure hypothetical worlds? I’m being purposefully obtuse (in the Dostoevskyan-fool spirit), since I do understand that too many solutions catch bad stuff only in theory without rigorously testing against what attackers will actually do (or without even really considering this during the product’s conception). But, its usage still makes me sad.</p>

<p>Another odd buzzword was <strong><em>holistic</em></strong>, also cited by seven companies, which is perhaps the most credible buzzword due to its close association with essential “healing” oils. However, the one I hate the most by far is <strong><em>quantum-resistant</em></strong>. For my sanity’s sake, I am grateful only one company chose to use that term.</p>

<hr />

<p><a name="cite-1"></a><a href="#back-1">[1]</a> I scoped it to the websites’ landing and product/platform pages (e.g. no blog content).</p>

<p><a name="cite-2"></a><a href="#back-2">[2]</a> When I say “large” security vendors, think those who are publicly traded, are more than ten years old, or who have entire product “suites.”</p>
]]></content>
        </item>
        
        <item>
            <title>Analyzing the 2019 RSA Innovation Sandbox Finalists</title>
            <link>//swagitda.com/blog/posts/analyzing-2019-rsa-innovation-sandbox-finalists/</link>
            <pubDate>Tue, 05 Feb 2019 19:02:53 -0400</pubDate>
            
            <guid>//swagitda.com/blog/posts/analyzing-2019-rsa-innovation-sandbox-finalists/</guid>
            <description>This year’s nominees for the 2019 edition of the RSA Conference’s Innovation Sandbox were announced this morning. As I’m wont to do, I wanted to explore the funding side of these ten startups.
Total Funding Raised &amp;amp; the Latest Stage for each 2019 RSA Innovation Sandbox Finalist ($USD millions)
The median funding raised by all ten startups is $10.5 million (mean of $10.3 million), which makes sense for an early stage-flavored competition.</description>
            <content type="html"><![CDATA[<p>This year’s nominees for the 2019 edition of the RSA Conference’s Innovation Sandbox were <a href="https://www.rsaconference.com/press/98/rsa-conference-announces-finalists-for-innovation">announced this morning</a>. As I’m wont to do, I wanted to explore the funding side of these ten startups.</p>

<p><img src="/blog/img/rsa-sandbox-02.png" alt="Chart of funding raised by RSA Innovation Sandbox finalists" /><em>Total Funding Raised &amp; the Latest Stage for each 2019 RSA Innovation Sandbox Finalist ($USD millions)</em></p>

<p>The median funding raised by all ten startups is $10.5 million (mean of $10.3 million), which makes sense for an early stage-flavored competition. Additional analysis is required to compare funding levels of finalists from prior years to see if there is a correlation with those who win the competition.</p>

<figure style="float:right; max-width:55%; padding-left: 10px">
    <img src="/blog/img/rsa-sandbox-01.png" alt="Chart of the Distribution of Innovation Sandbox Finalists, by Funding Stage">
    <figcaption>Distribution of Innovation Sandbox Finalists, by Funding Stage<figcaption>
</figure>

<p>The distribution of startups by stage is also consistent, with the highest concentration at the Series A stage. Of course, another way to determine startup maturity is through actual temporal age. The median number of months<a name="back-1"></a><a href="#cite-1">[1]</a> since the finalists’ founded dates is 27.5, equating to roughly 2.3 years. The eldest is just shy of four years old.</p>

<p>Category-wise, there is a healthy distribution across sub-sectors. The highest concentration is in DevSecOps-related tools, which range from detecting attacks in modern infrastructure to “API security.” There are also two finalists tackling data protection/privacy. The rest include one startup each tackling anti-fraud, appsec, asset management, firmware/hardware security, and IAM. If <a href="https://twitter.com/swagitda_">you follow me</a>, you know I find <a href="/blog/posts/2019-cybersecurity-predictions/">masochistic pleasure</a> in examining the nature of infosec buzzwords, and the Innovation Sandbox word cloud does not disappoint on this front:</p>

<p><img src="/blog/img/rsa-sandbox-wordcloud.png" alt="Wordcloud of buzzwords from RSA Innovation Sandbox finalists in 2019" /><em>“a security platform to automatically protect human fraud from exhaustive enforcement”</em></p>

<p>What is perhaps most peculiar about this group of finalists to those who follow VC funding trends in infosec (hopefully I am not alone in this nerddom), is that there is scant overlap between investors in these startups. Perhaps, like with movie studios and the Oscars, each VC selects the startup in their portfolio they believe is in the strongest position to win. The sole overlapping investor was ClearSky, who led Capsule8’s Series B last August, and also led CloudKnox’s “Venture Round” (which feels very Series A) last October. The full list of lead investors in these ten startups is:</p>

<ul>
<li>Bain Capital Ventures</li>
<li>Bessemer</li>
<li>ClearSky x 2</li>
<li>Foundation Capital</li>
<li>Madrona Venture Group</li>
<li>Mayfield Fund</li>
<li>New Enterprise Associates</li>
<li>PayPal</li>
<li>PSP Growth</li>
<li>Rally Ventures</li>
<li>S Capital</li>
<li>Team8</li>
<li>USVP</li>
<li>Y Combinator</li>
<li>YL Ventures</li>
</ul>

<p>All finalists are based inside of the U.S. Of those, 40% are based in California — New York has two, and Kansas, New Jersey, Oregon, and Virginia have one startup each.</p>

<p>As a final statistic, only one company’s founding team includes any female founders.</p>

<hr />

<p><a name="cite-1"></a><a href="#back-1">[1]</a> Note: Not every startup gives the precise day they were founded, so I used a combination of Crunchbase and LinkedIn data (e.g., the start date the founder lists) to get as close as possible.</p>
]]></content>
        </item>
        
        <item>
            <title>The Cyber Tub: Communicating the Dynamics of Information Security Risk Management</title>
            <link>//swagitda.com/blog/posts/the-cyber-tub/</link>
            <pubDate>Thu, 20 Dec 2018 22:02:49 -0400</pubDate>
            
            <guid>//swagitda.com/blog/posts/the-cyber-tub/</guid>
            <description>This is probably how a Cyber Tub looks, right?
People struggle to understand how risk accumulates in complex systems, thereby also not understanding the extent to which risk must be reduced. This misapprehension can lead to “wait and see” decisions that cause a problem to snowball, or mitigations that don’t meaningfully reduce risk, creating the feeling of just barely treading water in your security program.
It is challenging for people to understand risk dynamics conceptually for two primary reasons.</description>
            <content type="html"><![CDATA[

<p><img src="/blog/img/cyber-tub.png" alt="A tub covered in Matrix code" /><em>This is probably how a Cyber Tub looks, right?</em></p>

<p>People struggle to understand how risk accumulates in complex systems, thereby also not understanding the extent to which risk must be reduced. This misapprehension can lead to “wait and see” decisions that cause a problem to snowball, or mitigations that don’t meaningfully reduce risk, creating the feeling of just barely treading water in your security program.</p>

<p>It is challenging for people to understand risk dynamics conceptually for two primary reasons. First, we are bad at tying inflows and outflows to the current level of risk in a system, as we tend to believe outputs are positively correlated with inputs. For example, from the climate change realm, 63% of MIT graduate students erroneously believed that if you stabilize emissions above the rate they’re being removed, atmospheric CO2 would stabilize.<a name="back-1"></a><a href="#cite-1">[1]</a> If emissions are still higher than reductions, there still will be net pollution — it will just be added at a more stable rate.</p>

<p>Second, we tend to ignore the accumulation of effects from inflows.<a name="back-2"></a><a href="#cite-2">[2]</a> For example, you may reduce the amount you overspend in a given year, but that doesn’t mean your personal debt is being reduced — you must earn a surplus over a period of time to pay down the debt you already accumulated. Ignoring accumulation leads us to underestimate the magnitude of mitigations required to stabilize risk — let alone make a dent in decreasing the risk level towards our goal.</p>

<p>What can we do about this lack of intuitive comprehension? A useful analogy to help people conceptually is a bathtub, with its straightforward inflows and outflows. When discussing information security risk, this analogy can help policy makers grasp the true implications of the decisions they’re making. Too often, mitigations are believed to “solve” the problem, while in reality, the inflows contributing to the problem are still outpacing the benefits from mitigations — but no further action is taken, resulting in continued accumulation of risk.</p>

<p>Thus, I’ve conceived the “Cyber Tub” as a way to better communicate information security risk and ensuring its dynamics — from accumulation to reduction — are well understood. Let’s delve into the analogy.</p>

<hr />

<h3 id="the-spout">The Spout</h3>

<p>Let’s say you have a bathtub already full of water. The bathtub is actually a “Cyber Tub,” and the water already in it represents your risk level — it could include things like your legacy systems or even the risk of credential theft in modern systems. The spout is actively running, adding a steady, hot stream of complexity into the tub. You don’t want the tub to overflow, because in real life, that leads to a state where you want to tear your hair out from all the complexity you must manage, and probably something will go wrong. So what do you do?</p>

<h3 id="the-drain">The Drain</h3>

<p>First, you need to install a drain — the patch management drain, in fact — if you don’t have one already, which is going to be challenging given all the water already in the tub. If you do have a drain, it’s likely clogged with lots of gross hair from people using the tub, so you’re going to have to clean it out manually so it can drain — and you’ll have to do that each time manually when it gets clogged again.</p>

<p>But, of course, you’re very clever, so you decide to install a self-cleaning drain — an automated patch management solution. However, you already know it will take a lot of effort to implement, and it probably won’t work perfectly the first time (and probably not every time in the future, either). So, you perform some calculations to see which helps keep the tub from overflowing more effectively given how many manual uncloggers you have vs. how many auto-drain maintainers you have on your team.</p>

<h3 id="the-bucket">The Bucket</h3>

<p>Regrettably, your drain solution only keeps the tub from filling up more rather than helping the water level go down. Clearly you need a bucket to remove a bunch of water all at once. But where do you get the bucket? Which type of bucket is right? How do you dunk in the bucket without splashing a bunch of water out? Where do you put the water in the bucket after? Do you need multiple buckets? There are lots of things to think about when transitioning to the bucket life.</p>

<p>Think of this like transitioning to a cloud or containerized world — the transition costs will be non-trivial and involve a lot of thinking over how to carefully lift the water without losing any.<a name="back-3"></a><a href="#cite-3">[3]</a> You can also only transition a certain amount of code at a time, so there needs to be a rollout plan as well. After all, even if you can buy a bunch of buckets at one time, it’s unlikely you can dump them all in at once to clear out the tub without something going wrong.</p>

<p>As you can see, there’s a lot of calculation here, including whether the bucket-based transition to cloud / containers can clear out the tub, since we already know the patch management drain will cancel out the complexity faucet, but nothing more. You also can’t forget that managing a bunch of little buckets is different from managing one tub, so you’ll need to consider the potential risks from that, too.</p>

<h3 id="additional-examples">Additional Examples</h3>

<p>To solidify exactly what I mean through this analogy, let’s consider other examples from information security and how they can be viewed through the Cyber Tub lens:</p>

<ol>
<li>A design review by the security team helps steady the water level as you release a new feature or product. By addressing security issues at the design phase, you tackle problems before they go into production and come out of the spout (you can use a threat model to prioritize). You could also require sign-off by the security team before release. Therefore, when a new feature or product is released, it won’t add as much water to the Cyber Tub.</li>
<li>Pentesting only tells you a very rough measure of your water level is — do limited findings mean your app is secure, or instead that your testing team is inadequate? Even if you remedy issues from the results of the pentest, it may only be counteracting the spout slightly more — you can think of a pentest like a leaky bucket that may or may not help. Adopting a continuous testing model instead can act as a healthy drain and hedge against ongoing risk that point-in-time assessments cannot catch.</li>
<li>If you are concerned about credential theft risk, think of it as the water in the bathtub. The drain could represent requirements for complex passwords —perhaps enough to counteract the additional risk of each new credential added. The bucket could be enforcing the use of SSO and a password manager, which will actually lower the level of the risk engendered by the existing pool of credentials.<a name="back-4"></a><a href="#cite-4">[4]</a></li>
</ol>

<hr />

<h2 id="conclusion">Conclusion</h2>

<p>For any problem in the information security risk space, try to think of it in terms of this “Cyber Tub.” Consider these four key questions:</p>

<ol>
<li>What are the inflows that add to the problem?</li>
<li>What can you do to reduce water coming in from the spout?</li>
<li>How can you keep the drain open and draining quickly?</li>
<li>What mitigations will actually make a dent in the problem? What are the best buckets available?<a name="back-5"></a><a href="#cite-5">[5]</a></li>
</ol>

<p>While the Cyber Tub may seem like an overly simplistic analogy, that is part of its beauty — nearly anyone can understand it, as long as they’re familiar with the basics of how a bathtub works. We can’t expect to present facts and figures to non-experts and have them perfectly grasp the dynamic processes around information security risk. Let’s give everyone a helping hand and start presenting information security risk in a way people can actually understand it — a way that leads to smarter decision making.</p>

<hr />

<h2 id="references">References</h2>

<p><a name="cite-1"></a><a href="#back-1">[1]</a> Sterman, J. D., &amp; Sweeney, L. B. (2007). <a href="http://web.mit.edu/jsterman/www/StermanSweeneyClimaticChangeFinal.pdf">Understanding public complacency about climate change: Adults’ mental models of climate change violate conservation of matter.</a> <em>Climatic Change, 80</em>(3–4), 213–238. doi:10.1007/s10584–006–9107–5</p>

<p><a name="cite-2"></a><a href="#back-2">[2]</a> Sterman, J. D. (2012). <a href="http://jsterman.scripts.mit.edu/docs/Sterman%20Sustaining%20Sustainability%2010-2.pdf">Sustaining Sustainability: Creating a Systems Science in a Fragmented Academy and Polarized World.</a> <em>Sustainability Science</em>, 21–58. doi:10.1007/978–1–4614–3188–6_2</p>

<p><a name="cite-3"></a><a href="#back-3">[3]</a> It’s surprisingly difficult to find a succinct explanation for how modern infrastructure can help with information security, outside of vendor drivel. For now, please check out <a href="/blog/posts/red-pill-of-resilience-infosec/">the fulltext of my keynote on resilience in infosec</a>, specifically the “adaptability” and “transformability” sections. You can also view <a href="https://www.leviathansecurity.com/cloudsecurity">these resources by Leviathan</a>, although they are a bit out of date (2015).</p>

<p><a name="cite-4"></a><a href="#back-4">[4]</a> Thank you to <a href="https://medium.com/@HockeyInJune/product-security-14127b5838ba">Julian Cohen</a> for this example.</p>

<p><a name="cite-5"></a><a href="#back-5">[5]</a> You also have to consider the difficulty implementing these mitigations in the context of the stagnant bathwater — the problem — potentially obscuring your path to implementation (a topic for another time).</p>

<p>I also suggest reading: Åström, K.J. and Murray, R.M. <a href="http://www.cds.caltech.edu/~murray/books/AM08/pdf/am07-complete_17Jul07.pdf">Feedback Systems: An Introduction for Scientists and Engineers</a>.</p>

<p><em>Thank you to Julian Cohen, Camille Fournier, and Alex Rasmussen.</em></p>
]]></content>
        </item>
        
        <item>
            <title>My 2018 Reading List</title>
            <link>//swagitda.com/blog/posts/2018-reading-list/</link>
            <pubDate>Mon, 17 Dec 2018 18:35:02 -0400</pubDate>
            
            <guid>//swagitda.com/blog/posts/2018-reading-list/</guid>
            <description>My perennial New Year’s resolution is to read one fiction and one non-fiction book per month. I tend to fail, and this year I only averaged 1.33 books per month (which, interestingly, is the same as last year; 2016 was 1.5 per month).
As you can tell from this list, I became a bit obsessed with afrofuturism and am still in awe of the immersive worldbuilding within the genre’s novels I read.</description>
            <content type="html"><![CDATA[

<p>My perennial New Year’s resolution is to read one fiction and one non-fiction book per month. I tend to fail, and this year I only averaged 1.33 books per month (which, interestingly, is the same as last year; 2016 was 1.5 per month).</p>

<p>As you can tell from this list, I became a bit obsessed with <a href="https://en.wikipedia.org/wiki/Afrofuturism">afrofuturism</a> and am still in awe of the immersive worldbuilding within the genre’s novels I read. I gravitated more towards fiction this year in general, which meant I snuck in fewer non-fiction books than usual (I did read more academic papers this year, but they’re far more arid).</p>

<p>If you’re looking for more science fiction, speculative fiction, or non-fiction recommendations, check out <a href="/blog/posts/2017-reading-list">my 2017</a> and <a href="/blog/posts/2016-reading-list">my 2016</a> reading lists.</p>

<hr />

<h2 id="fiction">Fiction</h2>

<p><a href="https://www.amazon.com/1Q84-Vintage-International-Haruki-Murakami-ebook/dp/B004LROUW2/">1Q84</a> by Haruki Murakami</p>

<p><a href="https://www.amazon.com/After-Flare-Olukotun-Deji-Bryce-ebook/dp/B0759VMZ66/">After the Flare: A Novel</a> by Olukotun Deji Bryce</p>

<p><a href="https://www.amazon.com/All-Systems-Red-Kindle-Single-ebook/dp/B01MYZ8X5C">All Systems Red: the Murderbot Diaries</a> by Martha Wells</p>

<p><a href="https://www.amazon.com/Black-Gods-Drums-Dj%C3%A8l%C3%AD-Clark-ebook/dp/B0791JV58Z/">The Black God’s Drums</a> by P. Djèlí Clark</p>

<p><a href="https://www.amazon.com/Black-Gods-Drums-Dj%C3%A8l%C3%AD-Clark-ebook/dp/B0791JV58Z/">Children of Blood and Bone (Legacy of Orisha Book 1)</a> by Tomi Adeyemi</p>

<p><a href="https://www.amazon.com/Black-Gods-Drums-Dj%C3%A8l%C3%AD-Clark-ebook/dp/B0791JV58Z/">The Last Wish: Introducing the Witcher</a> by Andrzej Sapkowski</p>

<p><a href="https://www.amazon.com/Obelisk-Gate-Broken-Earth-Book-ebook/dp/B01922I1GG/">The Obelisk Gate (The Broken Earth Book 2)</a> by N. K. Jemisin</p>

<p><a href="https://www.amazon.com/Pale-Vintage-International-Vladimir-Nabokov-ebook/dp/B004KABDSY/">Pale Fire</a> by Vladimir Nabokov</p>

<p><a href="https://www.amazon.com/Stone-Sky-Broken-Earth-Book-ebook/dp/B01N7EQOFA/">The Stone Sky (The Broken Earth Book 3)</a> by N. K. Jemisin</p>

<p><a href="https://www.amazon.com/Tigers-Daughter-Ascendant-Book-ebook/dp/B01MT7C6T7/">The Tiger’s Daughter</a> by K Arsenault Rivera</p>

<p><a href="https://www.amazon.com/Tigers-Daughter-Ascendant-Book-ebook/dp/B01MT7C6T7/">Who Fears Death</a> by Nnedi Okorafor</p>

<p><a href="https://www.amazon.com/Wizard-Earthsea-Cycle-Book-ebook/dp/B008T9L6AM/">A Wizard of Earthsea (The Earthsea Cycle Series Book 1)</a> by Ursula K. Le Guin</p>

<hr />

<h2 id="non-fiction">Non-Fiction</h2>

<p><a href="https://www.amazon.com/Anticipating-Surprise-Analysis-Strategic-Warning-ebook/dp/B008H9Q5IW/">Anticipating Surprise: Analysis for Strategic Warning</a> by Cynthia M. Grabo</p>

<p><a href="https://www.amazon.com/Behind-Human-Error-David-Woods-ebook/dp/B075QFGTNP/">Behind Human Error</a> by David D. Woods, Sidney Dekker, Richard Cook, Leila Johannesen, Nadine Sarter</p>

<p>]The Book of Why: The New Science of Cause and Effect](<a href="https://www.amazon.com/Book-Why-Science-Cause-Effect-ebook/dp/B075CR9QBJ/">https://www.amazon.com/Book-Why-Science-Cause-Effect-ebook/dp/B075CR9QBJ/</a>) by Judea Pearl and Dana Mackenzie</p>

<p><a href="https://www.amazon.com/Complexity-Guided-Tour-Melanie-Mitchell-ebook/dp/B002SAUBWC/">Complexity: A Guided Tour</a> by Melanie Mitchell</p>
]]></content>
        </item>
        
        <item>
            <title>2019 Cyber Security Predictions</title>
            <link>//swagitda.com/blog/posts/2019-cyber-security-predictions/</link>
            <pubDate>Wed, 05 Dec 2018 20:07:01 -0400</pubDate>
            
            <guid>//swagitda.com/blog/posts/2019-cyber-security-predictions/</guid>
            <description>Fed up with ridiculous infosec predictions for the upcoming year, I decided to aggregate them all and use the power of Markov Chains to generate my own list. What follows is the result, very lightly edited solely for readability. You can see last year’s edition here.
In 2019, we predict 2019. Cyber espionage, cybercriminals — in 2019, they materialize. What if this is a dangerous reality? For example, consider how the world feels sometimes.</description>
            <content type="html"><![CDATA[

<p><em>Fed up with ridiculous infosec predictions for the upcoming year, I decided to aggregate them all and use the power of Markov Chains to generate my own list. What follows is the result, very lightly edited solely for readability. You can <a href="/blog/posts/2018-cybersecurity-predictions/">see last year’s edition here</a>.</em></p>

<p><img src="/blog/img/bad-cyberart-14.jpg" alt="An image of a digital chain" /></p>

<p>In 2019, we predict 2019. Cyber espionage, cybercriminals — in 2019, they materialize. What if this is a dangerous reality? For example, consider how the world feels sometimes. According to Ponemon, security leaders around the world feel sometimes.</p>

<p>During 2019 we expect to see an increase in cyber space. The prospects are understatement. If a sophisticated attack involves not one but five top-notch threats synergistically working together, the defense panorama could become very blurry. Security experts have a recipe for disaster.</p>

<p>We predict that criminals will further focus their efforts injudiciously, ignoring the lower severity vulnerabilities with known exploits in favor of largely academic high severity vulnerabilities. In 2019, we will see a version of this fictional attacker.</p>

<p>The purchase of cybersecurity has led to expanding attacks that will become more sophisticated in 2019 and beyond. We will continue to influence societal expectations on security, which will trickle down to companies through hundreds of thousands of vulnerable and easy targets for attackers to profit. Driven by many falling victim to feature misconceptions, more will become key targets. Cyber products that provide consolidated feature sets have a hard time understanding each customer’s specific pain points and the bad guys know this.</p>

<p>In 2019, even more high-profile breaches will push the security and privacy, finally. Security is argued about until we die. That’s a particularly terrifying threat.</p>

<hr />

<h2 id="prediction-1-ai-techniques-attacks-will-result-hackers">Prediction #1: AI TECHNIQUES: ATTACKS WILL RESULT, HACKERS</h2>

<p><img style="float:right; max-width:40%; padding-left: 10px" src="/blog/img/bad-cyberart-15.gif" alt="Gif of robot saying 'I will destroy humans'"></p>

<p>In this day and age of big data, artificial intelligence is the next weapon. The gold standard in hacking efficiency, weaponized AI offers attackers unparalleled insight into what, when, and where to strike. Attempts to weaponize AI offers attackers actual attacks. Systems could launch coordinated cyber criminals to increasingly AI. Is it a matter of anomalies.</p>

<p>AI could be exploited and could also leverage machine-learning and artificial intelligence and machine-learning technologies. The consistent threat is very real. In 2017, a Vietnamese security group claims to have created a mask that can learn incrementally from data scientists providing frequent feedback.</p>

<p>We predict AI-powered attacks become the keys for email scams. For example, imagine a fake AI-created phishing using AI to aid assaults. Unlike humans, machines can do it in seconds and continue even after business hours. They have gotten smarter about phishing and other human activities such as opening doors. Closer to home, AI will expose the mistakes they’ve made regarding human activities.</p>

<p>Automated systems powered by AI could also be used to evade detection by infrequently trained machine learning engines. This game of cat and machine-learning technology will be an investment in the new year. There will likely be future attacks focused on building robust centers for security breach infringement, but the AI bubble has many experts worried.</p>

<p>In 2019, we will see brute force attacks powered by AI. The attack requires automating out all the less interesting stuff so attackers can focus their resources on such attractive, data-rich environments, with no downtime to these utilities. More corporate attacks based on math will propel this trend forward.</p>

<hr />

<h2 id="prediction-2-the-ai-security-software-has-malicious-intent">Prediction #2: THE AI SECURITY SOFTWARE HAS MALICIOUS INTENT</h2>

<p>Skynet is becoming broader and more expansive. To combat this, organizations have turned to the promise of big data, artificial intelligence (AI), and machine learning. Automated systems powered by AI could help people better understand the tradeoffs involved when they give up personal information in their malicious software.</p>

<p>The fragility of some AI technologies will become the picklock that opens a much larger door. Certain algorithms may be too late. 2019 will demonstrate a lot of the “AI Winter” of 1969, in which Congress cut funding as results lagged behind lofty expectations. AI will bolster security in 2019 to a total of $206.2 billion, up from $175.8 billion in 2016, down to $14 billion by 2025.</p>

<p>The buzz for cybersecurity AI is expected to grow in popularity. As the report notes, the pure-play AI security story also has a dark side — they will start scamming you. In addition, certain algorithms may be too complex to understand what is driving a specific set of security firm activities that are popping up in Cyber Town, USA.</p>

<p>AI start-ups are going to exploit the growth of attacks. Analytics solutions will extort companies with 1,000 or more slippery endpoints. Based on developments we are seeing, this change will come as all teams recognize that cybersecurity AI in the purest sense is nonexistent, and we will continue raging.</p>

<hr />

<h2 id="prediction-3-cloud-will-slip-out-into-the-wild">Prediction #3: CLOUD WILL SLIP OUT INTO THE WILD</h2>

<p><img style="float:right; max-width:40%; padding-left: 10px" src="/blog/img/the-internet-was-a-mistake.gif" alt="Gif saying 'The internet was a mistake'">
Cloud adoption will begin to expand the world (though many dispute this story). By default, cloud is sensitive data. Also, the internet. In 2019, attackers will hold the internet hostage on a computer disc with Internet written on tape in sharpie.</p>

<p>Cloud adoption is game-changing in the threat equation. Many of the tried and true attacks of five years ago don’t work very well in the cloud. Organizations are rapidly shifting content to the cloud, therefore we predict a shortfall of 3.5 million cyber threats that demonstrates a real demand for these easy pickings.</p>

<p>Organizations will struggle to manipulate public cloud and will experience a massive security priority for 2019. Emerging technologies used to protect the cloud not only help capture the big picture but also are less effective at mitigating. Cloud and DevOps teams’ security experts are worried.</p>

<hr />

<h2 id="prediction-4-criminals-grow-more-confident-in-demanding-that-risks-involve-the-cloud">Prediction #4: CRIMINALS GROW MORE CONFIDENT IN DEMANDING THAT RISKS INVOLVE THE CLOUD</h2>

<p>Cyber criminals will use big-scale platforms to create instead of just one, five top-notch threats in today’s landscape. Such threats would be very difficult for hackers. Attacks are usually centered on the use of one threat. Bad actors concentrate their efforts on iterating and evolving one threat at a time for effectiveness and evasion.</p>

<p>With an attack surface of automated prevention methods, like embedded human microchips, for example, attackers will generate new threats such as AWS and Azure. Large-scale data breaches will be attributed to misconfigured Amazon S3 buckets. This is clearly not the fault of AWS. IDG, for example, calls 2019 “a seminal year” on the criminal to-do list, since criminals can silently steal thousands of open buckets and credentials.</p>

<p>Still, I make a brilliant, contrarian, and very accurate prediction: You might lose the data. There will be surprises, too, says Captain Obvious.</p>

<hr />

<h2 id="prediction-5-iot-powered-distributed-denial-of-cat">Prediction #5: IOT-POWERED DISTRIBUTED DENIAL OF CAT</h2>

<p><img style="float:right; max-width:40%; padding-left: 10px" src="/blog/img/drone-battle.gif" alt="A gif of a drone preparing for battle">
The security breaches will be IoT. There is an ever-increasing probability that these devices make their vulnerabilities. The Future often uses an IoT botnet, which runs the entire network. In one example, an attacker could compromise or alter a chip or add source code to avoid or delay botnet takedowns.</p>

<p>Another challenge is the newest form of an attack that combines card enumeration with smart gadgets, from plugs to TVs, coffee makers. In transportation, data has been accused of sneaking into a site connected to traffic lights. With IoT growth posing huge unknown risks to enterprises with the internet, which runs entirely in memory without effective mitigation, this tactic works. Refrigerators and washing malware will be undetected.</p>

<blockquote>
<p>“I think the big innovation is in best practice standards for IoT” — Damon Ponemon, Vice President of Technology to Detect Evil.</p>
</blockquote>

<hr />

<h2 id="prediction-6-evolving-definitions-of-privacy">Prediction #6: EVOLVING DEFINITIONS OF PRIVACY</h2>

<p>This year we highlighted privacy, finally, due to the European Union’s mid-2018 implementation of the internet. Nearly every nation has not been able to settle on a standard of constant privacy, which will continue to exacerbate in 2019. Singapore and India are consulting to adopt breach notification regimes, while Australia has already enforced GDPR-like legislation due to lack of attribution and accountability.</p>

<p>The Data Protection legislative and regulatory environment will become the de facto method for spreading malicious scripts directly on targeted subjects and organizations. The U.S. government will give birth to more advanced technology and employee training in order to distribute it quickly and surreptitiously to malware. Congress is already working on an RDP option.</p>

<blockquote>
<p>“Managing privacy will become a huge priority for the C-suite and board” — Prasad Woodridge, More Compliance Officer</p>
</blockquote>

<p>In 2019, black hat hackers will penetrate critical aspects of GDPR to become broadly deployed threats. The internet itself is ripe for the taking by someone with PCI or SOX. Well-crafted emails designed to avoid detection are likely to be life-threatening; however, we’re unlikely to see upticks in legislative and regulatory activity. With this in mind, even an organization that erased event logs and backups to avoid investigation will have to decide whether something that happened was supposed to happen.</p>

<hr />

<h2 id="prediction-7-malware-bots-tend-to-spread-chaos">Prediction #7: MALWARE BOTS TEND TO SPREAD CHAOS</h2>

<p><img style="float:right; max-width:40%; padding-left: 10px" src="/blog/img/bad-cyberart-16.gif" alt="Gif of someone typing in hacker-y things">
In 2019, we predict malware. Attackers will undoubtedly continue to evolve their tactics to steal credit cards and credentials. Malware authors will turn to either more targeted attacks using embedded chips on printers or use ransom techniques, including the manipulation of memory space and adding arbitrary code. Because the attack landscape continues to evade AI-based solutions, attackers will be able to use this naivete to their advantage and pull off a major attack with ransomware.</p>

<p>There is a race to get the most troubling widespread ransomware-as-a-service. These attacks often have costs far beyond the ransom itself. There is evidence that the author of GandCrab is already working on their marketing campaign to extort companies by threatening the data lakes. What can we do? What is permissible? What if we are missing the reasons synergic threats are becoming more than just real people? We will continue to falter.</p>

<p>In 2019, we’ll see the emergence of new threats such as cryptocurrency and the overwhelming demand for the large amounts of computing. Inevitably, there will be a battle as to which is more convenient than ransomware. An example is WaterMiner, which simply stops its mining process when the consumer is just about die.</p>

<hr />

<h2 id="prediction-8-identity-supply-chain-meets-blockchain">Prediction #8: IDENTITY SUPPLY CHAIN MEETS BLOCKCHAIN</h2>

<p>In 2019, cyber activities collide with physical worlds. New techniques will use attacks on critical infrastructure of blockchain, with a touch of “Huh?”</p>

<p>In 2019, the next vector in attacks will continue — privileged accounts, because bots. Identity is a fundamental shift in risk. Identity providers are exposed to an increase in the Open Authorization standard. Access management solutions are actually the intended malware — one was launched by Fancy Bear, the Russian cyber espionage.</p>

<p>“Edge device” breaches will push the security industry to finally solve the username/password problem. The ineffective username/password conundrum has plagued consumers and businesses for years. AI could be used in the hope that 2019 will see a more concerted effort to replace passwords altogether.</p>

<p>A ‘zero trust’ approach requires an organization and AI-enabled malware. This ‘zero trust’ approach can open up several attack vectors. First, it transfers risk and no one can rest easy. Second, organizations end up creating their own criminal activities. The embrace of Google’s BeyondCorp is a strategic guess by taking intelligence, which will become more clear across the field.</p>

<hr />

<h2 id="prediction-9-nation-state-attacks-will-cause-the-u-n-to-come-to-some-consensus">Prediction #9: NATION-STATE ATTACKS WILL CAUSE THE U.N. TO COME TO SOME CONSENSUS</h2>

<p><img style="float:right; max-width:40%; padding-left: 10px" src="/blog/img/computer-fire.gif" alt="Gif of someone setting computers on fire">
2019 might just be the toughest in the United States to date. While a direct cyberwar is not on the horizon, a nation-state will launch a “Fire Sale” attack: electronics on fire. You may remember the fictional concept of a “fire sale” attack from the 4th Die Hard movie, in which a terrorist demonstrated this.</p>

<p>Governments will be fed a false sense of security intelligence from tapped infected machines. Nation-states have launched huge distributed denial of services, Bitcoin mixers, and counter-antimalware services. These attacks mean governments are deeply suspicious of each threat actors’ criminal groups.</p>

<p>Brazil recently passed new process-injections and erased event logs to aid trade wars. North Korea, meanwhile, has allegedly attacked public and privacy needs. We are looking forward to seeing a steady increase in Iranian attackers that will continue to fall further and further behind in competency and integrity.</p>

<hr />

<h2 id="prediction-10-the-public-cloud-will-experience-a-massive-security-attack">Prediction #10: THE PUBLIC CLOUD WILL EXPERIENCE A MASSIVE SECURITY ATTACK</h2>

<p>The worldwide public cloud will experience a massive security attack<br />
The worldwide public cloud will experience a massive security attack<br />
The worldwide public cloud will experience a massive security attack<br />
The worldwide public cloud will experience a massive security attack<br />
The worldwide public cloud will experience a massive security attack<br />
The worldwide public cloud will experience a massive security attack<br />
The worldwide public cloud will experience a massive security attack<br />
The worldwide public cloud will experience a massive security attack<br />
The worldwide public cloud will experience a massive security attack<br />
The worldwide public cloud will experience a massive security attack<br />
The worldwide public cloud will experience a massive security attack<br />
The worldwide public cloud services market is still taking shape, with many brands still looking to develop weapons in the creation of malicious executables.</p>
]]></content>
        </item>
        
        <item>
            <title>Analyzing the Black Hat USA 2018 Business Hall</title>
            <link>//swagitda.com/blog/posts/analyzing-blackhatusa-business-hall-2018/</link>
            <pubDate>Mon, 06 Aug 2018 18:40:42 -0400</pubDate>
            
            <guid>//swagitda.com/blog/posts/analyzing-blackhatusa-business-hall-2018/</guid>
            <description>What type of vendors are showing themselves off in the Business Hall? Are they mostly startups? Not quite “mostly,” but 46% of vendors in the hall are indeed VC-backed companies at varying stages of maturity. Privately held companies are a non-trivial segment at 17%, and there are 30 Private Equity-owned companies making up 12% of the hall. There are also 12 companies who are acquisitions, primarily those who were acquired within the past year; booths require being booked far in advance, leaving insufficient time to be assimilated into their acquiror’s booth (if so desired).</description>
            <content type="html"><![CDATA[

<h2 id="what-type-of-vendors-are-showing-themselves-off-in-the-business-hall-are-they-mostly-startups">What type of vendors are showing themselves off in the Business Hall? Are they mostly startups?</h2>

<p><img style="display:block; margin-right:auto; margin-left:auto; max-width:80%;" src="/blog/img/bhusa2018/vendors-type.png" alt="Chart of number of vendors by funding type for the Black Hat USA 2018 Vendor Hall"></p>

<p>Not quite “mostly,” but 46% of vendors in the hall are indeed VC-backed companies at varying stages of maturity. Privately held companies are a non-trivial segment at 17%, and there are 30 Private Equity-owned companies making up 12% of the hall. There are also 12 companies who are acquisitions, primarily those who were acquired within the past year; booths require being booked far in advance, leaving insufficient time to be assimilated into their acquiror’s booth (if so desired).
<img style="float:right; max-width:50%; padding-left: 10px" src="/blog/img/bhusa2018/vc-by-age.png" alt="Chart of number of VC-backed companies by age of last raise"></p>

<p>Nearly half (44%) of VC-backed companies were funded within the past year, shooting up to 82% within the past two years and 92% in the past three. Only 10 companies have not been funded in the past three years, and one could hypothesize that they are considered long in the tooth by their VCs.</p>

<hr />

<h2 id="how-many-vcs-are-dedicated-to-investing-in-infosec">How many VCs are dedicated to investing in infosec?</h2>

<p><img style="float:right; max-width:50%; padding-left: 10px" src="/blog/img/bhusa2018/fund-numbers.png" alt="Chart of the number of VC funds by companies invested">
The overwhelming majority (69%) of Venture Capital firms are investors in only one company exhibiting in the BH USA 2018 Business Hall. Out of 350 total investors in 118 VC-backed companies, 46% <a href="https://avc.com/2013/09/leading-vs-following/">led</a> at least one deal — and of those 46%, 71% only led one deal.</p>

<p>I’ve long held a hypothesis that the list of “dedicated” investors in infosec is actually quite small, and that the incredible amount of deal volume we presently see is driven by one-off investors who want to dip their toe in the infosec waters, having seen blazing, FUD-ridden headlines declaring its relevance. The data seem to support this hypothesis.</p>

<p>There are 21 VC firms who participated in four deals or more (i.e. invested in four or more companies exhibiting). Thus, I consider them the most “dedicated.” Data Collective, while following in four deals, did not lead any deals, meaning they just miss the cut to be part of the “Top 20 VCs”:
<script src="https://gist.github.com/swagitda/c27200dedbac7090090d7a0a2fe98ba1.js"></script></p>

<p>You can explore all of the Venture Capital firms, ordered by number of companies in the Business Hall in which they’ve invested by <a href="https://github.com/swagitda/bhusa2018-bizhall/blob/master/vc-analysis/vc-fund-count-with-names.png">visiting the GitHub repo</a>, or scrolling to the very bottom of this post.</p>

<hr />

<h2 id="what-venture-stage-are-vendors-how-much-capital-are-they-raising">What venture stage are vendors &amp; how much capital are they raising?</h2>

<p><img style="float:right; max-width:50%; padding-left: 10px" src="/blog/img/bhusa2018/vc-by-round.png" alt="Chart of the number of VC-backed vendors, by latest round">
As you might suspect given the steep price for Black Hat booths, there are few seed-stage companies present. The VC-backed vendors are highly concentrated between Series A and Series C (68% of all VC-backed vendors), which reflects general infosec funding trends, as few companies get funded to the late stage, either being acquired or quietly starved for capital.</p>

<p>Note, “Venture Round” is a nebulous term and can mean basically anything — from earlier stage to later stage — so it should not be interpreted in the same timeline as the Series rounds.</p>

<p>The size of the funding round naturally grows the later stage the company reaches. Note that there are few data points for Series E and Series F rounds, so don’t read too much into the decline in size at Series F (a hypothesis might be that reaching such an exceedingly late stage means investors’ patience has worn thin).</p>

<p><img style="display:block; margin-right:auto; margin-left:auto; max-width:80%" src="/blog/img/bhusa2018/vc-dollars-by-round.png" alt="Chart of the average size of funding rounds, by stage"></p>

<hr />

<h2 id="how-many-private-equity-firms-are-backing-companies-on-the-floor">How many Private Equity firms are backing companies on the floor?</h2>

<p>There are 30 companies backed by 27 total Private Equity firms in the Business Hall this year. While the majority (81%) are mostly one-off investors (though some are also participants in late-stage VC rounds), there are five PE firms that back two or more companies presenting on the floor:
<script src="https://gist.github.com/swagitda/5115bd90d76dfe7b67029dc3f665c90d.js"></script></p>

<hr />

<h2 id="how-fresh-is-the-innovation-city">How fresh is the Innovation City?</h2>

<p><img style="float:right; max-width:50%; padding-left: 10px" src="/blog/img/bhusa2018/innovation-city.png" alt="Chart of Innovation City 'residents,' by type">
Innovation City, with 41 “residents,” has a higher concentration of VC-backed companies than the total population (59% vs. 46%), as well as of privately-held companies (34% vs. 17%). What might be surprising is that there are still two acquired companies and one PE-backed company presenting in Innovation City.</p>

<p>True to its marketing pitch of being a designated area for early-stage companies, the average amount of the latest raise by Innovation City residents who are VC-backed is $11.4mm, in contrast to the overall average of $27.2mm. 58% of residents most recently raised a Series A or Series B, while only 4% most recently raised a Series C or later (vs. 34% in the general VC-backed Business Hall population). Nearly double (25% vs. 14% in the general VC-backed population) raised a “Venture Round,” which again, being a nebulous term, is troublesome to place in the VC funding timeline.</p>

<hr />

<h2 id="how-u-s-centric-are-the-vendors-at-black-hat-anyway">How U.S.-centric are the vendors at Black Hat, anyway?</h2>

<p><img style="float:right; max-width:60%; padding-left: 10px" src="/blog/img/bhusa2018/geo-all.png" alt="Chart of all companies, by geography">
The answer is pretty U.S.-centric, at 83% of all vendors in the Business Hall (and 86% of all VC-backed vendors). While I know Brexit hasn’t happened yet, I do consider the UK infosec market somewhat distinct from the EU, as there’s a decently active infosec startup scene there and an emergent VC ecosystem, too. There are 13 (5%) EU-based vendors, but less than half of those (6) are VC-backed. The UK is slightly more VC-weighted, with 6 VC-backed companies out of 10 companies total (4% of all companies).</p>

<p><img style="float:right; max-width:40%; padding-left: 10px" src="/blog/img/bhusa2018/geo-vc.png" alt="Chart of VC-backed companies, by geography">
Although funding activity is exceptionally strong for Israel-based security startups, there are only 7 Israeli vendors (3%) in the hall this year, 3 of which are VC-backed startups. Given the stark contrast with the funding volume into Silicon Wadi I’ve personally witnessed this year, I suspect it’s either that the companies are too young to have reserved a booth in time for 2018, or that quite a few are playing the classic game of listing HQ in the U.S. so as not to deter customers or investors.</p>

<p>Within the U.S., California makes up nearly half (47%) of all U.S.-based vendors presenting, and over half (52%) of VC-backed startups in the Business Hall. After California, the usual suspects of Massachusetts, New York, and the D.C. area (Virginia/Maryland) round out the bulk of companies, along with growing areas of VC interest like Colorado and Texas.</p>

<p><img style="display:block; margin-right:auto; margin-left:auto; max-width:80%" src="/blog/img/bhusa2018/state-all.png" alt="Chart of all U.S. companies, by state"></p>

<hr />

<h2 id="how-many-companies-have-some-form-of-name-collision">How many companies have some form of name collision?</h2>

<p>We’re all familiar with the infosec startup tropes, so I decided to see whether the data support it. Unsurprisingly, many companies (13%!) have some form of “Security” or “Secure” in their name. Net (which can include Networks), and Cy (which can include Cyber) present a solid showing as well. More recent tropes, such as “Dark” or “Deep” are less prevalent than I assumed — in fact, Deep only lurks in two vendors’ names.</p>

<p><img style="display:block; margin-right:auto; margin-left:auto; max-width:80%" src="/blog/img/bhusa2018/name-trope.png" alt="Chart of the number of companies, by trope in name"></p>

<hr />

<h2 id="a-few-notes-on-the-data">A few notes on the data</h2>

<p>Vendors were retrieved from the <a href="http://www.expocad.com/host/fx/ubm/18blckh/exfx.html#exhibitors">Black Hat 2018 Business Hall Floorplan</a>, and exclude any federal agencies, educational organizations, or nonprofits. I also excluded any companies in the Career Zone, as they are aiming to recruit security talent rather than sell products or services — for example, I presume Major League Baseball is not selling the latest Threat Intelligence Automation on the Blockchain.</p>

<hr />

<p>Care to explore the data yourself? <a href="https://github.com/swagitda/bhusa2018-bizhall/blob/master/data-table/bhusa18-bizhall-list.md">The raw table is available on GitHub</a>, as is the <a href="https://github.com/swagitda/bhusa2018-bizhall">general project repo</a> (potentially more to come later!).</p>

<hr />

<h2 id="appendix">Appendix</h2>

<h3 id="exactly-which-vcs-funded-exactly-how-many-companies-exhibiting-in-the-black-hat-usa-business-hall-this-year">Exactly which VCs funded exactly how many companies exhibiting in the Black Hat USA Business Hall this year?</h3>

<p><img style="display:block; margin-right:auto; margin-left:auto" src="/blog/img/bhusa2018/fund-names-tall.png" alt="List of VCs, sorted by number of business hall vendors funded"></p>
]]></content>
        </item>
        
        <item>
            <title>The Red Pill of Resilience in InfoSec</title>
            <link>//swagitda.com/blog/posts/red-pill-of-resilience-infosec/</link>
            <pubDate>Wed, 01 Aug 2018 17:26:47 -0400</pubDate>
            
            <guid>//swagitda.com/blog/posts/red-pill-of-resilience-infosec/</guid>
            <description>What follows is the full text of my keynote, also available as a video and as slides.
There has been insufficient exploration of the first principles of resilience in the context of information security, despite the term being superficially peppered in our common discourse. Too often, resilience is conflated with robustness — to the detriment of us all.
To state more poetically, through the pen of the notable fantasy author Robert Jordan referencing one of Aesop’s fables, “The oak fought the wind and was broken, the willow bent when it must and survived.</description>
            <content type="html"><![CDATA[

<p><img src="/blog/img/resilience-01.jpg" alt="Image of two hands clasping each other" /></p>

<p><em>What follows is the full text of my keynote, also available as <a href="https://www.youtube.com/watch?v=ux--pHFpeac">a video</a> and <a href="https://swagitda.com/speaking/Red-Pill-of-Resilience-Shortridge-Countermeasure-2017.pdf">as slides</a>.</em></p>

<hr />

<p>There has been insufficient exploration of the first principles of resilience in the context of information security, despite the term being superficially peppered in our common discourse. Too often, resilience is conflated with robustness — to the detriment of us all.</p>

<p>To state more poetically, through the pen of the notable fantasy author Robert Jordan referencing one of Aesop’s fables, <em>“The oak fought the wind and was broken, the willow bent when it must and survived.”</em> To speak of protection without resilience is to believe you can always beat the wind. To speak of deterrence without resilience is to believe you can deter the wind from blowing at all.</p>

<p>There are times when attacks may be deterred or halted before damage is wrought, but one cannot always predict which way the wind will blow. We cannot predict the adversary we will face and the amount of resources they are willing to expend to compromise our systems.</p>

<p>Protection or deterrence can serve as a valuable tactic to grant some level of peace, but resilience — the ability to absorb change and survive — is the foundation on which survival rests. Again, far more poetically than I could ever say, Generalissimo <a href="https://en.wikipedia.org/wiki/Chiang_Kai-shek">Chiang Kai-shek</a> of the Republic of China advised, <em>“The more you sweat in peace, the less you bleed in war.”</em> Attempt the peace, but assure you can still survive the war.</p>

<p>But the discourse about resilience in information security has, to date, been far from poetic. Rather, it’s been sprinkled as a buzzword, floating along at a shallow level primarily in discussions of cyber insurance and how companies can transfer risk by buying policies. I believe this is a waste of its potentially torrential conceptual power. Resilience is ultimately about accepting reality and building a defensive strategy around reality.</p>

<p>As in the Matrix, resilience serves as the red pill — once you accept the reality, it’s impossible to go back to a strategy that ignores it. My goal in this talk is to show you how digging into this heart of resilience can drive a paradigm shift in how we architect security strategy (and I don’t throw the term “paradigm shift” around lightly).</p>

<p>I’ll first explore why the time for the resilience paradigm is nigh, and how our grief as an industry has led us to this point. I’ll next briefly share the etymology of resilience. Its application to information security is one of the newest, and there exists a rich history of its use in other domains — a history worth exploring to see if there are principles and findings to be shared.</p>

<p>Information security is not the only complex system, and ecological systems dealing with climate change and urban areas dealing with natural disasters represent analogical systems in which dynamics are nearly impossible to predict, and the number of interrelating factors is prohibitive to enumerate.</p>

<p>This cross-domain research presents a concept of resilience based on robustness, adaptability, and transformability. I’ll cover these three concepts in detail so we can understand how they fit under the umbrella of resilience. I’ll expand on this cross-domain lens, and it will serve as the one through which I’ll examine what resilience means in information security.</p>

<p>I am humbly attempting to define and establish a notion around what resilience means for information security — both in an intellectual and practical sense. I would be shocked if all of my thinking is on the mark. More than anything, I desperately want to encourage a discussion of first principles around resilience, and to begin a fruitful conversation around how practical implementations of resilience ideology for defensive security should look. I believe we have a fighting chance.</p>

<hr />

<h2 id="stages-of-grief-in-information-security">Stages of Grief in Information Security</h2>

<p><img src="/blog/img/resilience-02.jpg" alt="Image of a burning rose" /></p>

<p>But first, why is there rising talk about the term “resilience,” despite the industry not having solidly established what it means? I propose that it is the result of the final stage of grief — acceptance — of the fact that there is not, and likely will not ever be, such a thing as an un-hackable organization or system.</p>

<p>Over the past twenty years, the infosec industry has grieved the fact that companies are ever-vulnerable to attack. Very little can be done against the latest exploit or attack vector that is currently unpublished and unknown. The industry has not fully coped with this grief. Like most grief, it isn’t a linear process, and these cycles have ebbed and flowed at various times in the industry’s history.</p>

<p>The first stage of grief is <strong>denial</strong> — clinging to a false, preferable reality. This manifested as companies being hesitant to deploy security solutions at all, believing that they weren’t truly at risk.</p>

<p><strong>Anger</strong> — recognizing the denial cannot continue, and becoming frustrated — seeing security as an unwanted necessity. This manifested in harsh penalties and legislation, prosecuting and punishing vulnerability researchers — even ones doing work for free or disclosing responsibly.</p>

<p><strong>Bargaining</strong> — the hope that the cause of grief can be avoided — resulted in the explosion of new security tools in an attempt to stop the problem</p>

<p><strong>Depression</strong> — despair at the recognition of the predicament — led to the refrain of more recent years in the vein of, “You’re going to get hacked, there’s nothing you can do about it.”</p>

<p>Finally, <strong>acceptance</strong> — the stage into which I believe we are now transitioning, understanding that there is an inevitability of successful attack, but recognition that there is an ability to prepare and that not all is hopeless.</p>

<p>Unfortunately, I fear the bargaining stage has set us up for a challenging road to implementing acceptance. The explosion of security tools in an attempt to avoid the problem has resulted in an untenable <a href="https://en.wikipedia.org/wiki/The_Market_for_Lemons">market for lemons</a> in which tools and services are prescribed regardless of real need.</p>

<p>Fear, uncertainty, and doubt (FUD) specifically preys on this desperation to bargain, and its use by marketing departments is little different than selling hope of a cure to those who are chronically ill and in their own bargaining stage of grief through alternative, disproved methods. More simply put, the bargaining stage is the demand for which snake oil is the supply.</p>

<p>The resulting depression is unfortunately not the antidote for snake oil — security nihilism is an inaccurate conclusion and does little to incentivize practitioners to pursue more resilient strategies than unproven ones. Therefore, in this blossoming acceptance phase, it is important we have a conversation about what actually works — and to begin, I’d like to delve into what resilience has meant before we as an industry began to espouse it.</p>

<hr />

<h2 id="etymology-of-resilience">Etymology of Resilience</h2>

<p>Up until the early 19th century, the primary meaning of resilience was to “rebound.” Its first use in the context of engineering was in 1858, to imply strength and ductility, or a material’s ability to stretch under tensile stress.<a name="back-1"></a><a href="#cite-1">[1]</a> The abstraction from this physical characteristic — the time it takes for a system to return to a pre-determined, single equilibrium — is the one which has persisted in the common understanding of resilience.</p>

<p>However, in the 1970s, resilience began being used in psychology — understood as a process of changing current behaviors to cope with an adverse condition, for reverting to a prior psychological state post-incident is unrealistic and can actually represent unhealthy coping strategies. The 1970s also saw the beginning of resilience’s use in ecology, but over time, the concept gradually expanded into use across the social sciences and with the two domains’ coupling, or socio-ecological systems.</p>

<p>Most recently, it began to be applied to climate change adaption in the early 2010s, including the natural disaster risk management space due to increases in natural disasters that by all evidence are caused by climate change.</p>

<h3 id="what-does-resilience-mean-for-complex-systems">What does resilience mean for complex systems?</h3>

<p>A complex system is one in which many of its underlying components interact with each other, and one in which it is very difficult to predict behavior. More simply put, it is a system with non-linear activity in the aggregate. Examples of complex systems in our daily lives include our universe, our planet’s climate, our cities, our brains, and even living cells.</p>

<p>Information security is also complex system. Defensively speaking, it is plagued by an inability to predict attacker actions, and it also consists of highly interconnected, dynamic relationships. Both sides of the security equation — defenders and attackers — are human. But there are additional relationships beyond the direct conflict, including users, governments, software vendors, service providers, and so forth. To unfurl these relationships and attempt to fit them into a predictive model is very simply prohibitive.</p>

<p>The first application of resilience through the lens of complex systems was by C.S. Holling in 1973, an ecologist who was one of the founders of ecological economics. Ecological resilience, he said, is measured by the amount of change that could be absorbed before the system’s underlying structure changes.<a name="back-2"></a><a href="#cite-2">[2]</a> He asserted that an ecological system can be highly resilient, but also exhibit a high degree of instability — and, in fact, that the proper reaction of an ecological system was to continually adapt, rather than attempt to return to a static equilibrium.[1]
Heidelbach, W. (September 28, 2016). Chestnut.</p>

<p>For example, eastern North American forests were once full of chestnut trees until a chestnut blight in the first half of the 20th century wiped them out.<a name="back-3"></a><a href="#cite-3">[3]</a> However, oak and hickory trees began spreading in its stead. The forests changed in appearance and composition, but still survived as forests.</p>

<p>Evolutionary resilience, borne from analyzing socio-ecological systems, operates under the assumption of complex systems that co-evolve, focusing on adaptation and transformation. Rejecting the idea of thresholds within which a system should fluctuate, it instead suggests multiple levels of controls and the ability to adapt the status quo by reorganizing or regenerating around the change, thereby creating a new status quo.</p>

<p>For example, communities can diversify their agricultural landscapes and production systems, designating some areas for soil conservation and organic agriculture while promoting multicropping in others. They can protect some forested areas while designating others for the community and focusing reforestation efforts there.</p>

<p>This notion of evolutionary resilience can be summarized as consisting of three central characteristics: robustness, adaptability, and transformability.<a name="back-4"></a><a href="#cite-4">[4]</a> The core notion is that in order for a complex system to be resilient, it must be able to withstand a shock, adjust so as to incur less damage, and be open to challenging previous decisions and goals.</p>

<p>I will be keeping in mind these three core characteristics in the context of information security. With robustness, you must be able to withstand an attack; with adaptability, you must be able to adjust your environment so you incur less damage when attacked; with transformability, you challenge your existing assumptions and decisions, and potentially migrate from existing infrastructure as well as defensive strategies or current methods used in the way you model and understand your threats.</p>

<figure style="float:right; max-width:40%; padding-left: 10px">
    <img src="/blog/img/resilience-03.jpg" alt="Picture of Hurricane Harvey's Eye">
    <figcaption>NASA/NOAA. (August 26, 2017). Hurricane Harvey’s Eye.<figcaption>
</figure>

<p>In most domains, robustness proved dominant in defensive strategy, and can be linked to the concept of engineering resilience — a mistake from the evolutionary resilience perspective. For example, barriers are a form of robustness, blocking storm surges. However, as seen recently with Hurricane Harvey, the primary source of damage was flooding from ongoing rain — highlighting the need for adaptability and transformability to incur less damage going forward and rethink your existing strategies.<a name="back-5"></a><a href="#cite-5">[5]</a></p>

<p>Instead, evolutionary resilience must also include adaptation and dynamic change towards the goal of preservation, with robustness as an ingredient rather than the sole objective.</p>

<p>Although the expression, “it’s not about the destination, it’s about the journey” is somewhat trite, it’s quite true for resilience. Resilience must be framed as a continuous, evolving, but sustainable process rather than a goal. As ecological economics scholar Peter Timmerman described, resilience is the building of “buffering capacity” into a system, to improve its ability to continually cope going forward.<a name="back-6"></a><a href="#cite-6">[6]</a></p>

<p>A focus only on robustness can also lead to a misleading presentation of the problem as one only based on reducing the risk itself. As in the previous example, the problem could be seen only as, “how can we withstand the hurricane?” instead of “we know the hurricane will hit us, how can we change so that it doesn’t damage our community as much?” This highlights the contrast between robustness and the adaptability and transformability characteristics, which accept that the risk will exist, and instead stress the need to reduce the potential damage from the risk and restructure around the risk.</p>

<p>Furthermore, the efforts around attack prediction represent yet another symptom of collective grief — it’s an endeavor to regain the illusion of control. I’ve given another related presentation at length about <a href="https://swagitda.com/speaking/Dangerous-Folly-Attack-Prediction-Shortridge-Art-into-Science-2018.pdf">why attack prediction should not be our goal</a>, so I will not elaborate further here. Suffice to say, prediction was attempted in other complex systems and not only failed miserably, but wasted precious time, money, and brainpower that could have been spent on a pragmatic aim: resilience — the need to design systems under the assumption the negative shock will not be predicted. As eloquently stated by Susan Elizabeth Hough<a name="back-7"></a><a href="#cite-7">[7]</a>,</p>

<blockquote>
<p>“A building doesn’t care if an earthquake or shaking was predicted or not; it will withstand the shaking, or it won’t.”</p>
</blockquote>

<p>While our industry has come to accept that there are many “unknown unknowns,” our strategy is still one based in hubris — that we can save ourselves in a breach with systems that can withstand unknown risks at unknown times with unknown faces. The evolutionary resilience approach embraces these unknowns, understanding that change is inevitable — ensuring the system survives by absorbing these unknown changes, naturally adapting and reorganizing around this unknown risk, keeping the option open of bearing its own new, unknown face.</p>

<hr />

<h2 id="robustness">Robustness</h2>

<p><img src="/blog/img/resilience-08.jpg" alt="Image of a bridge" /><em>Image by <a href="https://unsplash.com/@spoony">Hieu Vu Minh</a></em></p>

<p>Robustness involves withstanding and resisting a negative event. Engineering used the concept of resilience only in terms of robustness, measured by how long it takes a system to return to its equilibrium after a shock. However, experiencing an acute stress event implies the normal state was vulnerable to the stress, and that it is thus an “undesirable state to go back to because it would perpetuate this vulnerability.”<a name="back-8"></a><a href="#cite-8">[8]</a></p>

<p>In disaster recovery, it’s dangerous to present the problem of flooding, for example, as simply one about excess water. If it’s simply about a physical issue, then solutions are presented that are restricted to just the physical issue. In reality, flooding is a problem because of people, who understandably don’t want to lose their homes or drown. It is unnecessarily restrictive to only consider technical solutions to address the excess water, rather than broader solutions to address the problem in a societal context.<a name="back-9"></a><a href="#cite-9">[9]</a></p>

<p>When it is believed that a technical control will help prevent a shock, then it tends to lead to larger potential damage. This is called the safe development paradox.<a name="back-10"></a><a href="#cite-10">[10]</a> The reason why it’s a paradox is that the stability and presumed safety gained by building a structural mitigation to the problem actually allows risk to accumulate over time due to the false sense of security, leading to a higher chance of catastrophic consequences.</p>

<p>The safe development paradox represents a maladaptive feedback loop — once a structural mitigation is in place, more development happens where it should not.<a name="back-11"></a><a href="#cite-11">[11]</a> As the development becomes entrenched, the need for structural mitigations becomes even greater — and once the mitigation is in place, more development occurs.</p>

<figure style="float:right; max-width:40%; padding-left: 10px">
    <img src="/blog/img/resilience-04.jpg" alt="Picture of pinecones burning">
    <figcaption>Picture by <a href="https://unsplash.com/@thomasbormans">Thomas Bormans</a>.<figcaption>
</figure>

<p>When fires are suppressed in forests that are fire-adapted, fuel builds up in the form of trees or shrubs.<a name="back-12"></a><a href="#cite-12">[12]</a> As more time passes without a fire, the probability of a ruinously-intense fire grows, posing more danger to nearby human settlements. This is exactly what happened in the mid-1990s in Florida as urban development expanded into fire-adapted pine forests and enjoyed trees and shrubs in their yards.[12] The result was fires during dry periods that resulted in higher damage than usual, destroying many homes in the process.</p>

<p>In security, implementing technical controls can lead to increased damage as well. Retroactively hardening or patching legacy systems in which vulnerabilities are frequently found, can lead to further development on top of these systems, and further entrenchment of those systems within the organization. Feeling like the threat is being prevented leads to development that relies on that assumption — and thus isn’t designed to absorb an attack.</p>

<p>In flood risk management, it’s known as the “levee paradox.”<a name="back-13"></a><a href="#cite-13">[13]</a> Building a levee can lead to a sense of the problem being prevented, supporting further development and construction on the risky floodplain.[9] For example, less than 3% of people living in Illinois in floodplains with levees in place carry flood insurance.[13] The levee clearly lowers people’s awareness of the risk and ability to respond appropriately to it.<a name="back-14"></a><a href="#cite-14">[14]</a></p>

<p>When implementing a robustness control, it’s essential to ensure that it isn’t encouraging further development within a vulnerable system that leaves it open to cataclysmic risk when the control fails. Don’t focus just on resistance in your controls. Doing so will simply “treat the symptoms of bad planning with structures.”[11]</p>

<p>There’s also a lesson here for cyber insurance. Back to the levee paradox, oftentimes areas with levees in place aren’t categorized as official floodplains. This means that homes or offices in those areas don’t have flood-related insurance requirements. The clear lesson I see is: firms offering cyber insurance should consider very carefully whether they exempt companies from certain requirements based on technical controls being in place.</p>

<p>Related to the safe development paradox is the fact that preventing a system from negative exposure means that the system will only function in the artificially stable state. In the levee paradox, it actually creates an artificially stable system which can only survive in dry conditions.</p>

<figure style="float:right; max-width:40%; padding-left: 10px">
    <img src="/blog/img/resilience-05.jpg" alt="Picture of coral">
    <figcaption>Picture by <a href="https://unsplash.com/@doto">Linus Nylund</a>.<figcaption>
</figure>

<p>Another example is with coral reefs. Marine reserves are maintained to protect coral from the damaging effects of climate change, such as ocean acidification and thermally-induced coral bleaching. However, unprotected coral actually proves more resilient to climate disturbance, since they’ve faced ongoing degradation due to exposure to the stressor and thus recomposed to have more disturbance-tolerant species.<a name="back-15"></a><a href="#cite-15">[15]</a></p>

<p>In information security, you must likewise expose your systems to stressors. Even if you’re building something to be internal-only, like APIs, you should design them with the same threat model as an externally-facing service — for instance, making sure you have data sanitization. Test your systems as if they were externally-exposed, to see if they are sufficiently resilient to global stressors. If it would take years to rebuild, reconsider what data you allow within the system.</p>

<p>The overwhelming focus to date in information security has been on robustness — how to withstand or resist an attack, before rebounding back to “normal.” The traditional components of security — firewalls, anti-virus, system hardening — are all components of a robustness strategy. Even when examining how startup security products are marketed, the words “stop,” or the more creative “thwart” are used, implying an improved ability to withstand an attack.</p>

<p>Remediation even plays into this singular focus on robustness. The goal of remediation within security is to most often fix any vulnerabilities, and, ideally, to return to “business as usual” by reversing damage from an attack. As we saw with the Equifax breach, there is absolutely no chance of “business as usual” when immutable data is compromised. Penetration tests often solely focus on vulnerabilities and what is needed to fix them, rather than proposing new technologies or architecture that would prove less vulnerable long-term.</p>

<p>Other domains have typically held this singular focus, as well. The engineering-led approaches sought to defy nature itself rather than allow the system to flux with nature. For example, the single equilibrium in flood risk management is to have dry conditions in floodplains so that people can continue living in them. Dikes, storm-surge barriers, and dams are all attempts to withstand a flood, and reflect engineering resilience approaches. Their goal is to keep the same artificial equilibrium, in spite of the water system’s natural behavior.[9]</p>

<p>An engineering-only focus leads to the current challenge of companies needing to constantly stay up to date on patches, but facing many hurdles in doing so — and having this be their primary line of defense. The model that must be embraced is one in which the system can survive even if patches aren’t immediately updated, or users still click on phishing links. Your systems must survive even if users download a pdf.zip.exe.</p>

<p>As we saw with coral, without palpable vulnerability through exposure to risk, it is unlikely that resilience will develop.[11] You need to assume that attackers will gain access to a system, and figure out how to reduce the impact. You need to actually practice and embrace disaster recovery, rather than just having a plan.</p>

<p>With all that said, robustness is absolutely important to resilience. But robustness needs to be performed correctly. Drawing from flood risk management, diversity is a cornerstone of robustness — there needs to be layers of controls and diversity of solutions.[9] For example, there are storm surge barriers, dikes, and dams for flood prevention.</p>

<figure style="float:right; max-width:40%; padding-left: 10px">
    <img src="/blog/img/resilience-06.jpg" alt="Picture of New York City">
    <figcaption>Picture by <a href="https://unsplash.com/@mdisc">Michael Discenza</a>.<figcaption>
</figure>

<p>New York City has published guidelines for climate change resiliency which also recommend a combination of controls. For example, for dealing with excess heat, they recommend backup generators to hybrid-power systems, using systems with higher heat tolerance, as well as passive cooling and ventilation through window shades or high-performance glazing.<a name="back-16"></a><a href="#cite-16">[16]</a></p>

<p>Diversity of controls helps provide redundancy in uncertain conditions. When complementing measures are in place, it’s less likely that there will be catastrophic damage through the failure of a singular control. But, the tradeoff is between efficiency and effectiveness. The easier route with lower upfront costs is to implement a single control. The effective route is to implement layered controls, which may cost more now, but will pay dividends in reduced consequences long-term.</p>

<p>I don’t believe this will be a new concept for many of you. For example, you could deploy a so-called APT-blocking appliance (aka the BlinkyBoxTM) on your network that purports to stop all attacks. However, what then happens when legitimate credentials are used to access a cloud-based service? Or, as we’ve seen recently with Kaspersky, what happens when the APT-blocking-box is hacked by the APT itself to gain access?</p>

<p>Diversity can also be seen through the lens of systems. While we think of fragmentation generally as poor to have, particularly in the context of asset management, there is an argument in its favor. Shared hosting providers can increase correlated risk. If there is a breach at one provider, or vulnerability in a key component or library used across all applications, then your risk exposure is far greater than it might have been otherwise.</p>

<p>The financial crisis in 2008 serves as a pertinent example of the dangers of ignoring correlated risk. There is something to be said for ensuring you have some level of diversity in your architecture. I am by no means the first to suggest that heterogeneity is important — Dan Geer was fired from @stake in 2003 for making that suggestion, specifically in regards to Microsoft’s hegemony.<a name="back-17"></a><a href="#cite-17">[17]</a></p>

<p>This sort of diversity also plays into the efficiency vs. effectiveness tradeoff. However, efficiency can actually lead to a more limited space in which you can operate. Being able to function using fragmented technologies and controls will ensure you can adapt much better to uncertainty. Systems diversity, through this lens, can provide the instability that can ensure survival. I posit that it is up for debate whether it is more optimal to have manageability through uniformity or limited impact of any one stressor through diversity.</p>

<p>Thinking in decision trees can help ensure robustness through proper diversity of controls. I’ve discussed decision trees towards information security strategy in prior talks, <a href="https://swagitda.com/speaking/us-17-Shortridge-Big-Game-Theory-Hunting.pdf">most notably at Black Hat</a>. Briefly, the goal should be to <a href="/blog/posts/choice-architecture-infosec-blue-teams">walk through what steps</a> an attacker would take to reach their goal in your organization. Naturally, there is not just one path an attacker will take; you have to consider what path they will take if they encounter a mitigation as well. From there, you can begin determining what cascading controls are necessary in order to raise the cost to the attacker as much as possible.</p>

<p>Raising the cost to the attacker serves as a bridge between robustness and adaptability. As frequently referenced, Dino Dai Zovi said, “Attackers will take the least cost path through an attack graph from their start node to their goal node.”<a name="back-18"></a><a href="#cite-18">[18]</a> If you can raise attacker cost, you can begin deterring attackers. Attackers will need greater resources and a greater level of sophistication if you do so. One way to raise cost is through robustness with strong, diversified controls. Another way is through adaptability.</p>

<hr />

<h2 id="adaptability">Adaptability</h2>

<p><img src="/blog/img/resilience-09.jpg" alt="Image of a chameleon" /><em>Image by <a href="https://unsplash.com/@_cecilencieux">Cécile Brasseur</a></em></p>

<p>Adaptability concerns reducing the costs of damage incurred and keeping your options open to support transformability. The evolutionary approach is one in which the assumption is that conditions will naturally change over time, and thus the system itself needs to incur long-term change. Reversion to the preexisting state is not necessarily — and often wholly — undesirable.</p>

<p>The Intergovernmental Panel on Climate Change (IPCC) highlights the need for realism and warns about the dangers of incremental changes under the guise of adaptation.<a name="back-19"></a><a href="#cite-19">[19]</a> They specifically recommend questioning underlying assumptions and existing structures, acknowledging the inevitability of macro-level change, and making managed transformation the goal. Pretending you’re adapting while only undergoing incremental change creates a false sense of security — similar to the safe development paradox. You may alleviate symptoms in the short-term, but you can only cultivate resilience through meaningful change towards adapting to reality.</p>

<figure style="float:right; max-width:40%; padding-left: 10px">
    <img src="/blog/img/resilience-07.jpg" alt="Picture of a panda">
    <figcaption>Picture by <a href="https://unsplash.com/@djmle29n">Debbie Molle</a>.<figcaption>
</figure>

<p>A macro-level example of adaptability is in the realm of climate change. While traditional protection strategies for wildlife at risk due to climate change have been focused on preserving their existing habitats, more recent research proposes alternative approaches. Protected areas are in static locations, and tend to become increasingly isolated, leaving nowhere to go. Preserving a species in such an isolated, at-risk area results in “genetic ghettos.”<a name="back-20"></a><a href="#cite-20">[20]</a> The species becomes increasingly acclimated to this limited environment, which consequently staves off any potential for evolutionary adaptation.</p>

<p>Instead, wildlife naturally has shifted ranges in response to previous instances of climate change, in which preferable conditions are “tracked.” Recommendations now include helping connect disparate ecosystems together so that wildlife can more easily migrate. For example, in areas with urban environments, a narrow strip of land can be preserved, or another sort of route created, so that populations can connect to a different climactic area.</p>

<p>One can think of existing territories like legacy systems. We try to “preserve” these habitats through patching and retroactive hardening. The adaptive model from nature is to move to new territories that fit preferred conditions — ones in which the species can survive — which is similar, in effect, to moving to new infrastructure or a new mode of operation that is more resilient to the new threat.</p>

<p>As a highly tangible example, consider the case of database queries. The organization’s status quo might be that they use inline PHP code within the HTML of their web apps to perform database queries. If an injection vulnerability is discovered in an instance of this inline PHP code, they’ll fix that instance, but likely not conduct a full review across all of their inline PHP code. In this case, they’d be improving robustness by patching the code, but they’d be returning to their so-called “stable equilibrium.”</p>

<p>In contrast, embracing adaptability would mean the organization should instead remove inline queries, and use one class that accesses the database. This one class would be completely responsible for all sanitization. The result is not only that now you only have to fix issues in one place, but also that developer turnover can be managed — rather than writing their own new inline code, they can use the new library that you’ve built instead.</p>

<p>Preservation can also lead to misleading indicators of resilience. For example, static measurements such as high coral cover or fish abundance can be poor indicators of coral reef resilience.<a name="back-21"></a><a href="#cite-21">[21]</a> These measures can just reflect favorable conditions in the past and not accurately reflect when resilience is being eroded.</p>

<p>Likewise, in information security, organizations using a library with no known vulnerabilities may currently treat their security model as complete and not perform continuous revisions. The issue is that the release of new vulnerabilities or attacker methods is not always well-publicized. Instead, organizations should frequently review their security posture to ensure threat models are not based on past favorable conditions, even if the product does not change. As a recent example, you likely had to update your threat models after the release of <a href="https://en.wikipedia.org/wiki/EternalBlue">EternalBlue</a> — but it was still privately operational well before disclosure.</p>

<p>In the realm of climate change, moving members of a species that are used to warm areas to intermingle with its kind who live in colder locations can help the cold-adapted population actually survive long-term.[20] Applications built using legacy systems and libraries which have never been exposed to the outside world, which suddenly need exposure to external APIs, tend to fare extremely poorly in security terms.</p>

<p>As mentioned in the example of unprotected coral, the lack of the system’s exposure to the threat over their lifespan has led them to exist in a weakened, unpatched state. Security-wise, you should intermingle your internally-facing systems with your externally-facing systems to ensure they meet the standards of the evolving “global” threat model.</p>

<p>The goal for cities in the face of natural disasters is to maintain a flexible approach in order to properly adapt their response to the changing nature of their risks. If cities do not cultivate a process which assumes uncertainty and surprise in their model, then it’s safe to say they are being wholly unrealistic about the ways of the world.</p>

<p>As defenders, you should test attacker playbooks against yourself to determine how quickly you can adapt to attacker methods. I’m sure many of you wish you could have in-house red teams. For those who do have them, use them to your advantage in this way. I mentioned decision trees earlier as a way to determine which diverse set of controls to use — have your red teams map out the decision trees they created during their course of action to add realistic data into your own trees.</p>

<p>You also must test your ability to absorb the impact of an attack, and minimize the damage. One such test is through failure injection. <a href="https://github.com/Netflix/SimianArmy/wiki/Chaos-Monkey">Chaos Monkey</a>, part of Netflix’s suite of tools called the “Simian Army,” is a service which randomly kills instances in order to test their ability to withstand failure. In fact, Chaos Monkey is described as a resiliency tool.</p>

<p>While it was designed with a performance use case in mind, it can be repurposed for security. If your infrastructure is continually fluctuating, with instances killed at random, it makes it exceptionally difficult for attackers to persist. Attackers would have to conduct whatever they needed within an uncertain time frame. This is, of course, not impossible, but it absolutely raises the attacker’s cost and level of skill required.</p>

<p>Netflix’s goal with Chaos Monkey is to “design a cloud architecture where individual components can fail without affecting the availability of the entire system.”<a name="back-22"></a><a href="#cite-22">[22]</a> Defenders should make it their goal to design a security architecture where individual controls can fail without affecting the security of the entire system. As I mentioned earlier, if your system becomes completely compromised because a user clicks on a malicious link, you must rethink your security architecture.</p>

<p>Rethinking security architecture is no easy feat. Defenders are considerably hindered in their ability to be adaptive and flexible. Most commonly, people think of organizational pressure as the key deterrent, but I would argue the infosec industry itself is the primary limiter. Defenders face an overwhelming level of complexity and uncertainty due to the sheer number of security vendors and the fragmentation of the solution space.</p>

<p>I believe some of the challenges can be solved by changing the types of infrastructure that are used to promote adaptability and support transformability. Deploying Chaos Monkey is one such example centered on adaptability, but a grander example that blends into transformability is using a container-based ecosystem.</p>

<p>Many of you have likely heard of the container revolution, though may not have used them yourselves. While I’m not a container expert, I’ll explain why containers are a natural fit for evolutionary resilience. Jess Frazelle — “the Keyser Söze of containers”— highlighted in her DevOpsDays talk that containers represent potential salvation from the tradeoff between usability and security.<a name="back-23"></a><a href="#cite-23">[23]</a> I believe she’s absolutely correct.</p>

<p>As per Microsoft, containers are “a way to wrap up an application in its own isolated box” and are “an isolated, resource-controlled, and portable runtime environment.”<a name="back-24"></a><a href="#cite-24">[24]</a> A container serves as a layer of abstraction between an application and the host server — which can be of any kind, whether virtualized or bare metal. Because of this, it allows for easier migration to and from underlying infrastructure without having to rebuild applications.</p>

<p>The most common buzzwords I hear for containers are flexibility, portability, and scalability, making them a natural fit for both the adaptability and transformability characteristics. Just as attackers need repeatability and scalability, so do defenders — as well as something that can adapt over time to changes in attacker methods. It cannot be overstated how much a container environment bolsters flexibility and flattens complexity.</p>

<p>When something goes wrong — whether security related or not — the legacy approach makes determining the root cause an effort in untangling and dependency management. With containers, verifiably working systems are available in one neat package, facilitating far less messy remediation. Even implementing them into existing legacy systems can help more easily manage dependencies and licenses.</p>

<p>In the vein of Chaos Monkey, if applications are attacked while running inside a container, all that must be done is kill the container and restart it. There is no need for vulnerability scanning, firewalls, anti-virus, and all the other fragments of the security solution space. You can instead isolate and shut down infected containers as it happens.</p>

<p><figure style="float:right; max-width:40%; padding-left: 10px">
    <img src="/blog/img/resilience-11.jpg" alt="Picture of trash on a beach">
    <figcaption>Picture by <a href="https://unsplash.com/@dwoodhouse">Dustan Woodhouse</a>.<figcaption>
</figure>
You have to ensure adaptability to manage resilience erosion as well. In the case of coral, there are “pulse-type stressors,” or acute stressors, which include tropical cyclones, coral bleaching events, and destructive fishing.[21] But there are also “press-type stressors,” which are stressors occurring over longer periods of time, such as pollution, sedimentation, overfishing, ocean warming, and acidification. With enough of the press-type stressors wearing it down, coral reef resilience is overwhelmed when a pulse-type stressor occurs.</p>

<p>Pulse-type stressors in information security can be thought of as new vulnerabilities or a new data breach. Press-type stressors can include large turnover of employees — particularly ones working on large projects — but I would say the most prevalent is complexity. As you add complexity to your applications and systems, it becomes more difficult to test every possible path to compromise, because the paths begin trending towards infinity. If you can no longer test every path because your system is too complex, you have eroded your resilience, a key part of which is flexibility — and will have neither adaptability or transformability.</p>

<hr />

<h2 id="transformability">Transformability</h2>

<p><img src="/blog/img/resilience-10.jpg" alt="Image of a butterfly" /><em>Image by <a href="https://unsplash.com/@erinw">Erin Wilson</a></em></p>

<p>Transformability can be thought of challenging your existing assumptions and reorganizing your system.</p>

<p>Returning to our previous example of an organization removing inline PHP database queries in favor of a single class, the latter approach also bolsters transformability. Because it is just one library, it allows for easier migration as-needed depending on how the company’s environment, or the threat environment, changes. You are not leaving your options open when your web app is riddled with inline code. You must be able to review and revise your previous choices — for example by moving to new tools or libraries.</p>

<p>Research from other domains has explored the policy implications of transformability, and how to implement the concept on a practical level. Disaster recovery in urban areas is one of the most well-researched domains in this regard. Given urban areas are dynamic systems, evolutionary resilience suggests that policy should encourage recovery efforts that prioritize re-building the urban area into an improved — or even better, optimized — system.[8] For example, in flood-prone areas, the policy should be to change the location and not build in those areas, while also implementing flood-proof construction for periphery areas.</p>

<p><figure style="float:right; max-width:40%; padding-left: 10px">
    <img src="/blog/img/resilience-12.jpg" alt="Picture of Christchurch Cathedral">
    <figcaption>NZ Defence Force. (February 23, 2011). Christchurch Cathedral.<figcaption>
</figure>
As a tangible example of transformability, let’s explore Christchurch. In 2011, a devastating magnitude 6.3 earthquake hit Christchurch, the second most populous city in New Zealand at the time. It killed 185 people and damaged over 100,000 houses, with a financial cost to rebuild estimated at over $40 billion.</p>

<p>After the quake, the Canterbury Earthquake Recovery Authority (CERA) designated a new “red zone” throughout the area. This red zone includes damaged or vulnerable land where they believe rebuilding would be “prolonged and uneconomic.”<a name="back-25"></a><a href="#cite-25">[25]</a> The assessment embraces transformability, rejecting the need to return to the status quo, and instead challenging the assumption that there should be buildings on the land at all.</p>

<p>As security professionals, you should work to identify what the red zones are within your IT systems. Organizations should identify which infrastructure or technologies present the most security challenges — whether through vulnerabilities or ongoing maintenance costs — and put them in the red zone for being phased out.</p>

<p>Defining the components of your own red zone calculation will be subjective, but I submit the following as potential criteria — systems that are directly exposed to external attacks, or entirely public facing, in particular:</p>

<ul>
<li>Those which expose complex or critical functionality and are accessible publicly</li>
<li>Newly deployed systems or architectures, particularly those developed by inexperienced professionals</li>
<li>Legacy systems using outdated libraries, software, or languages</li>
<li>Systems with no backups, or which can’t easily be restored</li>
<li>Any system with critical personally identifiable information (PII) or immutable data — such as in Equifax’s case</li>
<li>Systems with privileged access to other systems or accounts</li>
<li>Any system that has known or “accepted” risk associated with it</li>
<li>Easily fingerprintable or overly verbose systems</li>
<li>Anything that could be deemed a single point of failure for your organization</li>
<li>Systems that are prohibitive to patch or update</li>
</ul>

<p>Defining your security red zones isn’t about examining potential vulnerabilities or path to compromise in each of your systems. Instead, you want to identify any assets that fall under the red zone criteria, and attempt to move them out of the zone, into healthier systems.</p>

<p>For example, an organization might have an existing asset built using legacy technology, which now has to be exposed to public APIs. Furthermore, this asset consumes critical data and also has privileged access to backend APIs. This asset should likely be classified as being in the “red zone,” without actually assessing whether or not there are vulnerabilities. The goal is to move or rebuild the asset outside of the red zone and make it a safer system.</p>

<p>In this case, such measures could include:</p>

<ul>
<li>Locking down public exposure so that it’s only accessible via VPN</li>
<li>Rebuilding the asset using newer, non-legacy technologies (such as containers)</li>
<li>Avoiding storing critical data on this asset and proxy encrypted data further into the architecture’s core</li>
<li>Introducing security logging and monitoring</li>
<li>Locking down privileged access and enforcing the principle of least privilege</li>
</ul>

<p>By implementing all, or at least some, of these, the system would no longer be in the red zone. This would be similar to moving a power plant out of a flood plain, and instead building it in an elevated area with fortified materials and an early warning system.</p>

<p>Using the example of levees, researchers have proposed having planned decommission of levees ahead of known maintenance hurdles. This way, levees can be used as a stop-gap as communities embrace transformability and relocate. It’s unrealistic to assume that a community could uproot overnight, but it’s important that it isn’t treated as a permanent Band-Aid.</p>

<p>In security, it’s similarly unrealistic to assume you can transform overnight — and your organization would probably not be pleased with you. But, you need to be able to migrate. Like levees, you could have planned decommission of retroactive hardening and patching before moving off of legacy systems. This ensures you don’t renew on software or hardware before it becomes too embedded in your organization or costly to maintain.</p>

<p>In general, you should have plans in place to decommission technologies that will eventually be obsolete or replaced, even down to libraries, hashes, and software versions. Continually consider how you can prepare in advance for migration.</p>

<p>Evolutionary resilience research has highlighted the need for more collaborative planning across stakeholders in a complex system. Rather than relying on inferred knowledge towards a pre-defined goal, local groups should work together and compile their own information towards optimizing relevant processes in their system.</p>

<p>Drawing from flood risk management, those in charge of risk management should be the ones to communicate what realistic level of protection each sort of approach provides.[9] Otherwise, it’s difficult for communities to cultivate knowledge on their own of what protection is in place and what limitations remain for risk reduction.</p>

<p>The security function within an organization is also not an isolated unit. Security should foster collaboration across the business towards optimizing security processes. You all should also be open in sharing knowledge of what protections are in place, what risks the protection realistically reduces, what risk remains, and any uncertainties around the approach.</p>

<p>The most obvious group with whom security should partner is engineering, which I discuss further in another keynote, <a href="/blog/posts/security-as-a-product">Security as Product</a>. As is in the aforementioned case of containers, it’s possible that there are improvements engineering desires that might facilitate more adaptable security as well. The trend towards flexibility is perhaps the strongest in software development and engineering today, and security must embrace this trend as well. I highly recommend reading the recently released O’Reilly book on Agile Application Security for more in this vein.<a name="back-26"></a><a href="#cite-26">[26]</a></p>

<p>In this collaborative setting, the role of planners should be to manage transitions between states rather than create or mediate.<a name="back-27"></a><a href="#cite-27">[27]</a> I don’t think security professionals can fully remove the need to create or implement solutions to some extent. Focusing on managing the transition from the current state to a more secure state can potentially reduce some labor burden, however.</p>

<p>For example, drawing again on the transition to containers, the engineering group will conduct the majority of the labor towards this endeavor. Security should manage the project to ensure necessary controls are in place, such as detection of container compromise.</p>

<p><figure style="float:right; max-width:40%; padding-left: 10px">
    <img src="/blog/img/resilience-13.jpg" alt="Picture of people talking about tech">
    <figcaption>Image by <a href="https://unsplash.com/@nesabymakers">NESA by Makers</a><figcaption>
</figure>
As another example, security ideally should work with engineering to implement solutions like two-factor authentication. John “Four” Flynn at Facebook gave a great talk a few years ago about implementing 2FA at Facebook. Their goal was to put 2FA on SSH to make it hard for attackers to pivot into Facebook’s production environment.<a name="back-28"></a><a href="#cite-28">[28]</a> During the process, they thought very carefully about the user experience, realizing they needed to support frequent use, allow for a flexible range of factors, and minimize help desk requests.</p>

<p>They decided to use DuoSecurity and YubiKey Nano — with the YubiKey, the developers only needed to touch the side of their laptop to SSH, and DuoSec’s cloud-based tokens ensured they’d still have access even if they lost the YubiKey. One of their key discoveries during this project was that:</p>

<blockquote>
<p>“You can actually implement security controls that affect every single thing people are doing and still make them love it in the process.”</p>
</blockquote>

<p>I recognize that while “software is eating the world,” not every company is yet a technology company. Potentially, you will have a limited IT group with whom to collaborate on technical efforts. This doesn’t mean you can’t collaborate, however. It’s well recognized that there is division between even security and risk or fraud groups, let alone general counsel and financial functions. There is someone at your organization who wants their job to be easier. Your job then needs to be how security can make that happen, or at least fit into their existing workflows.</p>

<p>For this sort of transformability to happen, responsive governance systems are needed. Defenders must implement decision-making processes that are quick to identify and respond to emerging threats. Part of this in ensuring that your organization is learning from prior experiences — such as through the decision tree process I mentioned before, in which you can update your models after a breach.</p>

<p>However, your organization’s entire community must be involved in this learning process and be prepared to continually evaluate strategy. Implementing a security culture in your organization is perhaps the best chance of doing so.</p>

<hr />

<h2 id="conclusion">Conclusion</h2>

<p>This is my humble attempt at a definition of resilience in information security:</p>

<blockquote>
<p><strong>Resilience in security means a flexible system that can absorb an attack and reorganize around the threat.</strong></p>
</blockquote>

<p>The system, in this case, likely is your organization, although this can apply to its underlying systems as well. Crucially, resilience in security is not the ability to withstand or prevent an attack. That’s the blue pill.</p>

<p>The red pill is that the reality is that attacks will happen to you, and you must architect your security around this fact. I showed you how deep the rabbit hole goes on what security strategy fits this reality. Robustness, adaptability, and transformability are the keys to survival in Wonderland.</p>

<p><strong>Robustness</strong>, while not the silver bullet, should be optimized through diversity of controls. <strong>Adaptability</strong> seeks to minimize the impact of an attack and keep your options open, and new types of infrastructure, such as containers, can enable it. <strong>Transformability</strong> demands you challenge your assumptions and reorganize your system around the reality — a reality that affects communities, which requires a collaborative effort.</p>

<p>My favorite fictional character growing up was Ian Malcolm, from Michael Crichton’s “Jurassic Park” novels. I believe the full quote of one of his most notable lines summarizes how you, as defenders, should think of your strategy <a name="back-29"></a><a href="#cite-29">[29]</a>:</p>

<blockquote>
<p>“Because the history of evolution is that life escapes all barriers. Life breaks free. Life expands to new territories. Painfully, perhaps even dangerously. But life finds a way.”</p>
</blockquote>

<p>Consider how you can escape barriers, consider how you can expand to new territories, consider how you can find a way to evolve — because attackers are doing all of these things. Doing so will likely be painful at first for your organization. Your job, as part of implementing transformability, is to manage these transitions and minimize the pain and danger. Much like with life, as per Malcolm’s quote, you can think of it as the survival of your organization’s data being at stake.</p>

<p><figure style="float:right; max-width:40%; padding-left: 10px">
    <img src="/blog/img/resilience-14.jpg" alt="Picture of a cat wearing a firefighter hat">
</figure>
Attacks will happen. Attackers will continue to evolve their methods. We can evolve our methods, too. We face a choice, as an industry: we can either continue to indulge ourselves in anger, bargaining, and depression, or strive towards acceptance.</p>

<p>If we take this red pill of resilience, we can defend ourselves effectively and realistically. If we take the blue pill, we will keep attempting to rebound to an artificial equilibrium — relegating us to the role of a firefighting cat who is drunk on snake oil.</p>

<p>I am certain most of you are fed up with these dynamics. Instead of accepting snake oil, I encourage you to take the red pill of resilience instead.</p>

<hr />

<h2 id="references">References</h2>

<p><a name="cite-1"></a><a href="#back-1">[1]</a> Alexander, D. E. (2013). Resilience and disaster risk reduction: an etymological journey. Natural hazards and earth system sciences, 13(11), 2707–2716.</p>

<p><a name="cite-2"></a><a href="#back-2">[2]</a> Holling, C. S. (1996). Engineering resilience versus ecological resilience. Engineering within ecological constraints, 31(1996), 32.</p>

<p><a name="cite-3"></a><a href="#back-3">[3]</a> The American Chestnut Foundation. How Chestnut Blight Devastated the American Chestnut. Retrieved from <a href="https://www.acf.org/the-american-chestnut/">https://www.acf.org/the-american-chestnut/</a> (accessed September 2017).</p>

<p><a name="cite-4"></a><a href="#back-4">[4]</a> Restemeyer, B., Woltjer, J., &amp; van den Brink, M. (2015). A strategy-based framework for assessing the flood resilience of cities–A Hamburg case study. Planning Theory &amp; Practice, 16(1), 45–62.</p>

<p><a name="cite-5"></a><a href="#back-5">[5]</a> Blake, E.S. &amp; Zelinsky, D.A. (2018). National Hurricane Center Tropical Cyclone Report: Hurricane Harvey. National Oceanic and Atmospheric Administration.</p>

<p><a name="cite-6"></a><a href="#back-6">[6]</a> Timmermann, P. (1981). Vulnerability, resilience and the collapse of society. Environmental Monograph, 1, 1–42.</p>

<p><a name="cite-7"></a><a href="#back-7">[7]</a> Hough, S. E. (2016). Predicting the unpredictable: the tumultuous science of earthquake prediction. Princeton University Press.</p>

<p><a name="cite-8"></a><a href="#back-8">[8]</a> Sanchez, A. X., Osmond, P., &amp; van der Heijden, J. (2017). Are some forms of resilience more sustainable than others?. Procedia engineering, 180, 881–889.</p>

<p><a name="cite-9"></a><a href="#back-9">[9]</a> Tempels, B. (2016). Flood resilience: a co-evolutionary approach. Residents, spatial developments and flood risk management in the Dender basin.</p>

<p><a name="cite-10"></a><a href="#back-10">[10]</a> Burby, R. J. (2006). Hurricane Katrina and the paradoxes of government disaster policy: bringing about wise governmental decisions for hazardous areas. The Annals of the American Academy of Political and Social Science, 604(1), 171–191.</p>

<p><a name="cite-11"></a><a href="#back-11">[11]</a> Wenger, C. (2017). The oak or the reed: how resilience theories are translated into disaster management policies. Ecology and Society 22(3):18.</p>

<p><a name="cite-12"></a><a href="#back-12">[12]</a> Gunderson, L. (2010). Ecological and Human Community Resilience in Response to Natural Disasters. Ecology and Society 15(2): 18.</p>

<p><a name="cite-13"></a><a href="#back-13">[13]</a> Martindale, B., &amp; Osman P. (2007) Why the concerns with levees? They’re safe, right?. IASFM Fall 2007 Newsletter.</p>

<p><a name="cite-14"></a><a href="#back-14">[14]</a> Liao, K. H. (2012). A theory on urban resilience to floods — a basis for alternative planning practices. Ecology and Society 17(4): 48.</p>

<p><a name="cite-15"></a><a href="#back-15">[15]</a> Côté, I. M., &amp; Darling, E. S. (2010). Rethinking ecosystem resilience in the face of climate change. PLoS biology, 8(7), e1000438.</p>

<p><a name="cite-16"></a><a href="#back-16">[16]</a> NYC Mayor’s Office of Recovery and Resiliency. (2018). Climate Resiliency Design Guidelines.</p>

<p><a name="cite-17"></a><a href="#back-17">[17]</a> Verton, D. (October 1, 2003). Former @stake CTO Dan Geer on Microsoft report, firing. Retrieved from <a href="https://www.computerworld.com/article/2572315/security0/former--stake-cto-dan-geer-on-microsoft-report--firing.html">https://www.computerworld.com/article/2572315/security0/former--stake-cto-dan-geer-on-microsoft-report--firing.html</a></p>

<p><a name="cite-18"></a><a href="#back-18">[18]</a> Dai Zovi, D. Attacker “Math” 101.</p>

<p><a name="cite-19"></a><a href="#back-19">[19]</a> Intergovernmental Panel on Climate Change. (2014). Climate Change 2014 Synthesis Report Summary for Policymakers.</p>

<p><a name="cite-20"></a><a href="#back-20">[20]</a> Sgro, C. M., Lowe, A. J., &amp; Hoffmann, A. A. (2011). Building evolutionary resilience for conserving biodiversity under climate change. Evolutionary Applications, 4(2), 326–337.</p>

<p><a name="cite-21"></a><a href="#back-21">[21]</a> Anthony, K. R., Marshall, P. A., Abdulla, A., Beeden, R., Bergh, C., Black, R., … &amp; Green, A. (2015). Operationalizing resilience for adaptive coral reef management under global environmental change. Global change biology, 21(1), 48–61.</p>

<p><a name="cite-22"></a><a href="#back-22">[22]</a> Izrailevsky, Y., &amp; Tseitlin A. (July 18, 2011). The Netflix Simian Army. Retrieved from <a href="https://medium.com/netflix-techblog/the-netflix-simian-army-16e57fbab116">https://medium.com/netflix-techblog/the-netflix-simian-army-16e57fbab116</a>.</p>

<p><a name="cite-23"></a><a href="#back-23">[23]</a> Frazelle, J. (July 27, 2017). A Rant on Usable Security. Retrieved from <a href="https://blog.jessfraz.com/post/a-rant-on-usable-security/">https://blog.jessfraz.com/post/a-rant-on-usable-security/</a></p>

<p><a name="cite-24"></a><a href="#back-24">[24]</a> Brown, T., et al. Windows Containers. Retrieved from <a href="https://docs.microsoft.com/en-us/virtualization/windowscontainers/about/">https://docs.microsoft.com/en-us/virtualization/windowscontainers/about/</a> (accessed October 2017).</p>

<p><a name="cite-25"></a><a href="#back-25">[25]</a> Blundell, S. (April 19, 2016). Christchurch’s Game of Zones. Retrieved from <a href="https://www.noted.co.nz/currently/social-issues/christchurchs-game-of-zones/">https://www.noted.co.nz/currently/social-issues/christchurchs-game-of-zones/</a></p>

<p><a name="cite-26"></a><a href="#back-26">[26]</a> Bell, L., Bird, J., Brunton-Spall, M., Smith, R. (2017). Agile Application Security. O’Reilly Media.</p>

<p><a name="cite-27"></a><a href="#back-27">[27]</a> Batty, M. (2013). Complexity and Planning: Systems, Assemblages and Simulations, edited by Gert de Roo, Jean Hillier, and Joris van Wezemael. 2012. Farnham, UK and Burlington, Vermont: Ashgate Publishing. 443+ xviii. Journal of Regional Science, 53(4), 724–727.</p>

<p><a name="cite-28"></a><a href="#back-28">[28]</a> Flynn, J. (February 6, 2014). 2FAC: Facebook’s Internal Multi-factor Auth Platform — Security @ Scale 2014. Retrieved from <a href="https://www.youtube.com/watch?v=pY4FBGI7bHM">https://www.youtube.com/watch?v=pY4FBGI7bHM</a></p>

<p><a name="cite-29"></a><a href="#back-29">[29]</a> Crichton, M. (1990). Jurassic Park. Random House.</p>
]]></content>
        </item>
        
        <item>
            <title>Security as a Product</title>
            <link>//swagitda.com/blog/posts/security-as-a-product/</link>
            <pubDate>Fri, 18 May 2018 17:15:09 -0400</pubDate>
            
            <guid>//swagitda.com/blog/posts/security-as-a-product/</guid>
            <description>Originally given as the keynote at BSides Knoxville.
Security is a product, but we treat it like a sacred, immutable grail to preserve, unblemished by the sublunary needs of users. And yet, we wonder why defense remains stagnant, why we fail so consistently in progressing towards the glorious ideal of a “secure organization.” We will continue to fail — unless we treat security as a product. Are we trying to respect the phantasmal Elder Deities of Infosec and their stringent doctrine, or are we trying to ensure our organization can still thrive while operating in a perilous digital world?</description>
            <content type="html"><![CDATA[<p><em>Originally given as <a href="https://www.youtube.com/watch?v=Ia80fg7ivN4">the keynote</a> at BSides Knoxville.</em></p>

<p><img src="/blog/img/ball-of-lights.jpeg" alt="person clutching a ball of lights" /></p>

<p>Security is a product, but we treat it like a sacred, immutable grail to preserve, unblemished by the sublunary needs of users. And yet, we wonder why defense remains stagnant, why we fail so consistently in progressing towards the glorious ideal of a “secure organization.” We will continue to fail — unless we treat security as a product. Are we trying to respect the phantasmal Elder Deities of Infosec and their stringent doctrine, or are we trying to ensure our organization can still thrive while operating in a perilous digital world?</p>

<p>One definition of a product I prefer is “something created through a process that provides benefits to a market.” Security as product, therefore, is created through a process that provides benefits to a market — in this case, the organization in which it operates. The somewhat religious belief I hear espoused is that designing security to benefit your organization will result in a blasphemous mimicry of true security. That couldn’t be further from the truth. It’s a mimicry of your duty as a security professional to follow your personal beliefs rather than pursue strategies that benefit your organization.</p>

<p>But perhaps you don’t believe me. You think there’s some level of objective “truth” that is foolish to discard in the name of benefitting your organization. Whatever that truth is, that’s now your product, and if it doesn’t benefit your organization, you’re attempting to sell it into a market that doesn’t want it.</p>

<p>I’m often left perplexed at how some security professionals can see victory in forcing through a change that users viscerally dislike, as if their dissatisfaction represents a blood sacrifice. How is that possibly success? Success is solving a real problem in a way that delivers consistent value. Success is fostering consensus so that you are supported by the organization in effecting meaningful change — even if you implement something adding to your customer’s burden.</p>

<p>For example, when requiring multi-round hashing rather than storing credentials in plaintext, relevant stakeholders in your organization must be included and understand the need for a noble sacrifice. You will fail if the security of your organization rests on users adopting a strategy that neither provides them value, nor is one they support.</p>

<p>As Sarah Jamie Lewis insightfully tweeted:</p>

<div class="center">
    <blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">&quot;our software is secure if you use it correctly&quot; means &quot;our software is not secure&quot;</p>&mdash; Sarah Jamie Lewis (@SarahJamieLewis) <a href="https://twitter.com/SarahJamieLewis/status/996033014269296640?ref_src=twsrc%5Etfw">May 14, 2018</a></blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>

<p>Similarly, <em>if</em> your organization is secure if users follow your security policies correctly, your organization is not secure. Maintaining the dogmatic view that it’s “the users that must be wrong,” rather than accepting the situation for what it is — the failure of your security program — is why we continue to fail. How many more years are we going to lament that our users are poor at security before we actually start working on pragmatic solutions?</p>

<p>Pragmatism doesn’t require security sacrilege. All products, including security, are shared problems within an organization. Each stakeholder must feel they have a personal stake in whatever course of action is taken — a process called building consensus.</p>

<p>As a product manager, there are times when we will release changes or new features that may be contentious. If I pursue a strategy without regard for how my colleagues who interact with customers or potential customers every day feel, the product won’t succeed in the market, because my colleagues won’t have the confidence to sell it. If I come to my colleagues with evidence for why it is necessary, describe how it works towards the broader product vision, and actively listen to their concerns, we can design a strategy collectively in which all parties are confident — even in the face of uncomfortable change.</p>

<p>Security as a product doesn’t require the wearing down of strategies through compromise until they are rendered ineffective. It requires a purposeful strategy through an overarching vision of how security can support the organization’s survival in light of the fact that computers are somewhat terrible, but necessary for success.</p>

<p>At this point, I’ve mastered a stolid expression for when security professionals nonchalantly explain the improvisational nature of their strategy-making. Most assume I’m asking whether they have a strategy for a specific project and seem surprised when I ask if they’ve defined their long-term vision for their overall security program. In the same conversation, the CISO or security engineer with whom I’m speaking will unleash a passionate rant — perhaps you have heard some of these grievances, or uttered them yourself:</p>

<ul>
<li>“Things do sometimes sort of get accomplished… but slowly.”</li>
<li>“We don’t ever actually make progress, we’re just running around.”</li>
<li>“We keep making the same mistakes over and over — it doesn’t get better.”</li>
<li>“I don’t have any time to do research, I’m constantly in meetings where we don’t actually get anything done.”</li>
<li>“I just don’t even give a shit anymore, nothing changes.”</li>
</ul>

<p>I really do feel for your plight, but y’all can be tedious. There is clearly something amiss here that better or more tech nor people can fix — they will simply be likewise wasted. I hear these remonstrances nearly everywhere in infosec — from the smallest of teams to teams sprawling over multiple functional areas at Fortune 500 companies. There are countless passionate people working tirelessly, whom consistently feel like they aren’t accomplishing anything that is meaningfully improving security.</p>

<p>What is perhaps even worse is hearing that security teams have adopted “agile” methodology, then discovering that their tasks are based on the whims of the individual, the epics are ill-defined and focused on functional areas, and no one is looking at a higher level to see how many resources are being dedicated towards each effort.</p>

<p>What’s even more jarring is every time I play surrogate therapist — asking probing questions to discern why their teams are so inefficient at a macro level — they unabashedly disclose that their teams don’t have any overarching goals defined, let alone metrics to track progress.</p>

<p>And we remain shocked that we aren’t progressing?</p>

<hr />

<p>Of the “three pillars” of infosec — people, processes, and technology — I believe processes are most ignored and undervalued. I’ve grown exhausted by the number of articles about the “cyber skills shortage” as well as listening to — and speaking about — the pernicious complexity and misguidedness of the security technology space. Yet I don’t see nearly the same volume of fiery headlines and hot takes on Twitter about how our processes are failing us, despite the fact that processes are the underpinnings of how people work together and with technology.</p>

<p>As an example, I’ve been amazed at what our customers accomplished with Excel and two people prior to adopting our solution — managing vendor risk programs covering thousands of vendors across many lines of business. A trait in common with each of those customers succeeding despite their people and technology constraints is how easily they can articulate their process — and it’s because they’ve comprehensively defined it.</p>

<p>A process is “a series of actions or steps taken in order to achieve a particular end.” You can have the best people and the best technology, but if you cannot define to what end they can be used, and how they can be used, success is unlikely to manifest. You also must determine the “what” before the “how” — it’s prohibitive to determine the steps necessary until you define what the particular end should be. I believe there is insufficient attention paid in security programs to what particular ends should be. Is “making Organization, Inc. secure” really the pinnacle of defining goals for our security programs?</p>

<p>The foundation for any product is understand your goal for the product. Fundamentally, what is the product’s purpose? What are you trying to help users accomplish? Viewing security as a product forces you to define your goals and come to terms with your team’s purpose. It also ensures you’re prioritizing actions appropriately — honing in on what will actually improve the product and your customer’s experience.</p>

<p>If your company is publicly traded, have you read their annual report? Can you summarize the Risk Factors they outline in their <a href="https://en.wikipedia.org/wiki/Form_10-K">10-K</a>? The <a href="https://www.sec.gov/fast-answers/answersreada10khtm.html">Risk Factors section</a> is quite literally a cheat sheet, a ranking of your organization’s risks in order of their priority. If you do not understand the risks to the business’ ongoing operations from the organization’s perspective of priority, how could you possibly understand what is most essential to protect?</p>

<p>I assure you that you do not have to dive deeply into the mysterious waters of product management to improve your security program. The aforementioned rants by my blue team friends are painful primarily because they include examples of what you definitely <em>shouldn’t</em> do in product management if you want to create continuously successful products. Even <em>not</em> doing those things will help significantly — and doing the <em>right</em> things will empower you even more.</p>

<p>Because what lurks beneath the frustration expressed by so many in our industry is a sense of helplessness. We don’t feel empowered, we feel stifled and downtrodden. I would argue any profession in which you expend a lot of intellectual effort and time-capital into improving a problem, only to feel like you are running in place, will rapidly burn people out.</p>

<p>In infosec, despite a common understanding that reactive approaches to defense are misguided, we maintain reactive processes. Security teams are accustomed to receiving direction externally, feeling burdened with priorities that defy their beliefs of what is important — as if a secular organization should dictate the priorities of such a sacred order.</p>

<p>Once you adopt the mindset of security as a product, you can begin to take control. One of the “basics” of product management is that solely delivering exactly what customers demand, without understanding the motivation for their demands, will lead to poor outcomes and potentially monstrously disjointed user experiences. You have to proactively understand your customer’s perspective and look beneath the surface of what they are requesting to discern the underlying challenge or desire.</p>

<p>How many of you have worked in retail or other customer-facing service jobs? I have as well, at a department store and later at a frozen yogurt shop, and if security professionals believe they are treated poorly, I promise that you cannot fathom the depths of brutality customers can reach. I ask, because a cornerstone of many customer-facing service jobs is the notion of anticipating needs.</p>

<p>Anticipating needs means understanding your customer’s challenges, desires, and beliefs. For example, one method by which I sold higher-SKU merchandise in the department store was by efficiently learning about my individual customer. I asked questions about why they were shopping and what frustrates them sartorially.</p>

<p>Since I was in the contemporary dresses department, usually the woman was shopping in anticipation of an event, whether a date or party. I listened carefully to pick up on any clues indicating her challenges — for example, one with which I deeply relate is “I hate wearing dresses,” or “I’m going to be on my feet all night.”</p>

<p>Even a morsel of such data was sufficient for me to find additional options for her beyond the items she had chosen. Perhaps a dress with pockets and an elastic waist, that still looks chic while maximizing comfort. Many of the dress-wearing people reading can likely relate to the ecstasy of wearing a dress with pockets, which can both cache snacks or conceal fidgeting hands due to social anxiety. Or, I might offer a maxi dress, which conveniently veils one’s shoes, allowing the option of feet-sparing flats rather than heels.</p>

<p>I’m cognizant that I’m essentially describing a robust recommendation engine (more Netflix than Amazon) — but as it happens, humans can excel in this effort, too. I would imagine most would appreciate a professional faerie godparent constantly anticipating your needs and making your life easier, all without you having to request or nag.</p>

<p>Afforded the cover of not being, strictly speaking, a “security professional,” people are quite honest with me in how they perceive the security team. Security teams frequently are considered the opposite of the faerie godparent— more like a sulking demon that seems to relish an arduous professional life and decrees you are forbidden from doing the things you need to, without ever seeming to care about what those things are.</p>

<p>Security teams both rely primarily on direction and yet seem resentful of this dependence — but ironically also begrudge the notion of reaching out proactively to their organization’s stakeholders to discern what needs to be done.</p>

<p>This inconsistency of thinking leads me to somewhat believe that many security people fundamentally want to dictate what’s important to the company from a security perspective, based on their own opinions, so as to serve the Elder Infosec Deities. Frankly, it sometimes takes considerable effort not to adopt my best Regina George face after listening to a security person elaborately envisioning their Blue Team utopia (what I call a “Blutopia”) at great length and ask pointedly, “Have you ever considered that your opinions might be wrong?”</p>

<p>Part of the reason I don’t ask is that I truly don’t require additional help in amplifying social awkwardness. But the larger part is also that I don’t believe they would be deterred if their opinions are deemed “wrong” by someone else. My conclusion is this is because of the Steve Jobs Myth.</p>

<hr />

<p>I don’t like Steve Jobs. My personal opinion is that he was a jerk and is a wretched role model for leadership. However, I recognize that he is idolized by the type of people who are prioritizing their personal opinion over what their organization actually needs, because of this Steve Jobs Myth. The myth is that through the spellbinding magic of Jobs’ gut instinct alone, and defying all evidence and user analysis, Apple forged ahead with the iPhone and consequently revolutionized the cellular device market.</p>

<p>That isn’t actually what happened.</p>

<p>What actually happened is there was an experimental project initiated without Jobs’ knowledge, which received lukewarm reception by Jobs once presented to him because he believed cell phones “sucked.” However, he trusted the team to work through the technical details and even allowed the head of the project to hire Apple engineers from other projects. His insisted in return on seeing <a href="https://www.cnbc.com/2017/06/16/steve-jobs-iphone-creation-story-proves-even-the-smartest-executives-need-help-making-decisions.html">“an interface that might be intuitive and exciting to lay-users”</a> before he’d be convinced.</p>

<p>The Steve Jobs Myth perpetuates the idea that Jobs gave minimal thought to user needs — which generally makes some people feel empowered to not care, either — and that it is the only way to conceive brilliance and truly leave users awestruck. Jobs’ concern was actually that you cannot simply ask people, “What’s the next big thing?” and that <em>market</em> research is insufficient to conceive a product that customers will love.</p>

<p>However, he viewed <em>user</em> research as essential — as seen in his requirement for the continued development of the iPhone. What he understood is that people won’t always say — or even know — what they want, but through user research, you can see which preferences they truly hold based on how they behave.</p>

<p>Within behavioral economics, there’s a clear hierarchy between stated vs. revealed preferences. Humans can be proficient in fooling themselves in what their preferences are, or if they’re being interviewed, in saving face. For example, if someone asked me, “are you more likely to prepare chicken and broccoli for dinner, or a can of tuna?” I am not necessarily inclined to reveal that I’m sometimes indistinguishable from a cat in behavior and will answer the chicken and broccoli with my ideal self in mind. But if you observed the dinners I made that week, you would see cans of tuna — a vastly firmer source of truth for answering that question.</p>

<p>If you ask your organization, “Do you find SSO easy to use?” you might discover a variety of answers. Maybe they answer “yes” because they don’t want to feel less intelligent by not finding it easy, or they use it so infrequently that they’ve forgotten the frustration of their last use. Maybe they answer “no” because in their customer meeting an hour ago they unsuccessfully accessed a crucial piece of data because of an issue, which made them feel embarrassed in front of the customer. You might even find that people’s answers switch between one week and the next. None of this is particularly helpful.</p>

<p>You can examine revealed preferences instead, looking for the number customer support tickets filed for SSO, the number of multiple push notifications in a row, the number of password reset requests, or how many people re-enter the URL of the service after being directed to SSO. These metrics more accurately tell the “truth” of the user’s experience, and how much it’s aiding or hindering their work.</p>

<p>Another issue arising from the Jobs Myth is that its believers use it to justify proceeding with their projects, generally with the assumption that “users will learn to love it,” because they believe Steve Jobs’ ideas were so provocative and progressive that even if users didn’t know they wanted it, they’d want it in time. That is also thoroughly inaccurate. As Jobs himself <a href="http://www.businessinsider.com/steve-jobs-response-to-an-insult-is-an-example-everyone-should-follow-2017-7">stated</a>:</p>

<blockquote>
<p>“And one of the things I’ve always found is that you’ve got to start with the customer experience and work backwards to the technology. You can’t start with the technology and try to figure out where you’re going to try to sell it.”</p>
</blockquote>

<p>Simply because you personally believe something is valuable or important, does not mean it is. You have to understand the problems that are actually meaningful, and work backwards to how to solve them. This is not just an issue with blue teams, but also with infosec startup founders — the classic blunder of creating a hammer in search of a nail.</p>

<p>Your job is not to determine priorities in your sandboxed mindspace and convince the organization that securing something is of vital importance when it does not present material business risk. Your job is to determine priorities based on what veritably helps the organization and explain why your solution is the right one to help.</p>

<p>An extension of this fallacy is also the reverse — that security people can be presented with a valid solution to the organization’s problem but reject it because they personally don’t believe the problem is important.</p>

<p>As a real-world example, one security professional I know pushed for a specific product to be purchased in their organization. They presented the four-figure cost and offered a variety of use cases where it could be of use — such as simplifying the ability for engineers to implement early detection in the company’s infrastructure. They shopped around the idea to non-security groups on its usefulness and gained their buy-in as well. However, the person in charge of procurement held the personal opinion that this product type isn’t useful, and consequently pushed back on the request.</p>

<p>I see this so regularly I began calling it “Security Morals,” but I now think it should really be “Security Dogma” instead. What I mean by that specifically is that there are somewhat rigid “principles” common among security professionals that are treated as dogma. As aforementioned, there is a seemingly insatiable desire to please the Elder Infosec Deities by strictly adhering to their doctrine, even if it defies the organization’s needs.</p>

<p>In a SaaS product, if an engineer refuses to add a print button because they personally think it’s useless when you can “just” right click and select print, despite all user research indicating that users are at present confused how to print, their personal opinion will be demoted in favor of concrete evidence. If they did this regularly enough, they might be placed on a performance plan.</p>

<p>In security, similar behavior seems rewarded, as if performance is measured by how steadfast your belief is in Security Dogma. Such behavior would not be rewarded if viewing security as a product.</p>

<p>When speaking with defenders, I notice a non-trivial amount bristle at the notion that they have customers — that they aren’t a neutral force above the fray, akin to the Federal Reserve. It was in thinking through why so many defenders hate the concept of having customers that my notion of Security Dogma solidified — that there are principles of security treated as incontrovertibly true and mandatory to implement regardless of the reality of the organization, what determines its fortunes, or what endangers its continuing operation most.</p>

<p>Security professionals may view themselves as a heroic knight, but to others in the organization, they might look like the Knights Templar. As <a href="http://tvtropes.org/pmwiki/pmwiki.php/Main/KnightTemplar">in the trope</a>, even minor security offenses are treated as critical, enforcing “justice” is considered paramount and non-negotiable, and an egotistical complex emerges of interpreting resistance to your “noble” intentions as evidence of principles in need of correction. While you are not, in fact, a knight, you do have the opportunity to be a hero — but not by rescuing someone who is not actually in distress because you believe they need rescuing.</p>

<p><strong>Your customer is your organization.</strong> Imagine if you attempted to order food in an app and it told you “no” because the food was insufficiently healthy, while also never explaining how it defined “healthy” food. Would you enjoy the app? Being realistic, most of us would repeatedly rage at it, even when you begrudgingly conceded that it had a point about your midnight pizza endeavors. This is more than not being likable — which isn’t strictly necessary to be effective. You need to be respected. If you are perceived as dogmatic, I promise you that you will not engender the respect you need to be effective.</p>

<p>I’m usually astonished at how little security teams work on cultivating organizational buy-in, since that’s a core part of my job as a product manager. I personally don’t believe a security program can succeed without it. This doesn’t mean everything becomes watered down, worn meaningless by only acting on things which have perfect agreement.</p>

<p>It instead means ensuring that the organization feels as if it is a stakeholder in security, that it’s along for the journey, and that security is not their adversary, but a fellow team attempting to better the organization. You could actually never implement things that other teams specifically request and still foster a sense of consensus by presenting your point of view with a sense of empathy.</p>

<p>I’ve personally struggled to practice empathy consistently. Particularly for those of us who are on the spectrum, actively seeing the world from someone else’s vantage can feel unnatural. But I assure you it’s not impossible, and your job will become substantially easier when you begin listening to people and ensuring you understand their point of view, rather than trying to dismiss theirs and ram your own point of view down their throat. <a href="https://en.wikipedia.org/wiki/Active_listening">Active listening</a> is one of the most useful life skills you can develop.</p>

<p>Cultivating customer empathy is the first step you should take in your transition to treating security as a product. An example method is through <a href="https://en.wikipedia.org/wiki/5_Whys">the 5 Whys</a>. The goal is to dig deeper into why something is a problem and identify its root causes. For example:</p>

<ul>
<li>“Why do you not want to implement 2FA for Salesforce?”</li>
<li>“Why do you not want to add a step for salespeople to login to Salesforce?”</li>
<li>“Why can’t salespeople afford to take the additional time?”</li>
<li>“Why do salespeople need to log their call notes immediately after a call?”</li>
<li>“Why do salespeople need to transfer notes from Google Docs to Salesforce?”</li>
</ul>

<p>The root cause is arguably that there is friction between the notes salespeople take during a call and where they are meant to log the call. The solution might be to integrate Google Docs into Salesforce, meaning the user has to log into only one service during the course of their work — meaning implementing 2FA will be more palatable. As in this case, you may hear answers that do not seem to be pertinent to the security team, as they are squarely in the business domain — but your role is to connect the dots between business operations and security risks that threaten them.</p>

<p>I strongly believe your highest value as a security professional is, perhaps, in empathizing with the organization’s business risk and identifying where digital risks arise that amplify or solidify business risk. Your customer knows what endangers them — but they do not know how that danger manifests through digital means.</p>

<p>Once you feel you truly understand where customers are struggling, you can begin architecting your vision. Consider your vision for your security program as its story that will unfold over time. Themes serve as the heart of stories, the foundation for the central idea the author is attempting to convey. The plot — or the events that unfold within the story — supports the theme and carries the story towards its goal.</p>

<p>In a security as product model, you will also have themes. Those themes will also have plots — courses of work that drive towards the stated goal and the actions you need to take within those courses of work. Before defining any of the work, however, you have to envision the overarching story. Few people are naturally proficient storytellers, but you can practice by expressing your program’s story through a caricatured, fairy-tale lens.</p>

<hr />

<p>At the dawn of the year, our band of heroes embarked on their quest in Engineersville. They heard the cries from the local farmers of meager yields and slow harvests due to bugs. It would not suffice simply for the heroes to kill all bugs as they appeared — after all, there are many quests elsewhere to complete. They knew their noble purpose was now to help the farmers ensure a bountiful, efficient harvest that they could sustain on their own.</p>

<p>Our heroes’ first goal was to reduce the amount of time it took to squash bugs spotted in the fields, as the bugs could hurt the harvest if they were left alive. Come spring’s first blossom, our heroes transitioned to their second goal — ensuring fewer bugs were being introduced to the crops. They helped the farmers map out how their field architecture would look ahead of planting to determine where bugs could spring up.</p>

<p>As summer began sizzling, they toiled to ensure that their tools could be used by the locals as well, beginning the work on crafting one master tool the locals could use that would automatically determine which specialized tool was best for reducing bugs in the type of fields being sown.</p>

<p>As the first leaves of autumn fell, our heroes tested this magical tool among a small group of farmers, carefully analyzing results and finally releasing it to all locals so that they could begin their next year empowered to have a bug-free harvest. This meant the heroes would have to do even less work of patching and helping locals tend to their fields, allowing them to focus on new quests.</p>

<p>(A wizard hat is optional in crafting stories, but recommended.)</p>

<hr />

<p>Are there any security principles truly sacrificed in this story? The overarching goal is to reduce the number of vulnerabilities in production. As in this story, there may be multiple themes that are part of the same story — reducing the mean time to fix vulnerabilities, adding threat modelling in the design phase to introduce fewer bugs, and creating an automated tool that abstracts multiple security products away from the engineer so they can test their code easily and efficiently during development.</p>

<p>The goal is still fundamentally a security goal, but the themes show customer empathy. The engineers want minimal friction in their workflows. As close as you can provide “push button, get security,” the more productive they will be. Your team, as a stakeholder, is also not ignored. The two initial themes are enablers to the longer-term goal, reducing workload off your team to support progress towards an even more efficient solution that will reduce workloads further.</p>

<p>It is essential to view it as a full story and not be disheartened that your end-goal cannot be accomplished immediately. Setting themes and dreaming up your vision can inspire you so fully that you find yourself with a cornucopia of ideas. Unless you are exceptionally fortunate, the vast majority of teams will not have the resources to pursue every theme and must prioritize them.</p>

<p>Prioritization is one of those tasks that’s very easily said — you “just” rank which themes are most important to you — but is formidable in practice. When I build roadmaps in my work, there is often an excruciating “this or that” decision that requires you to push back work on something which still would absolutely benefit customers… just not as much as the other theme.</p>

<p>My first word of caution to you is avoiding prioritizing themes based on what you <em>feel</em> is most important. Waging a war of opinions is one in which everyone loses — and that’s ultimately what you will be doing, unless you prefer a dictatorship style, if you use your personal views as the basis for your prioritization.</p>

<p>Instead, you must again return to the perspective of your customer. While you personally may believe the theme of “reducing the volume of emails with malicious attachments” is the most important one, your organization may have their deployment frequency and lead time metrics hampered by an arduous appsec process, which more tangibly affects business performance.</p>

<p>How do you differentiate which themes to prioritize? You collect and analyze data — both qualitative and quantitative. A good engineering program will be tracking metrics such as availability, customer tickets, deployment frequency, error rates, lead time, <a href="http://kpilibrary.com/kpis/mean-time-to-detect-mttd-2">mean time to detect (MTTD)</a>, and <a href="http://kpilibrary.com/kpis/mean-time-to-repair-mttr">mean time to repair or recovery (MTTR)</a>. Ask engineering how those metrics are being impacted by security requirements. Ask engineers how they would explain some of the mutual challenges you face — you may be surprised at how aligned DevOps engineering teams are with security (but that is a topic for another time).</p>

<p>If you aren’t tracking metrics on your security program, you should be, as it’s essential for measuring progress in a product. This includes your own MTTD and MTTR — such as how quickly you remediate product security tickets. It should also include measuring the frequency of configuration management changes, such as firewall rule updates, patching, hardening — anything to measure the tempo of your program.</p>

<p>You can also measure how resources — specifically your security team’s time — are being used. Are they spending half of their time extinguishing fires? Is a third of their day dedicated to configuring your SIEM? Do they lose a week each month asking routine questions for threat modelling exercises? These represent opportunities for automation, as there is benefit in reducing the cost of your recurring security tasks and freeing up resources for more impactful streams of work. You should also poll how they want to spend their time, to ensure you retain your talent and avoid needing to worry about the “pipeline problem” in the first place.</p>

<p>Beyond this, you also need to quantitatively measure how your organization perceives the efficacy of your program. For example, conduct the equivalent of <a href="https://en.wikipedia.org/wiki/Net_Promoter">NPS surveys</a> for the security organization, where teams with whom security interacts rate how satisfied they are with the security team. I’d recommend keeping the NPS anonymous with the option of entering a comment to give more detail. After all, security people can sometimes come across as a bit intimidating, and you want to find out the truth.</p>

<p>Quantitative data won’t necessarily tell the entire picture, however. Qualitative data helps fill in detail and may even expose concerns that are difficult to discern from quantitative data. Talk with a selection of individuals across different roles and levels in your organization to hear their feedback on how security can better meet their needs and work with them. You should also ask people on your team, from junior to senior, to give their feedback as well. Again, anonymous surveys can be your friend here in order to promote honesty.</p>

<p>My security fairy tale above could be an example of hitting the nexus of what your data is telling you, thus rising in priority. Your engineers are dissatisfied with having to wrestle with security testing products themselves, and their lead time to deploy is suffering. Half of your product security team’s time is spent on patching and last-minute security testing before GA, because engineering finds it too onerous to currently conduct earlier in the process. If you have three product security people making $100,000 each, you are spending $12,500 per month on something your customer doesn’t like anyway. And perhaps as a last data point, your product security team has expressed the desire to do more research and build custom tools.</p>

<p>A project to build a custom tool that lets engineers self-serve security testing in the development process and to standardize a threat model for the design stage would tangibly improve the data points you have collected. It also happens to be straightforward to measure, which makes likelihood of success even greater, since you can more easily determine what more needs to be done to drive the story.</p>

<p>There are also a few economic angles to consider when prioritizing. First is opportunity cost. By supporting legacy tech with time and money, from what else are you taking away resources? Some of the CISOs I most admire share — coincidentally or not — the trait of thinking in terms of monetary costs of work. This importantly includes pricing in the “total cost” of a security product, which includes the amount of maintenance, tuning, tweaking, and troubleshooting that your team will have to perform on an ongoing basis. Any expenditure of effort by your security team on an action is directly taking away investment into another action.</p>

<p>Second is the sunk cost fallacy. Just because you’ve invested a lot of time and money into something already, doesn’t mean it’s still worth pursuing. Throwing strong resources at weak purposes will deteriorate your product. As in the aforementioned example of opportunity cost, if a legacy security product requires substantial ongoing maintenance to perform as you need, prioritizing a theme of moving to a newer, less burdensome product might be necessary. While this may add a short-term resource sink, it will allow the plot in your story to ultimately move forward.</p>

<hr />

<p>You now feel confident which with themes you prioritized — you know what your story will tell, in what order. However, this story is a shared one, as any security initiatives will inherently be shared due to the nature of affecting the organization. Your customer must be brought along in your journey, and feel like they have a stake in your story.</p>

<p>When you’re soliciting feedback from other people, it’s an opportunity to grow the working relationship — and ultimately engender trust. Rather than nixing their ideas on the spot if you don’t think they’re worthwhile, use language like, “I hadn’t considered that — my team will have to look into it.” You don’t want to promise that all suggestions will be implemented, or you’ll result in a lot of disappointed people, but you do want to make people feel as if they’ve been heard. And, if you do end up implementing something they suggested, or a use case they emphasized, they’ll be delighted.</p>

<p>Be transparent with your story. To start, determine who the right stakeholders are in each organization and ask if you can bring by coffee and treats while you present the story to them. Ask them what they think of it — are there any assumptions with which they disagree? Are there any risks that haven’t been captured? How do they feel it will impact them? Ask open-ended questions so as not to guide them. Before trust is established, phrasing a question as “How will this help you or not?” may compel them to be supportive rather than expressing the full range of their impressions.</p>

<p>As someone working on a product with a third party risk management use case, I can attest that no matter your industry, some of your organization’s prospective customers are asking sales about your security practices. Presenting your vision and progress towards that vision gives them a differentiator to reference, even if far removed from the primary use case of whatever your organization is offering.</p>

<p>Connect with product managers or whomever is designing whatever your organization offers. Not only will it benefit you by receiving feedback, whether to prioritize or to determine the “how,” but it will inspire them to keep you abreast of their own roadmaps. Having security included earlier in the product process will only serve to benefit the entire organization.</p>

<p>As far as how you present your story, some sort of visual aid is generally advisable, rather than purely speaking to it. If you’ve seen <a href="https://swagitda.com/speaking/index.html">my slide presentations before</a>, you can likely guess that I expend substantial effort into how my ideas are visually presented. As a product manager, the slides I create describing my project are not nearly so sparse and beautiful — but I do always consider what I want the listener to take away and leave the rest to voiceover rather than text on the slide.</p>

<p>Bear in mind that while technical meat may sound delicious to you, it can be a repellent to colleagues elsewhere in the business. Your goal is to cultivate consensus around your themes — around the journey of your security program, not the intricate details of the plot. You need to express, in accessible terms, what the theme is, the value it brings to the organization, and any risks or considerations that will be shared challenges across the organization.</p>

<p>Returning to our fairy tale, a slide deck could be presented as follows:</p>

<ul>
<li>Our vision is to reduce the number of vulnerabilities in production</li>
<li>Our goals are to reduce the lead time to deployment, mean time to patch, and security team time spent on application testing</li>
<li>The primary benefit Organization, Inc. is less friction for engineers to test for security vulnerabilities, allowing for our products to be released more quickly</li>
<li>The secondary benefit to Organization, Inc. is reducing the cost of security testing, helping with scalability as well as freeing up resources to accomplish other security goals</li>
<li>We will need to partner with the Engineering team to understand workflows and ensure a security testing orchestrator is deployed appropriately into workflows</li>
<li>We will need to partner with PM to introduce threat modelling during the design phase, which will require a near-term time tradeoff for longer-term cost reduction</li>
</ul>

<p>You begin by inspiring stakeholders, then end with what you need from them to accomplish the vision. This has another benefit of putting those requirements on their radar in advance of when they will need to execute upon them, resulting in quicker turnaround times for you.</p>

<hr />

<p>By the end of the prioritization process, I hope you feel emboldened by the knowledge of which themes are most important to accomplish, and in which order. Now is the time of execution, defining which steps need to be taken towards your goal.</p>

<p>If you have a program manager, this is exactly where to loop them in. If you do not, or cannot loan one from another team for advice, then please do not pretend to be one. That is, you should not assume you understand the abilities and constraints of your team members and assign tasks to them without checking with them first.</p>

<p>While you can lead the charge on the “what,” you must include others in figuring out the “how.” Look to the real Steve Jobs, not the fallacious Steve Jobs Myth, and recall how he trusted the project lead to determine the underlying technical detail so long as his requirements were met. Depending on the size of your team, there will perhaps only be one or two individuals able to take on tasks. Where I see security managers often fail is vacillating between extremes of giving sparse direction and minimal feedback to delving too far in the weeds.</p>

<p>Through the process I outlined, you already defined the requirements of the project through the need to present across the organization — so there isn’t necessarily much more work to be done on your end, save for clarifying requirements on request.</p>

<p>If you want to really make an impact, begin tackling your security debt. You may be familiar with technical debt, which is when quality is sacrificed for speed, typically with the false promise of “we’ll fix it later.” Ironically, by not treating security as a product, you are vastly more likely to accumulate security debt as part of your crusade to integrate your gospel.</p>

<p>Embracing security as a product involves treating it almost as a living thing, one which decays and requires nurturing to stay alive. For each “shortcut” you take, are you considering what challenges will be created later? Did you document why you can’t address it effectively today, for example because there will be a superior way to fix the problem if you wait? How frequently are you returning to those shortcuts and paying down your debt?</p>

<p>There is power in ownership. The <a href="https://en.wikipedia.org/wiki/Endowment_effect">endowment effect</a> is a discovery by behavioral economics that people ascribe more value to things they own, far more than they “rationally” should. Your security program being a product means you own that product — it is <em>your</em> vision and <em>your</em> story. In a fashion, you can consciously nudge yourself into a mindset that will inherently encourage you to take better care of your security program.</p>

<p>I stress this because a lamentable consequence of the nihilism I see in defenders is they cease to care about the security of their organization on a time horizon that surpasses their planned tenure. With the tumultuous turnover of security talent in most organizations, if the strategy is Security Dogma, devotion to it dies when the believer leaves, and a new messenger of the Elder Infosec Deities comes in to spread their own interpretation of the Dogma.</p>

<p>By creating your vision for the security program, you describe a map for the security program’s journey. An incoming hero unburdened by zealotry can see where they are in the journey and the end destination. It’s unlikely that every stop on the journey will be entirely discarded unless there is scant evidence for its value. What is more likely is that the “how” will change most drastically, while the overarching quest — your vision, and to some extent, your legacy — remains intact.</p>

<p>The product process even aids you when switching organizations. What is changing is the end customer — not the process. Even so, just as when I helped women pick out dresses, there will be customers with characteristics in common to each other, rendering your maps meaningful beyond the initial customer.</p>

<p>And if you want to rise the ranks, the practice of articulating a clear vision and fostering consensus will only serve to demonstrate competency. It can demonstrate to your executives or your board of directors that you understand them as a customer and will nourish their trust in your ability to deftly manage risk in a way that supports their success.</p>

<hr />

<p>This is the fairy-tale ending to my own vision I shared with you today — that you can ride into the sunset knowing you were a hero in the way that helped the realm prosper. The fanatics who sought to serve spurious justice will never reach their dream of security nirvana, wailing relentlessly into the wind about their persecution at the hands of locals wanting to prosper.</p>

<p>Inspire your organization with your story of how security can allow it to thrive and make them feel they have a part to play in it. Your band of heroes can and should include colleagues outside of security, who will be far more willing to aid you in your quest — however long or arduous — if you take the time to discuss its purpose to them from a position of empathy.</p>

<p>Security is a product, and reluctance to embrace that is like rejecting scientific evidence in deference to zealotry. There is a way forward that does not rely on worshiping the Elder Infosec Gods through enforcing Security Dogma, which is, in fact, the path of least resistance — despite being less dictatorial.</p>

<p>Quixotism in the name of security purity will crumble as a foundation for a “Blutopia,” but a pragmatic approach, the support of devoted followers within your organization, and a visionary quest just might be the right start to our collective journey.</p>
]]></content>
        </item>
        
        <item>
            <title>2018 Cyber Security Predictions</title>
            <link>//swagitda.com/blog/posts/2018-cybersecurity-predictions/</link>
            <pubDate>Thu, 21 Dec 2017 16:52:25 -0400</pubDate>
            
            <guid>//swagitda.com/blog/posts/2018-cybersecurity-predictions/</guid>
            <description>Fed up with ridiculous infosec predictions for the upcoming year, I decided to aggregate them all and use the power of Markov Chains to generate my own list. What follows is the result, lightly edited solely for readability. I hope to be pioneering the next-gen AI-powered thought leadering market segment.
In 2018, security. Cyber security people will die. We’ve long debated where security people will die. We expect this lucrative trend to continue through 2018.</description>
            <content type="html"><![CDATA[

<p><em>Fed up with ridiculous infosec predictions for the upcoming year, I decided to aggregate them all and use the power of Markov Chains to generate my own list. What follows is the result, lightly edited solely for readability. I hope to be pioneering the next-gen AI-powered thought leadering market segment.</em></p>

<p><img src="/blog/img/cyber-dynomite.png" alt="cyber dynomite" /></p>

<p>In 2018, security. Cyber security people will die. We’ve long debated where security people will die. We expect this lucrative trend to continue through 2018.</p>

<p>2017 predictions were fake, but we received the word. Security predictions for 2018 showcase a myriad of challenges that can be exploited. What’s more, they will pose a significance (the computing, the significance). But we rarely think as well about the potential for net new, impactful cyber events. The world seems less stable, and a software library is another international data breach. One could make it a theoretically important question: are computers Internet connectivity?</p>

<p>Companies can’t count on the internet. We knew full well that this was the near future. It’s simply a “good” business environment of valuable data, data that allows them to move into 2018. Any of their data is one thing to blame, and security will be front and center. Data breaches are from human error, yet traditional hacking is on critical data.</p>

<p>We are at the rising edge of a return to securing applications instead of building complex, expensive and defensive strategies for APT attacks. These breaches that plague organizations today are primarily the information security community’s ability to script, automate, scale, and more efficiently analyze the mass quantities of data involved in cyberattacks for more than a decade.</p>

<p>Organizations will continue to be a popular hacking method. Our children face an amazing future of gadgets, services, and experiences, but they also face tremendous growth of the marketspace and a necessity for organizations. Software will help overcome cultural resistance and arm organizations. The growing awareness is due to significant monetary gains and because problems are always easier to solve when security.</p>

<p>Reality is only automation.</p>

<hr />

<h2 id="prediction-1-the-dark-but-lucrative-trend-in-ransomware-will-continue-to-explode-in-the-cloud">Prediction #1: THE DARK BUT LUCRATIVE TREND IN RANSOMWARE WILL CONTINUE TO EXPLODE IN THE CLOUD</h2>

<p><img style="float: right; max-width:50%; padding: 5px" src="/blog/img/trinity-0day.gif" alt="Trinity's 0day in the Matrix">
The dark but lucrative trend in ransomware will emerge from the shadows and escalate, directly impacting the legal challenge of IT professionals, which will deepen. With this rise in ransomware solutions, businesses will exploit models that will ignite a bit of fun! While we predicted increases in ransomware last year, companies scrambled to update vision and strategy against each other.</p>

<p>Ransomware protects expensive and often inefficient perimeter defenses. FAKEAV and ransomware — like peanut butter and jelly or Thelma and Louise, the two go together. The integration has been to encourage the use of human behavior-directed attacks in the war on cybercriminal technology and help them find a better way for vulnerabilities to require security prediction.</p>

<p>While hackers are already heavily sanctioned, with the rise of populism, 44% of organizations will escalate to a very scary pitch, with each side threatening to go public — exposing you to the risk of huge fines — unless you pay the ransom. For hackers, ransomware will attack each other for years. The Equifax hackers will demand $2.6 million USD — even for a target whose network of seemingly unlimited endpoints contains a massive Equifax breach.</p>

<hr />

<h2 id="prediction-2-bitcoin-wallet-exploits-will-result-in-another-major-ddos-attack-against-critical-infrastructure">Prediction #2: BITCOIN WALLET EXPLOITS WILL RESULT IN ANOTHER MAJOR DDOS ATTACK AGAINST CRITICAL INFRASTRUCTURE</h2>

<p><img style="float: right; max-width:50%; padding: 5px" src="/blog/img/crypto-mining.gif" alt="Doge mining dogecoin">
In 2018, the cryptocurrency escalates. The value of cryptocurrency exchanges and the age of them becomes a top priority for organizations to get the basics of cyber security prediction. They’ve become the payment method of choice for cyberattacks with security experts. Blockchain technology makes them attractive to hackers, as opposed to PCs.</p>

<p>The industry will ultimately find a cheap, dirty, and effective way to monitor sugar levels, and blockchain technologies will increasingly come under mounting pressure to better combat the new threat that will emerge in 2018. Vendor-agnostic implemented blockchain technology underpins the transaction ledgers used by most cryptocurrencies and will increase, driven by third-party security policies that will still lack teeth.</p>

<p>Our prediction of what many deem to be past abuses that came to light with the blockchain technology has started making serious financial impact. 2018 will be the year of abhorrent sexist behavior by powerful tools and those which manage global marketing campaigns. Next year’s newfound love will be forced to only be not-authorized.</p>

<p>Automation will let BTC wallets be hacked and remotely controlled. As with any political drama of the past year, Gartner forecasts 8.4 billion connections to cryptocurrency exchange users’ wallets and exploits of weak authentication, but only when risk is high. For as little as US$ 5, you can actually pay someone to do the attack for you! This is just one issue the GDPR aims to resolve for European citizens.</p>

<hr />

<h2 id="prediction-3-enforcement-on-smart-devices-or-suppliers-will-fail">Prediction #3: ENFORCEMENT ON SMART DEVICES OR SUPPLIERS WILL FAIL</h2>

<p>Following the trend in 2018, the IoT world will continue to grow. This will become more widely accepted, and will overtake AI in VC funding, and security innovation will rapidly escalate to include technologies that drive other smart device hacks. Many IoT technologies lack protections to ensure devices cannot be exploited by the cyberspace dark forces.</p>

<p>The IoT space gets even messier before it adopts a common framework. Given the difficulty of managing IoT sensors in the absence of standards, most solutions remain proprietary and geared toward solving very purpose-driven functions. Expect 2018 to be the year that your device is about to be confiscated.</p>

<p>Hackers who want to gain control over devices are to materialize in 2018 and organizations, including freelance groups hired by the government, will administer DDoS attacks and cyber warfare. Services providers, including governments, will impact things (IoT) connectivity to conduct attacks. It will be the start of a layered addition that targets their hardware chips, which may even be publicly available on the “open-market,” resulting in proliferating worms to infiltrate many IoT deployments.</p>

<p>Vigilante hacking smart meters and installing fileless malware attempts have begun. Major car manufacturers are not yet routinely building security into their target. Will we see self-driving cars seriously hacked? Amazon Echo devices submitted into our crystal ball to manage realization tasks will continue to grow through unpatched new vectors. Drones are used to create serious disruption of things, to say, open a garage door to legitimate organizations. The boardroom needs access to these malicious devices, so as not to have to fend off cyber security gaps using pirated social media spamming.</p>

<hr />

<h2 id="prediction-4-tech-vs-government-round-ii">Prediction #4: TECH VS GOVERNMENT — ROUND II</h2>

<p>We predict increases in the United States launching cyber attacks against other nations. This offers very little incentive towards limiting the Cold War. If they can find a weak link in a system which already established that cyber-risk is now a prominent red exclamation mark in a triangle, we expect to see supply chain issues.</p>

<p>Fake news comes into play when GDPR gets imposed. It’s hard to argue that fake news may or may not have influenced the 2016 presidential election. When it comes to grips, the US elections are building secure fraudsters. The fake news triangle consists of: motivations of proper mobile devices, freelance groups hired by governments, and stealing information projects. A reminder is just around the corner with the US mid-term elections in the aforementioned battle between authentic and fake. Expect lobbyists, foreign and domestic, to push fake news to further their agenda.</p>

<p>International governments and vulnerability of data is embedded into business requirements, and overall levels of social information will accelerate. Singapore has recently been tasked with protecting people, data, intellectual property, stockholder loyalty, and brand protection. In 2018, Africa will emerge to help enterprises, which when left unsecured, can become slave nodes. British security evolves in areas such as China and its role in a free society. Each area alone could make 2018 an interesting year.</p>

<p>Malaysia has also recently analyzed this data as quickly as possible. Malaysia and Indonesia are already looking for alternatives to SSNs, including machine learning that lets computers emulate this to meet the ground up. Alternatives to SSNs could include the defense-in-depth strategy that address the systemic vulnerabilities in the user, coming from devices built on blockchain-related cyber security numbers. Action: Volunteer your time to fully eradicate SSNs from the credit process.</p>

<hr />

<h2 id="prediction-5-prediction-is-gdpr">Prediction #5: PREDICTION IS… GDPR</h2>

<p>Prediction: the European Union (EU) will become untenable. The goal of GDPR is to harmonize data so privacy watchdogs can interfere with businesses worldwide. A group known as the ‘Cutting Sword of Justice’ took credit for GDPR compliance, so companies outside the European Union (EU) will face fines of up to 20,000,000 EUR or up to 4% of their total security. They need to assess whether they will ignite discussions on a politicized role beyond our wildest dreams.</p>

<p>Legislation will mean artificial intelligence in the first regulation (GDPR) becomes enforced. This rule would disable biometrics or a company’s data via the “troll farm” behind Twitter. Ransomware will still be outnumbered by the regulation’s impact on their operations, and in turn, lead to an increase in automated toolsets to drive success. Data regulations in developing markets on the Dark Web offer a sophisticated nature of the user’s physical location, all contacts, or access to their data.</p>

<p>Again, don’t take GDPR seriously or experience it by using machine learning engines.</p>

<hr />

<h2 id="prediction-6-cyber-recycling">Prediction #6: CYBER RECYCLING</h2>

<p><img style="float: right; max-width:50%; padding: 5px" src="/blog/img/machine-learning-oprah.gif" alt="Oprah saying 'And you get a machine learning'">
AI is a tool that can and will be exploited much more than just a convenient way to learn about today’s weather or get the latest sports scores. AI is a tool that can show genuine concern for protecting the privacy debate. AI will also open the way to new vulnerabilities. AI will permit attacks to scale far beyond the techniques that are frequently used. Insurance companies will continue to target holes in machine learning, AI, analyzing our smart devices, and even multi-factor solutions.</p>

<p>Machine learning may also be a powerful tool, and those wielding it will believe that it should not completely take over security mechanisms. It should be considered an additional security layer incorporated into an in-depth defense strategy, and not a silver bullet.</p>

<p>Most still have not seen widespread advertising to deceive machine learning. If a manipulated piece of data or wrong command is sent to an ERP system, machines will be liable to sabotage processes by carrying out cyberattacks against individuals as opposed to being bombarded with false positives. 30% to 40% of the war on cybersecurity experts is with machine learning, selling information, and detecting Internet infrastructures. Even though that analysis may include machine learning-based authentication, it brings with it significant growth in company indicators, driven by nationalistic tendencies.</p>

<p>Machine learning and managed security will move away from detect-and-respond alerts and data. We can spot patterns or those who have superficial attack components. Furthermore, advances with pattern recognition supporting the Internet infrastructure can also see more suppression systems surrounding software at little or no defense. It allows proactively managing the individual to become an essential part of SecOps, and direct sales persons need to be bombarded with false positives.</p>

<hr />

<h2 id="prediction-7-corporate-security-budgets-don-t-produce-income">Prediction #7: CORPORATE SECURITY BUDGETS DON’T PRODUCE INCOME</h2>

<p>We will see increased adoption of cyber security frameworks. Cloud Access Service Brokers (CASB) and other cloud security frameworks have been acquiring certificates that make UEFI an attractive target for cyberattacks. A prime example is Windows 10.</p>

<p><img style="float: right; max-width:50%; padding: 5px" src="/blog/img/oh-no-hackers.gif" alt="Oh no, hackers in the mainframe">
Next-generation security incident response exercise projects will face lawsuits. Sadly, it’s simply to notify them, and report to the fire department. As such, it is not about embedding cybersecurity practices, and large companies are not secure by design. Users and enterprises are advised to routinely check for software.</p>

<p>Managed security processes will deploy a defense-in-depth strategy. Unfortunately, GDPR will provide accurate detection and Response (MDR) services, including techniques such as advanced phishing and social media to help stories spread rapidly. Action: Try shopping at the reconnaissance phase before it’s too late.</p>

<p>Previous attacks are the gift that continues to become an entry point to the central networks. We predict that these networks (which base their success on quantified metrics like ‘daily active users’ and cyber behaviors at the human point) are growing. Numerous readily available fortresses are not sufficient. A hoard of locusts will control systems daily.</p>

<p>In 2018, we will be protected by HTTPS. Those not using HTTPS inspection/decryption are at risk. TLS 1.2 is widely available to anyone who feels the risk level oversight and the human-centric root of risk. Once red teams incorporate into an in-depth defense strategy, not a silver bullet, they should be disabled on all website traffic using HTTPS by default. We discovered that the customers’ red teams were conducting penetration testing, which has repercussions for the industry marketing hype.</p>

<hr />

<p><em>Thanks to Andrew Ruef</em></p>
]]></content>
        </item>
        
        <item>
            <title>My 2017 Reading List</title>
            <link>//swagitda.com/blog/posts/2017-reading-list/</link>
            <pubDate>Thu, 07 Dec 2017 16:44:58 -0400</pubDate>
            
            <guid>//swagitda.com/blog/posts/2017-reading-list/</guid>
            <description>As I wrote about last year, my ongoing New Year’s resolution is to try to read one non-fiction and one fiction book per month. I was unable to fully accomplish this goal for 2017 (I tend to gravitate more towards fiction in tumultuous times), but I loved the books I did manage to read along the way.
In the vein of last year’s post, I sought out non-fiction and science / speculative fiction books by a diverse set of authors.</description>
            <content type="html"><![CDATA[

<p>As I <a href="/blog/posts/2016-reading-list">wrote about last year</a>, my ongoing New Year’s resolution is to try to read one non-fiction and one fiction book per month. I was unable to fully accomplish this goal for 2017 (I tend to gravitate more towards fiction in tumultuous times), but I loved the books I did manage to read along the way.</p>

<p>In the vein of last year’s post, I sought out non-fiction and science / speculative fiction books by a diverse set of authors. While last year I aimed to discover as many new female authors as possible, this year I specifically strove to experience sci-fi from other cultural perspectives and underrepresented voices, as well as learn more about the history of non-Western civilizations.</p>

<p>My reading list is below, including links to each book’s Amazon page if you’d like to check them out. There were no “thumbs down” books I read this year, and I’m bad at book reviews anyway, so I’ll leave judgement up to y’all.</p>

<h2 id="non-fiction">Non-Fiction</h2>

<p><a href="https://www.amazon.com/1491-Revelations-Americas-Before-Columbus/dp/1400032059">1491: New Revelations of the Americas Before Columbus</a> by Charles C. Mann</p>

<p><a href="https://www.amazon.com/1491-Revelations-Americas-Before-Columbus/dp/1400032059">The Plague of War: Athens, Sparta, and the Struggle for Ancient Greece</a> by Jennifer T. Roberts</p>

<p><a href="https://www.amazon.com/Predicting-Unpredictable-Tumultuous-Earthquake-Prediction/dp/0691138168">Predicting the Unpredictable: The Tumultuous Science of Earthquake Prediction</a> by Susan Elizabeth Hough</p>

<p><a href="https://www.amazon.com/Secret-History-Mongol-Queens-Daughters/dp/0307407160">The Secret History of the Mongol Queens: How the Daughters of Genghis Khan Rescued His Empire</a> by Jack Weatherford</p>

<p><a href="https://www.amazon.com/What-Works-Gender-Equality-Design/dp/0674089030">What Works: Gender Equality by Design</a> by Iris Bohnet</p>

<h2 id="fiction">Fiction</h2>

<p><a href="https://www.amazon.com/Autonomous-Novel-Annalee-Newitz/dp/0765392070/">Autonomous: A Novel</a> by Annalee Newitz</p>

<p><a href="https://www.amazon.com/Binti-Nnedi-Okorafor/dp/0765385252/">Binti</a> by Nnedi Okorafor</p>

<p><a href="https://www.amazon.com/Fifth-Season-Broken-Earth/dp/0316229296/">The Fifth Season</a> by N.K. Jemisin</p>

<p><a href="https://www.amazon.com/Fifth-Season-Broken-Earth/dp/0316229296/">Infomocracy: A Novel</a> by Malka Older</p>

<p><a href="https://www.amazon.com/Kalpa-Imperial-Greatest-Empire-Never/dp/1931520054/">Kalpa Imperial: The Greatest Empire That Never Was</a> by Angelica Gorodischer</p>

<p><a href="https://www.amazon.com/Mountains-Mourning-Vorkosigan-Saga-ebook/dp/B004O4C13W/">The Mountains of Mourning</a> by Lois McMaster Bujold</p>

<p><a href="https://www.amazon.com/Ninefox-Gambit-Machineries-Empire-Yoon/dp/1781084491">Ninefox Gambit</a> by Yoon Ha Lee</p>

<p><a href="https://www.amazon.com/Queue-Basma-Abdel-Aziz/dp/1612195164/">The Queue</a> by Basma Abdel Aziz</p>

<p><a href="https://www.amazon.com/Three-Body-Problem-Cixin-Liu/dp/0765382032/">The Three-Body Problem</a> by Cixin Liu</p>

<p><a href="https://www.amazon.com/Three-Body-Problem-Cixin-Liu/dp/0765382032/">The Three Stigmata of Palmer Eldritch</a> by Philip K. Dick</p>

<p><a href="https://www.amazon.com/Warcross-Marie-Lu/dp/0399547967/">Warcross</a> by Marie Lu</p>
]]></content>
        </item>
        
        <item>
            <title>First Principles of Building Security Products</title>
            <link>//swagitda.com/blog/posts/first-principles-building-security-products/</link>
            <pubDate>Mon, 12 Jun 2017 16:32:41 -0400</pubDate>
            
            <guid>//swagitda.com/blog/posts/first-principles-building-security-products/</guid>
            <description>Using Shamir’s 10 Commandments of Commercial Security to build better security products
A printout of Adi Shamir’s 10 Commandments of Commercial Security has been my #1 office essential since I first stumbled upon them, and I argue it should be yours, too. Shamir outlined these commandments in his talk at the Crypto ‘95 conference (yes, way back in 1995), and they not only spell out the first principles of enterprise security, but serve as a poignant reminder that although it feels as if the industry is evolving around us at a feverish pace, the fundamentals of building security products are true even two decades later.</description>
            <content type="html"><![CDATA[

<p><em>Using Shamir’s 10 Commandments of Commercial Security to build better security products</em></p>

<p>A printout of Adi Shamir’s <a href="http://www.ieee-security.org/Cipher/ConfReports/conf-rep-Crypto95.html">10 Commandments of Commercial Security</a> has been my #1 office essential since I first stumbled upon them, and I argue it should be yours, too. Shamir outlined these commandments in his talk at the Crypto ‘95 conference (yes, way back in 1995), and they not only spell out the first principles of enterprise security, but serve as a poignant reminder that although it feels as if the industry is evolving around us at a feverish pace, the fundamentals of building security products are true even two decades later.</p>

<p>Despite these tenets being evergreen, I rarely see them referenced — hence, I want to re-post and draw attention to them, and discuss why they’re still pertinent even today. While it’s absolutely beneficial reading for blue teams, the examination after the list is specifically about their importance from the perspective of people building commercial information security products.</p>

<hr />

<h2 id="10-commandments-of-commercial-security">10 Commandments of Commercial Security</h2>

<p>Directly quoted from <a href="http://www.ieee-security.org/Cipher/ConfReports/conf-rep-Crypto95.html">Adi Shamir, Crypto ‘95</a></p>

<blockquote>
<p><strong>1. Don’t aim for perfect security</strong>
So, be realistic, and do the best you can within your limits. Roughly, you should double security expenditure to halve risk.</p>

<p><strong>2. Don’t solve the wrong problem</strong>
For example, note that US banks lose 10 billion dollars a year in check fraud but only 5 million in online fraud. [naturally, these are 1995 figures and no longer accurate]</p>

<p><strong>3. Don’t sell security bottom-up</strong>
(in terms of the personnel hierarchy).</p>

<p><strong>4. Don’t use cryptographic overkill</strong>
Even bad crypto is usually the strong part of the system.</p>

<p><strong>5. Don’t make it complicated</strong>
This yields more places to attack the system, and it encourages users to find ways to bypass security.</p>

<p><strong>6. Don’t make it expensive</strong></p>

<p><strong>7. Don’t use a single line of defense</strong>
Have several layers so security can be maintained without expensive replacement of the primary line.</p>

<p><strong>8. Don’t forget the “mystery attack”</strong>
Be able to regenerate security even when you have no idea what’s going wrong. For example, smart cards are attackable but are great for quick cheap recovery.</p>

<p><strong>9. Don’t trust systems</strong></p>

<p><strong>10. Don’t trust people</strong></p>
</blockquote>

<hr />

<h2 id="how-can-these-commandments-help-us-build-better-security-products">How can these commandments help us build better security products?</h2>

<p>Let’s review these commandments from the product perspective one by one.</p>

<p><strong>1. Don’t aim for perfect security</strong></p>

<p>No matter what sort of security product you’re building, you must be realistic that you will never block, detect, monitor, “thwart,” mitigate, or remediate all attacks. While for blue teams the rule of thumb is you should double your security budget to halve risk, for product teams, it should be that you should double your R&amp;D budget to halve your customer’s risk. Don’t invest too much time or money on addressing niche threats just because doing so sounds sexy — and don’t just consider what would meaningfully decrease your customer’s risk today, but also 1–2 years from today. Nor should you prioritize trendy attacks that will be passé to attackers next year.</p>

<p><strong>2. Don’t solve the wrong problem</strong></p>

<p>I believe this is the primary failing of most security product teams. The problem is never about stopping X attack. Let’s explore this with the “5 Whys” method (in which you may not need all 5 whys). Why does the customer need to stop X attack? X attack can cause a data breach, which blue teams want to prevent. Why? Data breaches lead to regulatory fines, reputational impact, etc. that blue teams want to prevent. Why? Damage to the organization makes the blue team look incompetent, which blue teams don’t want. Why? Blue teams looking incompetent can lead to them losing their jobs.</p>

<p>Ultimately, if your product is not helping blue teams look competent — meeting compliance, demonstrating mastery of your organization’s security posture to executives, being able to demonstrate why a breach was not the result of negligence — then you aren’t solving the right problem. This is why dashboards, reporting, visualizations, scoring, compliance modules, and all the other “boring” things matter.</p>

<p><strong>3. Don’t sell security bottom-up</strong></p>

<p>This is why buyer vs. user personas matter. This is also why including the aforementioned “boring” things like dashboards, reporting, and other high-level elements matter. You must be able to demonstrate value to CISOs, SOC directors, AppSec managers, and any other relevant team leads. This doesn’t mean your UX should be geared towards the buyer — your PoC/PoV will fall flat if so. It means you should consider the value to the buyer, and how you can articulate and demonstrate that in your product. Design UX for drill-downs, but market with dashboards.</p>

<p><strong>4. Don’t use cryptographic overkill</strong></p>

<p>The classic advice is “don’t roll your own crypto,” but what this really means is don’t rely on crypto to check the box for deeming your product “secure.” While Shamir may not have envisioned a future so dismal that <a href="https://www.av-test.org/en/news/news-single-view/32-products-put-to-the-test-how-good-is-antivirus-software-at-protecting-itself/">AV products don’t even deliver updates over SSL</a>, the point stands that with shameful frequency, security product vendors don’t rigorously audit their own product’s security.</p>

<p><strong>5. Don’t make it complicated</strong></p>

<p>One of my paramount frustrations with the infosec industry is the earnest shock and sneering contempt infosec professionals seem to have regarding the fact that users often bypass security protections. While a good handful of the booths at RSA this year finally demonstrated apt attention towards UX, too often UX is ignored in security product design — and specifically, UX to end users.</p>

<p>Yes, infosec is hard, but design is hard as well, and security product builders should respect it far more. Mediocre security that users love — or even simply tolerate in their workflows — will always be superior to fine security that users will spend vast amounts of effort looking to circumvent. And guess which will win in the market, too?</p>

<p>The other point here is that introducing complexity — more features, more parsers, more attempts at “all in one” security — degrades the security of the systems the product is meant to protect. Complexity means more limited ability for vendors to verify security, and even now, big vendors often get away with providing assurances of security standards without actually meeting them.</p>

<p><strong>6. Don’t make it expensive</strong></p>

<p>Security vendors violate this all the time, catering to only the largest of enterprises, while the medium enterprise market remains one of the juiciest yet most neglected. For example, Okta, who had an incredibly successful IPO and is adored by the public market, explicitly developed their product in a mid-size enterprise-friendly manner, but one from which large enterprises could still benefit. Their pricing is transparent with a menu of potential products, but as a reference, their bread and butter SSO offering would run an organization of 1,000 employees about $24,000 per year.</p>

<p>Compare this to a solution like the original FireEye box priced at $250,000 per year. In fact, FireEye has somewhat turned their previously lagging fortunes around in large part by releasing FireEye Helix, a far simpler — and far less expensive — platform available in an as-a-service subscription model. Selling to the large banks or generating massive services fees may allow for some initial success, but will set security products up for failure in the broader market.</p>

<p><strong>7. Don’t use a single line of defense</strong></p>

<p>For product builders, this commandment means you should assume that your product will not serve as your customer’s single line of defense. Do not try to make your product the solution to all problems. This doesn’t mean you can’t offer multiple products under one “platform,” because, frankly, most of the time what people mean by platform is just that all the vendor’s products have API rails to talk to each other, and that there may be a central console from which you can click buttons to access each of the products.</p>

<p>This does mean that each product should aim to tackle one particular challenge, and tackle it well. To quote the illustrious Ron Swanson, <a href="https://www.youtube.com/watch?v=zl-HalherjQ">“don’t half-ass two things, whole ass one thing.”</a></p>

<p><strong>8. Don’t forget the “mystery attack”</strong></p>

<p>Consider how your product helps your customers deal with unknown unknowns. This can take many forms, such as presenting valuable context or creating resilient environments. As a simple example, offering detection only with signatures creates a binary of “bad” and “not bad,” while looking for behaviors can allow for risk scores whose traits can be used to help narrow down attacker activity when a breach is discovered. Or, by exposing context around events to security analysts, you can present relevant information to assist in threat hunting and incident response, rather than only showing the direct cause of an alert.</p>

<p>Further, with the rise of VMs and containers, it’s become easier to tear infrastructure up and down should compromise occur. For example, Slack rebuilt each component of their cloud infrastructure from scratch after a major breach in March 2015. Infosec vendors should consider leveraging these technologies and strategies in their own products and systems, too.</p>

<p><strong>9. Don&rsquo;t trust systems</strong></p>

<p>Design your product in a way that assumes the customer’s estate is compromised and that your product can be compromised. The former should absolutely be taken into consideration for any machine learning or baselining approach, but it also means that compromised customers can lead to attackers learning how your product works — and thus developing countermeasures. The latter assumption is related to #4 and #5, in that you should consider how to minimize impact to your customer should an attacker exploit your product.</p>

<p>This naturally includes being careful about the third parties on which you rely, such as by auditing any proprietary or open source libraries you incorporate in your product — but also ensuring that your product minimizes privileges and permissions. This is, in part, why customers are wary of adopting host agents, as evidenced by the fact that Tavis Ormandy has found <a href="https://www.wired.com/2016/06/symantecs-woes-expose-antivirus-software-security-gaps/">many highly embarrassing bugs</a> due to endpoint protection products running at a high level of privileges.</p>

<p><strong>10. Don&rsquo;t trust people</strong></p>

<p>Don’t assume end users will operate like <em>Homo securitas</em>, perfectly adhering to proper security hygiene and willing to bear some inconvenience by understanding that security is so important. For whatever reason, it doesn’t seem like the industry accepts yet that users will click things they shouldn’t, bypass things that get in the way of their work, and not understand things that you think are obvious. From the customer angle, although it’s more of a basic principle of UX, assume that your customers will also often use your product in a way that you didn’t intend — whether benign or malicious.</p>

<p>Consider a particularly iniquitous example: if you’ve built a user monitoring product for finding insider threats, bear in mind that it could be used as a tool for <a href="https://en.m.wikipedia.org/wiki/LOVEINT">LOVEINT</a> and harassment depending on how it’s designed, allowing someone to effectively spy on the activity of another employee.</p>

<p>Or, DLP solutions that give granular visibility into documents going in and out of the organization’s estate could expose potentially revealing titles — like M&amp;A-Agreement-With-Acquiror.pdf or Reputationally-Sensitive-Deal-Draft.docx— that could violate confidentiality agreements or cause other damage should an employee of either the customer or the vendor see — or worse, leak — this information.</p>

<hr />

<h2 id="conclusion">Conclusion</h2>

<p>I hope I’ve adequately convinced you of these commandments’ relevancy today (and going forward), and that they can serve as a constructive framework for building security products. For product managers, challenge your roadmap against these commandments. For engineers, consider if your plan for how to architect product features and underlying systems adheres to these tenets. For marketers, leverage these principles in messaging value to the customer and demonstrating alignment with their priorities.</p>

<p>The reality of the infosec industry is that few products adhere to these commandments, which means this framework offers opportunities for differentiation. <a href="https://en.wikipedia.org/wiki/KISS_principle">Keep it simple, stupid</a>, and design your product to help customers return to these first principles, even if they don’t know yet that they need them — build the car, not faster horses.</p>

<hr />

<p>Many thanks to <a href="https://twitter.com/snare?lang=en">Leigh Honeywell</a> for reviewing.</p>
]]></content>
        </item>
        
        <item>
            <title>Choice Architecture for InfoSec Blue Teams</title>
            <link>//swagitda.com/blog/posts/choice-architecture-infosec-blue-teams/</link>
            <pubDate>Wed, 08 Feb 2017 16:17:34 -0400</pubDate>
            
            <guid>//swagitda.com/blog/posts/choice-architecture-infosec-blue-teams/</guid>
            <description>I recently spoke at Art into Science: A Conference for Defense, which was an intellectually-stimulating (and delightfully quirky) conference focused on moving towards a professional discipline for defensive infosec. Sadly, I had to rush through the last part of my presentation, so I wanted to do it justice by fleshing out my thoughts here. I’m going to skip through the first two sections of my talk— an introduction to cognitive biases and how they manifest in infosec, then challenges that arise due to the group nature of blue teams — but feel free to check out the slides here.</description>
            <content type="html"><![CDATA[

<p><img src="/blog/img/frederic-audet-161331.jpg" alt="Image of Darth Jar Jar" /></p>

<p>I recently spoke at Art into Science: A Conference for Defense, which was an intellectually-stimulating (and delightfully quirky) conference focused on moving towards a professional discipline for defensive infosec. Sadly, I had to rush through the last part of my presentation, so I wanted to do it justice by fleshing out my thoughts here. I’m going to skip through the first two sections of my talk— an introduction to cognitive biases and how they manifest in infosec, then challenges that arise due to the group nature of blue teams — but feel free to <a href="https://swagitda.com/speaking/Know-Thyself-Kelly-Shortridge-ACoD-2017.pdf">check out the slides here</a>.</p>

<p>The last part of the presentation focused on the “what to do about it” — I developed a choice architecture for blue teams in information security. Choice architecture is a term coined by Richard Thaler and Cass Sunstein in their famous book “Nudge,” and means the design of how choices can be presented to people.</p>

<p>The implication is that it can be designed in a way that impacts decision making, and more specifically in a way that minimizes errors due to cognitive biases. It’s basically a “how do we fix it?” response to the flaws in thinking that behavioral economics exposes (see <a href="/blog/posts/behavioral-models-infosec-prospect-theory/">my post on Prospect Theory &amp; Information Security</a> for a primer on some of these flaws as they appear in infosec).</p>

<p>I’ll walk you through my proposed choice architecture for how blue teams can develop a decision-making process that is resilient to cognitive biases, or you can just skip to the conclusion for the 6-step guide.</p>

<ol>
<li><a href="#belief-prompting">Belief Prompting</a></li>
<li><a href="#decision-trees">Decision Trees</a></li>
<li><a href="#social-tactics">Social Tactics</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ol>

<hr />

<h2 id="a-name-belief-prompting-a-belief-prompting"><a name="belief-prompting"></a>Belief Prompting</h2>

<p><img src="/blog/img/thinking-clueless.gif" alt="Gif of Cher from Clueless thinking" /><em>Get your thinky thinky face on</em></p>

<p>People have beliefs about their opponents in any confrontation. People also tend to believe their opponents are less rational than they actually are — and often make imprudent decisions as if their opponent is randomly choosing their decisions. A counter to this blunder is asking players for their explicit beliefs about what their opponents will do, known as <a href="https://books.google.com/books?id=bMWHDAAAQBAJ&amp;pg=PA132&amp;lpg=PA132&amp;dq=%22belief-prompting%22&amp;source=bl&amp;ots=QsfBOYXBlM&amp;sig=EDB2DND3JdfbnXJG7CJycLQOMjk&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjlupiu7fTRAhXq5YMKHfzxClgQ6AEIKjAD"><strong>belief prompting</strong></a>. Think of it as increasing one’s thinking by an additional step — how will your opponent respond to your move?</p>

<p>What beliefs about adversaries need to be evaluated in information security? I believe the answer is capital, time, equipment and risk aversion. You can also use the kill chain as a guide for the timeline of moves you need to consider. It’s critical to keep in mind that attackers aren’t profligate; as Dino Dai Zovi said, “attackers will take the least cost path through an attack graph from their start node to their goal node.”</p>

<p>Thus, theorizing probabilities of each type of move is necessary in order to consider weighted risk — for example, your adversary using iOS 0day on one of your employees may have a 1% chance (probably even less) of successfully occurring, so it most likely shouldn’t be the top influence in your decision making.</p>

<p>Some example questions to ask yourself, or when discussing with team members, are:</p>

<ul>
<li>Which of our assets will attackers want?</li>
<li>How does our adversary choose and craft their delivery method?</li>
<li>What countermeasures does our adversary anticipate?</li>
<li>How would an attacker bypass our [insert security product / solution / strategy here]?</li>
<li>How would an attacker respond to our [insert security product / solution / strategy here]?</li>
<li>What are the cost / resources required for an attacker to make [insert type of offensive move]?</li>
<li>What is the probability that an attacker will conduct [insert type of offensive move here]?</li>
</ul>

<p>As an example of where these questions might lead you, here’s a belief prompting process for exfiltration, with defensive moves in blue and offensive moves in orange:</p>

<p><img src="/blog/img/belief-prompting-example.png" alt="An example of belief prompting for information security, by Kelly Shortridge" /></p>

<hr />

<h2 id="a-name-decision-trees-a-decision-trees"><a name="decision-trees"></a>Decision Trees</h2>

<p><img src="/blog/img/jared-swot.gif" alt="Jared from the show Silicon Valley presenting a SWOT analysis" /><em>I’m basically Jared for the infosec industry</em></p>

<p>Creating a decision tree allows for a feedback loop that is invaluable in aiding the decision-making process, particularly in prioritizing strategies. I believe a decision tree model is the most efficient means of solving a few challenges in decision-making, as it:</p>

<ol>
<li>Forces you to belief-prompt and increase your thinking by many additional steps</li>
<li>Provides an auditable risk model so you can identify where your assumptions broke down (and thus mitigate the “doubling down” effect and self-justification) in event of a breach — an attempt to remove politics out of security strategy (e.g. favoring a product because you implemented it)</li>
<li>Allows for easy refinement as data is generated from incidents</li>
<li>Lets you see commonalities between attack trees where certain solutions might counter multiple moves</li>
<li>Helps you visualize the hardest path for attackers so you can tune your strategy to force them down that path</li>
</ol>

<p>In creating the decision tree, you need to map out how attackers will respond to each of your countermeasures and to assign probabilities to the likelihood that they will pursue a certain option, as well as what options defenders have and the probability that these countermeasures will successfully prevent the offensive move.</p>

<p>The decision tree below from my presentation is for illustrative purposes— it covers a criminal group gaining access to a company’s server. It’s based on a Defender-Attacker-Defender model (see <a href="https://swagitda.com/speaking/Volatile-Memory-Kelly-Shortridge-Troopers-2017.pdf">my upcoming talk at Troopers</a> to go deeper!), with potential countermeasures by defense in blue and potential moves by attackers in gold.</p>

<p>The left branches in each set of moves represent the lower-cost moves, while the right branches represent the more expensive moves (cost here meaning monetary, human and time capital). For example, “#YOLO” means doing absolutely nothing, while “Privilege Separation” requires doing more than nothing, thus making it the more expensive option.</p>

<p><img src="/blog/img/decision-tree-example.png" alt="An example of a decision tree for information security, by Kelly Shortridge" /></p>

<p>For those who need a walkthrough — if you start with the “Criminal Group” adversary that has just arrived on one of your servers, you can consider a preemptive defensive move being either: implementing privilege separation, which has an guesstimated 60% chance of deterring or thwarting the attacker; or “implementing” the #YOLO strategy of doing nothing, which has a 0% chance of deterring the attacker.</p>

<p>If you decide to implement Privilege Separation, the attacker’s response will either be the lower cost option — to scan for reachable data from their lower-privilege vantage with a guesstimated 50% likelihood of leading the attacker to a valuable box — or they could pursue the more resource-intensive option of throwing a known exploit, which has a guesstimated 50% chance of successfully working.</p>

<p>Following the “hard path,” whereby the attacker chooses using the known known exploit, the defender then contemplates what countermeasures they can put in place to challenge the attacker further. For example, they could implement seccomp, which filters sys calls and has a guesstimated rate of thwarting the known exploits 50% of the time, or they could implement GRSec, which bears the higher cost of being barely usable but blocks basically all known exploits.</p>

<p>If you go down the GRSec path, the attacker’s only option becomes “elite” 0day (meaning 0day that requires certain level of finesse and reliability), which takes significantly more time to craft — and the probability of it being deployed successfully is very low.</p>

<p>I know how y’all can be, so let me emphasize: <strong>the goal is not to quibble over exact probabilities until you can prove who is the better pedant</strong>. It’s meant to be a framework to aid in decision-making by visualizing your belief-prompting and is a starting point from which you can tweak your assumptions as you ingest real-world data.</p>

<hr />

<h2 id="social-tactics">Social Tactics</h2>

<p><img src="/blog/img/kittens-meeting.gif" alt="A gif of kittens having a &quot;meeting&quot;" /><em>omg it&rsquo;s kittens having a meeting</em></p>

<p>I won’t cover the social dynamics of teams here (<a href="https://swagitda.com/speaking/Know-Thyself-Kelly-Shortridge-ACoD-2017.pdf">check out the presentation</a> to read about it if interested), but belief-prompting vis a vis probability-labeled decision-trees ameliorates team-based biases as well. However, blue team leaders need to consider a few additional tactics to round out their decision-making model — most of which are in the vein of framing, i.e. how choices are presented (see <a href="https://en.wikipedia.org/wiki/Framing_effect_%28psychology%29">framing effects</a> for more).</p>

<p>First and foremost, leaders shouldn’t state their own views before soliciting feedback, lest it anchor the rest of the team’s opinions. It could lead to the team then guessing, “Ok, what do I think the boss believes?” but hopefully there’s a sufficient culture of respect that there isn’t that sort of paranoia. In that vein, using a decision tree can also serve as a starting point to solicit dissenting feedback — it can be easier to disagree with a probability or label on a tree rather than words coming out of someone’s mouth.</p>

<p>A key challenge for blue teams is how to deter short-termism and overly risky decisions. The nebulousness of costs and benefits of certain strategies or implementing solutions results in uncertainty of blue team members in regards to how their performance is evaluated. Soliciting longer-term views on these decisions, clearly articulating what constitutes success and failure for them, and agreeing on these figures with whomever is the “doer” for the project would alleviate much of this uncertainty.</p>

<p>The goal is to not pressure team members into agreeing to something just to show off their skill level, or because they fear they will look incompetent if they refuse, but to foster a sense of buy-in to the plan and maintain explicit expectations.</p>

<p>Finally, blue teams can leverage the decision trees when weighing different types of strategies or solution options, by estimating how much of a difference it would make in decreasing the probability of an attack succeeding vs. its cost — including monetary cost as well as personnel cost (having to hire a new person just to manage a product would require a hefty reduction in risk to justify).</p>

<hr />

<h2 id="conclusion">Conclusion</h2>

<p>Ultimately, the ideal bias-resilient decision making process for blue teams in information security looks something like:</p>

<ol>
<li>State beliefs about your adversaries</li>
<li>Model decision trees</li>
<li>Create a spectrum of success / failure for each decision</li>
<li>Develop a probability / payoff matrix for different decision options (leveraging the decision tree)</li>
<li>Prioritize rationality and overall benefit over risk-taking</li>
<li>Revisit and refine your decision trees after each incident</li>
</ol>

<p>I don’t claim to have all the answers, and welcome any and all feedback on how to refine this model of a decision-making model. In the spirit of the Art into Science con, we need to collaborate in order to move the ball forward meaningfully in the philosophy of defense and ultimately give defenders a more auspicious foundation upon which to architect their security strategy.</p>

<hr />

<p>Many thanks to <a href="https://twitter.com/snare?lang=en">snare</a> for reviewing.</p>
]]></content>
        </item>
        
        <item>
            <title>Russia used the U.S. Influence Ops Playbook</title>
            <link>//swagitda.com/blog/posts/russia-used-us-influence-ops-playbook/</link>
            <pubDate>Wed, 18 Jan 2017 16:05:49 -0400</pubDate>
            
            <guid>//swagitda.com/blog/posts/russia-used-us-influence-ops-playbook/</guid>
            <description>Roger Trinquier, the French counterinsurgency theorist, said, “The sine qua non of victory in [insurgent/counterinsurgent] warfare is the unconditional support of the people.” In Influence Operations, success is ultimately about the ability to overcome one’s status as an outsider.
The past few months have seen cyberwar intersect with Influence Operations, in Russia using chicanery along these lines to influence the U.S. presidential election. In wanting to learn more about Influence Ops, I stumbled across a paper from the Naval War College entitled “Influence Operations &amp;amp; the Human Domain.</description>
            <content type="html"><![CDATA[<p><img src="/blog/img/spy-vs-spy.png" alt="An image of the two spys from the comic &quot;Spy vs. Spy&quot; shaking hands" /></p>

<p>Roger Trinquier, the French counterinsurgency theorist, said, “The <a href="https://en.wikipedia.org/wiki/Sine_qua_non">sine qua non</a> of victory in [insurgent/counterinsurgent] warfare is the unconditional support of the people.” In Influence Operations, success is ultimately about the ability to overcome one’s status as an outsider.</p>

<p>The past few months have seen cyberwar intersect with Influence Operations, in Russia using chicanery along these lines to influence the U.S. presidential election. In wanting to learn more about Influence Ops, I stumbled across a paper from the Naval War College entitled <a href="https://www.usnwc.edu/getattachment/Departments---Colleges/Center-on-Irregular-Warfare---Armed-Groups/Publications/Scanzillo-and-Lopacienski---Influence-Operations-and-the-Human-Domain.pdf.aspx">“Influence Operations &amp; the Human Domain.”</a> They helpfully give an example of their offensive playbook, from their Influence Ops campaign in the Philippines. But as I read it, I felt my expression of horror intensifying.</p>

<p><img src="/blog/img/britney-shock.gif" alt="A gif of Britney Spears reacting with shock, of the negative kind" /></p>

<p>The similarities in the U.S.’s Influence Ops strategy in the Philippines and Russia’s Influence Ops strategy for the U.S. presidential election are stark. The primary difference is the Philippines campaign mentions leveraging social media, but it occurred at the dawn of its ubiquity, whereas Russia had access to social media’s fuller potency. I’ve pulled quotes on the U.S. strategy from the paper, and written my thoughts on how it maps to the Russian ops below — judge for yourself.</p>

<hr />

<p>The first step is determining the worthwhile targets of your Influence Ops campaign. These targets seem to be referred to as your “mobilizable population,” which falls into three distinct categories:</p>

<ol>
<li>Core supporters of the state (think: offense’s side — supporters of the Philippines government in the USG case; Trump’s supporter base in the Russian case)</li>
<li>Core supporters of the insurgency (think: offense’s opposition — terrorists in the USG case; the “establishment,” and more specifically the Democratic party in the Russian case)</li>
<li>Large middle group who are prepared to support one side or the other depending on circumstances (think: swing voters)</li>
</ol>

<blockquote>
<p>[The third group’s members] are the fence sitters weighing the cost and benefit of aligning with one side or the other. This group is the focal point of the influence struggle. The first two groups are generally ideologically driven and are highly unlikely to change sides.</p>
</blockquote>

<p>The enmity between groups of individuals in the United States that are party-first is well known— they’re intransigent and will only ever vote for their party, no matter the candidate’s actual qualifications or policy positions. However, it’s also known that winning swing voters is the only way to achieve victory — only winning the party’s base is not enough to guarantee success.</p>

<blockquote>
<p>For the core supporters of the state, a specialized U.S. task force conducting Influence Operations and working with host-nation forces generally provides the host government with the resources, training, and/or support that is most appropriate for the operating environment</p>
</blockquote>

<p>If we equate “core supporters of the state” with “our side,” this translates to: “A specialized Russian task force conducting Influence Operations and working with U.S. forces generally provides the Trump campaign with the resources, training, and/or support that is most appropriate for the operating environment.”</p>

<blockquote>
<p>That large middle group, the impressionable majority of the population, becomes the focal point in a struggle between the insurgents and counterinsurgents for decisive influence. Many in this group will have an initial preference toward one side, but the side they choose to support depends on the expected costs and benefits of their alternatives.</p>
</blockquote>

<p>Commonly, you hear that swing voters went to Trump for economic reasons. The goal is also to minimize the perceived cost, however — making it seem like the cost of a Trump Presidency was minimal. And indeed, the cost to cisgender, heterosexual, middle class and up white people is most likely minimal.</p>

<blockquote>
<p>The primary target audience for JSOTF-P’s Influence Operations was the diverse Philippine population within the joint operations area. Secondary audiences included local Philippine government officials, Philippine security forces, and the Philippine population not directly affected or targeted by the insurgents.</p>
</blockquote>

<p>I’d argue the “operations area” here is the electorate with the highest chance of turning towards Trump — so the Minivan Majority plus disaffected swing voters and Democrats. Secondary audiences included local American government officials, American security forces (likely Comey &amp; the FBI, given their seeming fealty to Trump), and the American population not directly affected or targeted by the “establishment” (upper middle class, educated white voters).</p>

<blockquote>
<p>The phrase “as they create a secure and stable environment” was particularly significant. It remained critical for the Filipino population to see their own government in the lead, which made enhancing the Philippine Security Forces’ capacity to operate autonomously and more effectively a primary JSOTF-P mission.</p>
</blockquote>

<p>It was crucial that American voters saw Trump as a candidate with his own agency, rather than as a Russian puppet.</p>

<blockquote>
<p>The method of application began with a targeting process to identify which communities were most vulnerable to a particular threat or hostile influence.</p>
</blockquote>

<p>Russia thus identified the communities, primarily white and non-urban, that felt most vulnerable to a “particular threat” — in this case, the multicultural globalization movement that they feel has left them behind socially and economically.</p>

<blockquote>
<p>1. The first PSYOP LOE supported JSOTF-P’s civil-military engagement by personalizing AFP and JSOTF-P support to local communities</p>
</blockquote>

<p>Russia’s Influence Ops and the Trump campaign did an excellent job of tailoring the message to each rally and to propaganda spread throughout social media channels.</p>

<blockquote>
<p>2. The second PSYOP LOE was focused on disrupting insurgent operations by creating dissent among the insurgents as well as between the insurgents and the communities that traditionally supported or tolerated them.</p>
</blockquote>

<p>The Russian Influence Ops campaign published the DNC emails via Wikileaks specifically for this scurrilous purpose — by making it seem that Bernie Sanders had been robbed of the nomination, they drew some of his supporters to Trump. The leaks in general were the foundation for their polemic against “the establishment,” and implicating Clinton in it.</p>

<blockquote>
<p>3. The third major PSYOP LOE was the Rewards for Justice Campaign. This LOE identified the most heinous insurgent leaders, offered rewards leading to their arrest, and, more importantly, made personal connections between the atrocities committed and the insurgent leaders responsible for them.</p>
</blockquote>

<p>In general, the Russian ops was adroit in making Clinton the boogeyman — painting her as the putative zenith of USG’s and the “global elite’s ” corruption and haughty ways. In particular, the <a href="https://www.washingtonpost.com/news/the-fix/wp/2016/11/22/a-brief-history-of-the-lock-her-up-chant-as-it-looks-like-trump-might-not-even-try/">“Lock Her Up!” chant</a> I think is an exemplary use of this sort of seditious tactic. Not to mention flouting Clinton’s “Wall Street ties” and global focus — both seen as enemies of Main Street.</p>

<blockquote>
<p>4. JSOTF-P’s fourth PSYOP LOE — the Mass Media Campaign — provided operational-level influence support to the task force as a whole and galvanized all three previous PSYOP LOEs together through an extensive and overt commercial multimedia campaign.</p>
</blockquote>

<p>While they received some help from Fox News, for the most part the Russian Influence Ops campaign disseminated their calumny against existing U.S. power structures and Clinton via alt-right and white nationalist sites like Breitbart, or <a href="http://www.propornot.com/p/the-list.html">set up new domains for propaganda dissemination</a>, such as americanlookout.com or endoftheamericandream.com. These became “mass media” through rampant social media sharing, particularly through Facebook.</p>

<blockquote>
<p>Creating dissent within and between the two insurgent groups and the populace was dependent on fostering trust and developing favorable options for the affected people, thereby providing a viable and desirable alternative to living with an insurgent presence.</p>
</blockquote>

<p>Trump decisively labeled Clinton as being part of the “establishment” and thus not for the “people.” However, what helped him appear contumacious the most was breaking away from the GOP, labeling them the establishment as well, and significantly departing from their policy positions — for example by being against free trade and pro-Putin.</p>

<blockquote>
<p>A unique aspect of JSOTF-P’s influence messaging was the primacy of using CME and face-to-face engagements to validate the influence messages instead of employing reactive messages to address events after they occurred.</p>
</blockquote>

<p>Trump held countless rallies, personalizing apoplectic messaging for each local community.</p>

<blockquote>
<p>Our PSYOP messaging mediums capitalized upon these seams by amplifying the population’s silent-majority concerns and grievances with the ASG.</p>
</blockquote>

<p>An example of amplifying the population’s “silent-majority” concerns and grievances were Trump’s anti-immigration and anti-Muslim rhetoric, but also dog-whistling white nationalism by <a href="https://www.theatlantic.com/business/archive/2016/10/trump-african-american-inner-city/503744/">tying black communities to inner cities</a>, <a href="https://www.washingtonpost.com/news/fact-checker/wp/2015/07/08/donald-trumps-false-comments-connecting-mexican-immigrants-and-crime/">Mexican immigrants to rapists</a> and even leveraging <a href="https://www.washingtonpost.com/opinions/anti-semitism-is-no-longer-an-undertone-of-trumps-campaign-its-the-melody/2016/11/07/b1ad6e22-a50a-11e6-8042-f4d111c862d1_story.html">anti-semitic global-banking-conspiracy</a> / <a href="https://en.wikipedia.org/wiki/New_World_Order_%28conspiracy_theory%29">New World Order</a> themes.</p>

<blockquote>
<p>The PSYOP detachment paid careful attention not to show carnage but to encapsulate the fear and anguish of the witnesses, as well as the grim determination of the AFP and U.S. forces that were often the first to arrive on the scene with medical aid and security.</p>
</blockquote>

<p>Ironically, it tends to be Trump’s base that talks about <a href="https://en.wiktionary.org/wiki/feels_over_reals">“feels over reals,”</a> but ultimately Trump’s campaign was about suppurating their feels— the white middle class is afraid of being less relevant in a globalized world, afraid of the erosion of their privilege that greater equality — both on a national and global scale — brings. Trump gave very few tangible policy, let alone execution, strategies, but preyed on the “anguish” of his base that their jobs have been lost to automation and globalization.</p>

<blockquote>
<p>The U.S. supported the Philippine government and security forces with access to information, intelligence, and modern technology to assist their efforts to build and maintain situational awareness, provide predictive analysis, and react to insurgent threats.</p>
</blockquote>

<p>Replace with “Russia supported the Trump Campaign (and Wikileaks) with access to information, intelligence, and modern technology to assist their efforts to build and maintain situational awareness, provide predictive analysis, and react to threats from the Clinton campaign.”</p>

<blockquote>
<p>Key to JSOTF-P’s Intelligence LOE success was the ability of U.S. intelligence personnel to “export” the processing, exploitation, and dissemination of the collected intelligence to the partner or host in order to build their capacity and give them ownership of the decision-making cycle.</p>
</blockquote>

<p>A good chunk of this was giving hacked data from the DNC over to Wikileaks, but assuredly also to the Trump campaign.</p>

<blockquote>
<p>The desired effect for terror groups is dissent within their ranks, discord from the populace, and their surrender, dissolution, and demonstrated defeat</p>
</blockquote>

<p>Well, Russia certainly succeeded in sowing dissent within liberal ranks and discord from the populace…but they aren’t to demonstrated defeat yet.</p>

<blockquote>
<p>Each village, community, province, and hostile group was unique within the concept of population-centric warfare, but they all shared cultural and personal commonalities.</p>
</blockquote>

<p>Trump’s base shared a few key cultural and personal commonalities, such as being white as well as typically being less educated and living in more rural areas.</p>

<blockquote>
<p>In the affected nation, the relevant population will generally choose the side that provides them with the greatest stability.</p>
</blockquote>

<p>While many who recognize the instability that a Russian puppet brings to our democracy, you can see why voters — primarily those who are white, in the Midwest, and lacking the skills to compete in an increasingly globalized, technological society — would view Trump as the candidate who would provide them with the greatest stability by his theoretical rolling back of the clock to the days of yore.</p>

<hr />

<p>This, of course, is just one case study, but why I like it is that it shows that the U.S. IC was well aware of how a nation state conducts an Influence Ops campaign in the social media era. However, I couldn’t find any papers on countering a nation state’s Influence Ops campaign. Perhaps it’s classified, or perhaps we never assumed that a nation state would use our playbook against us.</p>

<p>That’s not to say that creating such a strategy would be trivial — I personally don’t have a great notion of how a playbook to counter nation-state Influence Ops would look. I’m all too aware of the realities a decentralized media ecosystem brings and how puissant social media has become — it is prohibitive for the government to squash damaging propaganda distributed through those channels.</p>

<p>The Grugq recently wrote about <a href="https://medium.com/@thegrugq/security-cyber-and-elections-part-4-e327e527132a#.ab5xfs6za">the challenges in countering such a campaign</a>, and I’m inclined to believe he’s correct in his analysis. I also agree with McCain’s strong suggestion recently of starting a new <a href="https://en.wikipedia.org/wiki/United_States_Information_Agency">USIA</a>, though it will require a radically different approach than what was employed before.</p>

<p>Finally, if you buy into my pattern-matching above, it’s challenging to arrive at the conclusion that the Trump campaign did not have a Russian retinue to coordinate their efforts with Russia’s influence ops campaign. It’s either an exceptionally convenient coincidence they were so in sync in pulling off this strategy, or something is rotten in the state of Denmark.</p>
]]></content>
        </item>
        
        <item>
            <title>Revisiting 2016 Security Predictions</title>
            <link>//swagitda.com/blog/posts/revisiting-2016-security-predictions/</link>
            <pubDate>Fri, 30 Dec 2016 21:08:55 -0400</pubDate>
            
            <guid>//swagitda.com/blog/posts/revisiting-2016-security-predictions/</guid>
            <description>The flaming word cloud of the cyberpocalypse, from 16 of the larger security vendors’ 2016 predictions
I totally get it — it’d be boring to say year after year, “Yep, phishing definitely still works,” so security vendors instead pour creative thinking and marketing pizzazz into their annual security predictions. Most seem to aim to match the majority of their predictions to other vendors’, with generally one or two unique predictions thrown in to show off their innovative thought leadering.</description>
            <content type="html"><![CDATA[

<p><img src="/blog/img/2016-predictions.png" alt="Word cloud of 2016 predictions" /><em>The flaming word cloud of the cyberpocalypse, from 16 of the larger security vendors’ 2016 predictions</em></p>

<p>I totally get it — it’d be boring to say year after year, “Yep, phishing definitely still works,” so security vendors instead pour creative thinking and marketing pizzazz into their annual security predictions. Most seem to aim to match the majority of their predictions to other vendors’, with generally one or two unique predictions thrown in to show off their innovative thought leadering. Sometimes those unique ones are spot-on and glorify their authors, and other times we can look back a year later and share a good chuckle.</p>

<p>I chose 16 prediction reports for my analysis, published between September 2015 and February 2016 (later than that would give an unfair advantage), mostly by the larger security vendors and also Wired, since they’re one of the few publications who publishes their own predictions rather than crowdsourcing.</p>

<p>I&rsquo;ll delve into:</p>

<ol>
<li><a href="#what-happened">What actually happened in 2016?</a></li>
<li><a href="#correct-predictions">Which predictions were right? (naming names)</a></li>
<li><a href="#wrong-predictions">Which predictions were off? (not naming names)</a></li>
</ol>

<hr />

<p><img src="/blog/img/fancy-bear.jpg" alt="The fancy bear logo by Crowdstrike" /></p>

<h2 id="a-name-what-happened-a-what-actually-happened-in-2016"><a name="what-happened"></a>What actually happened in 2016?</h2>

<p>Looking back on infosec news in 2016, these seem to have been the biggest stories:</p>

<h3 id="grizzly-steppe-fancy-bear">Grizzly Steppe / Fancy Bear</h3>

<p>Unless you’ve been living under a rock, Russia waged a (successful) campaign to stoke chaos and influence the U.S. election — hacking the DNC (and RNC), spreading fake news, etc. <a href="https://www.us-cert.gov/sites/default/files/publications/JAR_16-20296.pdf">FBI / DHS just released a joint report on the campaign</a>, deeming it “GRIZZLY STEPPE.” Crowdstrike named the group <a href="https://www.crowdstrike.com/blog/who-is-fancy-bear/">“Fancy Bear,”</a> and has written about its intrusion into the DNC as well as its use of <a href="https://www.crowdstrike.com/wp-content/brochures/FancyBearTracksUkrainianArtillery.pdf">Android malware to infiltrate Ukranian field artillery units</a> (which seemed to be a response to skeptics of attributing the DNC hack to Russia).</p>

<p>So how was this epic, history-shaping attack conducted? The initial delivery vector was spear-phishing…if it ain’t broke, don’t fix it. To be fair, this was “sophisticated” as far as spear-phishing goes — using legitimate domains and then spoofing a Google suspicious account activity email (not something obvious like malware.delivery.plzclick.ru), which ultimately tricked recipients into entering in their passwords through a fake webmail domain that appeared to be a password reset page. Then, the credentials were used to harvest the emails, exfiltrate them through encrypted communications and create the #Podesta debacle.</p>

<h3 id="mirai-botnet-dyn-ddos">Mirai Botnet (Dyn DDoS)</h3>

<p>People panicked when Dyn, a DNS service provider, was getting DDoSed, breaking the internet even more than Kim Kardashian. Then, we found out that the nemesis was a botnet composed of <a href="http://www.theregister.co.uk/2016/10/21/dyn_dns_ddos_explained/">hundreds of thousands connected devices</a>, serving as a zombie army of “things” for their overlords. All Mirai did was log into devices by using the factory default passwords, which was successful enough to infect over 1 million devices.</p>

<p>The author of Mirai open sourced it, and given that the security of IoT devices hasn’t radically improved in the past few months, and consumers are generally allergic to security responsibility, <a href="https://www.wired.com/2016/12/botnet-broke-internet-isnt-going-away/">it’s easy to imagine massive IoT botnets happening again</a>.</p>

<h3 id="crypto-ransomware-boom">Crypto-ransomware boom</h3>

<p>Most recently, San Francisco’s Municipal Transportation Agency <a href="http://arstechnica.com/security/2016/11/san-francisco-muni-hit-by-black-friday-ransomware-attack/">was infected by ransomware (specifically HDDCryptor)</a>, with the attackers demanding $73k in exchange for restoring the data. However, SFMTA decided instead to let riders ride for free for a bit, and then fixed the problem by using a backup to restore their systems. The initial access was gained by a vulnerability involving Oracle’s WebLogic server and the Apache Commons library (a deserialization vulnerability), and SFMTA only became a target because the attackers used a web scanner to find vulnerable servers.</p>

<p><a href="https://www.wired.com/2016/02/hack-brief-hackers-are-holding-an-la-hospitals-computers-hostage/">Hollywood Presbyterian Medical Center in Los Angeles was also held hostage</a> through the ransomware “Locky,” but ended up paying only $17k in BTC vs. the initial demand of $3.4 million. Locky ended up infecting a few other hospitals, as well, and is particularly nasty because it looks for and erases Volume Shadow Copy files, which means automatic backups are erased.</p>

<h3 id="swift-wire-transfer-hax">SWIFT wire transfer hax</h3>

<p><a href="https://www.wired.com/2016/05/insane-81m-bangladesh-bank-heist-heres-know/">Hackers stole $81 million from accounts at Bangladesh’s central bank</a> within a few hours, and that wasn’t the only bank they attacked. They achieved this by getting bank employees’ credentials to SWIFT, the network between banks that processes most of the world’s wire transfers. There’s nothing concrete on how they got the credentials, but we do know they used malware to subvert SWIFT’s software for recording money transfers — which is far from ideal. It resulted in SWIFT to push for 2FA, which should help in the future.</p>

<h3 id="adult-friend-finder">Adult Friend Finder</h3>

<p>This breach received perfect 10 on the <a href="http://breachlevelindex.com/top-data-breaches">breach level index</a> — <a href="http://breachlevelindex.com/top-data-breaches">over 400 million records were exposed</a>. There isn’t much information on the breach yet, but it appears the passwords were kept in plaintext or used SHA1. The attackers allegedly exploited a <a href="http://www.csoonline.com/article/3132533/security/researcher-says-adult-friend-finder-vulnerable-to-file-inclusion-vulnerabilities.html">local file inclusion vulnerability</a>, meaning it’s yet another web app attack.</p>

<p>Honorable mention goes to two other hookup sites that were breached and stored their passwords in plaintext. Fling.com had credentials for <a href="http://www.ibtimes.co.uk/fling-com-breach-passwords-sexual-preferences-40-million-users-sale-dark-web-1558711">40 million of its users on sale for 0.8888BTC</a> in May (~$400 at the time). Mate1.com had <a href="https://motherboard.vice.com/read/hacker-claims-to-have-sold-27m-dating-site-passwords-mate1-com-hell-forum">over 27 million plaintext passwords for sale</a>, but for 20BTC in February (~$8,700 at the time). For Mate1, the hacker said that they compromised the server and dumped the MySQL database, with no further details.</p>

<h3 id="yahoo-data-breach">Yahoo data breach</h3>

<p><a href="http://www.nytimes.com/2016/12/14/technology/yahoo-hack.html?_r=0">It affected 1 billion user accounts</a>…which was surprising because most people never thought Yahoo had that many users. I won’t be counting this towards scoring the predictions, given the actual attack happened in 2013. But, there’s no denying it received substantial coverage, and will be an interesting case study going forward if it results in <a href="https://techcrunch.com/2016/10/06/report-verizon-wants-1-billion-discount-after-yahoo-privacy-concerns/">Verizon successfully reducing the purchase price of the acquisition</a>…or not acquiring them at all.</p>

<hr />

<p><img src="/blog/img/bad-cyberart-09.jpg" alt="A crystal ball" /></p>

<h2 id="a-name-correct-predictions-a-which-predictions-were-right-naming-names"><a name="correct-predictions"></a>Which predictions were right? (naming names)</h2>

<p>Spear phishing was mentioned 11 times total in all of the predictions — half of those were about mobile-specific spear phishing, and the rest mostly about the need to train employees on how to look out for it. Only McAfee and Sophos made any mention of hackers using “sophisticated” spear phishing more often, and even then it was a minor point in their larger reports. Election-themed phishing was predicted by Forcepoint, which I cover below, but no mention of spear phishing methods specifically. So, no one really wins on this one.</p>

<p>Kaspersky and Intel Security accurately predicted — although embedded rather than one of their primary predictions — that libraries used by servers (particularly open source libraries) would increasingly be targets, which was relevant in the SFMTA case. Otherwise, web app / server-side app attacks were virtually ignored — despite being a prevalent attack vector year after year. And exactly zero of the reports mentioned anything about SWIFT, or wire transfers more generally — the focus was primarily on credit card systems.</p>

<p>But other events fared better in their coverage:</p>

<h3 id="influence-operations-u-s-election-themed-attacks">Influence operations &amp; U.S. election-themed attacks</h3>

<p>First up, ForcePoint (Raytheon) hit the nail directly on the head with their prediction on U.S. election-related influence operations. While a few others brought up electronic voting systems, ForcePoint specifically called out the fake news phenomenon (albeit missing involvement by nation-state threat actors):
&gt;     Information on social media is often spread and accepted before fact can catch up with fiction, giving determined hacktivists an opening to misrepresent and/or misdirect the public’s perception of individuals and events…
&gt; …the other hand suggests there’s little to prevent incendiary, inaccurate information from virally spreading and being accepted by the public as factual. Even if such information is later corrected, this false information lives forever on the Internet, with the potential to inform opinions and as a result misinform — and potentially direct the actions of — the electorate.</p>

<p>They also correctly predicted the use of election-themed phishing campaigns, and even that candidates would be targeted:</p>

<blockquote>
<p>However, given the influence the choice of a U.S. President can have…it’s not hard to envision a circumstance where factions hoping to gain insight or advantage in an election or following it, might target a candidate or groups involved in promoting them for useful data in keeping ahead of or undermining the competition</p>
</blockquote>

<p>So, congrats on them for being right, however unfortunate for the rest of us.</p>

<h3 id="iot-botnets">IoT Botnets</h3>

<p>The other big winner is Wired, who predicted the Mirai botnet, or what they called “the rise of the IoT Zombie Botnet.”</p>

<blockquote>
<p>One trend we’ve already spotted is the commandeering of IoT devices for botnets. Instead of hackers hijacking your laptop for their zombie army, they will commandeer large networks of IoT devices — like CCTV surveillance cameras, smart TVs, and home automation systems.</p>
</blockquote>

<p>Anomali also gets some credit for this, too, although it was a minor point in their larger prediction about IoT exploitation.</p>

<h3 id="ransomware-to-a-certain-extent">Ransomware…to a certain extent</h3>

<p>This was easily one the safest predictions to make last year, which is probably why nearly all the vendors mentioned it. Some took a more hyperbolic approach and overshot its impact and potential damage. Others head-scratching-ly said that ransomware will “go corporate,” although there had already been plenty of documented cases of corporate ransomware before 2016. But technically they weren’t wrong about it hurting businesses more, so I’ll allow it.</p>

<p>Many warned about potential extortion, in which the attackers would threaten to go public with data in the hopes of receiving a higher ransom, but for obvious reasons we probably wouldn’t hear about those cases publicly, so the jury’s still out. I’d argue this fear was overblown, given we saw three major hookup site breaches, credentials for which were being sold on dark web forums (when they could’ve been used for extortion).</p>

<p>Predictions that ransomware would become cross-platform were also technically accurate, as there are now documented cases of ransomware for <a href="http://www.computerworld.com/article/3113658/security/new-ransomware-threat-deletes-files-from-linux-web-servers.html">Linux</a> and <a href="http://www.theregister.co.uk/2016/03/09/first_macosx_ransomware_actually_linux_port/">Mac</a> — although not many documented families of ransomware for these platforms as of yet. I’d personally say Linux ransomware could be a possibility for 2017 — after all, it’s what constitutes modern infrastructure for most enterprises.</p>

<h3 id="attacks-hidden-in-ssl-vs-cleartext">Attacks hidden in SSL vs cleartext</h3>

<p>According to an <a href="https://www.a10networks.com/news/cybersecurity-report-organizations-victimized-by-malware-hidden-in-encrypted-traffic">A10 Networks / Ponemon study</a> from this summer 41% of attacks used malware hidden in SSL traffic to evade detection. Appliances have a difficult time quickly inspecting SSL traffic to detect malware, making it a new headache for enterprises. And turns out A10 Networks was the only one to predict this trend.</p>

<hr />

<p><img src="/blog/img/bad-cyberart-10.png" alt="A digital thumbs down" /></p>

<h2 id="a-name-wrong-predictions-a-which-predictions-were-off-not-naming-names"><a name="wrong-predictions"></a>Which predictions were off? (not naming names)</h2>

<h3 id="iot-in-theory-but-not-reality">IoT in theory, but not reality</h3>

<blockquote>
<p>We won’t see widespread examples of attackers getting IoT devices to run arbitrary code any time soon.</p>
</blockquote>

<p>Welp…see the aforementioned Mirai botnet.</p>

<h3 id="pki-ubiquity">PKI ubiquity</h3>

<blockquote>
<p>In 2016, we expect that PKI will become ubiquitous security technology within the IoT market.</p>
</blockquote>

<p>A safe prediction for 2017 is that PKI will continue to be a mess, and so will IoT security.</p>

<h3 id="drone-hackpocalypse">Drone hackpocalypse</h3>

<blockquote>
<p>However, drones also present a wide range of risks, from privacy invasion to corporate espionage to terrorism.</p>
</blockquote>

<p>Sure, but realistically it’s confined to nation-state level for now, so it’s dubious if it belongs in a predictions report aimed at corporate CISOs. Some <a href="https://www.wired.com/2016/03/hacker-says-can-hijack-35k-police-drone-mile-away/">research</a> <a href="https://hub.jhu.edu/2016/06/08/hacking-drones-security-flaws/">was</a> <a href="http://arstechnica.com/security/2016/10/drone-hijacker-gives-hackers-complete-control-of-aircraft-in-midflight/">published</a> on how to hack drones, but there’s nothing confirmed in-the-wild. Amazon also just <a href="http://qz.com/873920/amazon-has-a-plan-to-defend-drones-from-hackers-and-bow-and-arrow-wielding-troublemakers/">published a patent</a> for protecting its delivery drones against hacking…and against bows and arrows. My guess is this prediction is supposed to fall under the <a href="http://qz.com/873920/amazon-has-a-plan-to-defend-drones-from-hackers-and-bow-and-arrow-wielding-troublemakers/">Rule of Cool</a> rather than having a logical basis for impact to enterprises.</p>

<h3 id="terrorists-pick-up-the-cyber-bomb">Terrorists pick up the (cyber)bomb</h3>

<blockquote>
<p>In 2016, we will increasingly see the convergence of physical and cyber terrorism aimed at wreaking far-reaching havoc.</p>
</blockquote>

<p>Terrorists did not become cyber ninjas blowing up power plants all over the place. The Grugq has <a href="https://medium.com/@thegrugq/isis-cyber-security-skills-suck-cc3466aa73f7#.jhezy91mj">already written</a> <a href="https://medium.com/@thegrugq/just-the-facts-isis-encryption-c70f258c0f7#.e1ovjsbl8">about this</a>. <a href="http://cybersquirrel1.com/">Squirrels are still the better conductors of cyber war ops</a> against our critical infrastructure.</p>

<h3 id="post-quantum-crypto">Post-quantum crypto</h3>

<blockquote>
<p>“The cryptopocalypse is nigh.” (due to quantum computing)</p>
</blockquote>

<p>Nope. Probably a prediction that can be safely shelved for a few years.</p>

<h3 id="cyber-insurance-changes-everything">Cyber. insurance. changes. everything.</h3>

<blockquote>
<p>The cyber insurance market will dramatically disrupt businesses in the next 12 months.
In 2016 many companies will turn to cyber insurance as another layer of protection, particularly as cyber attacks start mirroring physical world attacks.</p>
</blockquote>

<p>Looks like most companies still think insurance is <a href="https://www.sans.org/reading-room/whitepapers/analyst/bridging-insurance-infosec-gap-2016-cyber-insurance-survey-37062">inadequate risk mitigation</a>. According to the <a href="https://www.sans.org/reading-room/whitepapers/analyst/bridging-insurance-infosec-gap-2016-cyber-insurance-survey-37062">SANS survey</a>, only 33.5% of companies have cyber insurance. What’s more, <a href="http://www.partnerre.com/assets/uploads/docs/PartnerRe_Cyber_Liability_Trends_Survey_2016.pdf">83% of respondents to a PartnerRe survey</a> said that cyber insurance policies are only “sometimes” meeting the needs of insured companies. So, while it may be blossoming as a new “check the box” item, it’s nowhere near disrupting enterprise security strategies yet.</p>

<h3 id="mobile-app-exploitation">Mobile app exploitation</h3>

<blockquote>
<p>With the growing amount of malware and the vulnerabilities present in legitimate mobile apps, a major breach is bound to happen, potentially on a massive scale.</p>
</blockquote>

<p>While many of the predictions highlighted mobile as an attack vector in general, where they specifically got it wrong is in thinking that vulnerabilities in mobile apps would be exploited. Why expend the effort when malicious apps still work just fine? See: <a href="https://www.crowdstrike.com/blog/danger-close-fancy-bear-tracking-ukrainian-field-artillery-units/">the poisoned app Fancy Bear used to hack Ukranian field artillery units</a>. There haven’t been any major corporate breaches directly tied to mobile malware, though I’ll concede it’s possible they just aren’t public. Mobile malware grew at a very healthy pace in 2016, however:</p>

<p><img src="/blog/img/new-mobile-malware-2016.png" alt="Chart of new mobile malware by McAfee Labs" /></p>

<h3 id="wearables-as-the-new-sexy-attack-vector">Wearables as the new sexy attack vector</h3>

<blockquote>
<p>Initially, we doubt that a smartphone will be completely compromised by an attack through a wearables device, but we expect to see the control apps for wearables compromised in the next 12 to 18 months in a way that will provide valuable data for spear-phishing attacks.</p>
</blockquote>

<p>There’s been research on hacking them, but as far as I can find, no evidence that hacking wearables has ever been used as part of a corporate data breach. The only headlines in 2016 are variations of “Hackers targeting your wearables data?” or “Can Wearable Technology Threaten the Security of Your Biz?” so I’ll invoke <a href="https://en.wikipedia.org/wiki/Betteridge%27s_law_of_headlines">Betteridge’s law of headlines</a> and say the answer is no.</p>

<h3 id="remote-controlled-cars">Remote-controlled cars</h3>

<blockquote>
<p>Attacks on automobile systems will increase rapidly in 2016 due to the rapid increase in connected automobile hardware built without foundational security principles.</p>
</blockquote>

<p>This didn’t happen. <a href="https://www.wired.com/2016/08/jeep-hackers-return-high-speed-steering-acceleration-hacks/">Seriously cool research</a>, but no reports in the wild — just because something can be hacked doesn’t mean it’s easily hacked.</p>

<div class="center">
    <blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">Why is it that every headline is &#39;product X easily hacked&#39;. Hacking a car was hard. Hacking is hard. It is good that it is hard.</p>&mdash; Chris Valasek (@nudehaberdasher) <a href="https://twitter.com/nudehaberdasher/status/805419828756496384?ref_src=twsrc%5Etfw">December 4, 2016</a></blockquote>
    <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>
</div>

<h3 id="password-reuse-attacks-will-decline">Password reuse attacks will decline</h3>

<blockquote>
<p>Password reuse attacks will begin to decline… people are starting to adopt password managers… Advancements in biometrics are also helping the cause…most major Internet layers are also adding two-factor authentication as a standard option</p>
</blockquote>

<p>All of those things might be true, but not enough to make a dent. These are entities publicly victims to password reuse attacks in this year alone: <a href="https://www.carbonite.com/en/cloud-backup/business/resources/carbonite-blog/carbonite-password-attack/">Carbonite</a>, <a href="https://threatpost.com/gotomypc-suffers-major-password-reuse-attack/118781/">Citrix GoToMyPC</a>, <a href="http://www.bbc.com/news/technology-38070985">Deliveroo</a>, <a href="https://techcrunch.com/2016/06/16/github-accounts-targeted-in-password-reuse-attack/">GitHub</a>, <a href="http://www.batblue.com/groupon-users-hit-password-reuse-attack/">Groupon</a>, <a href="http://www.vanityfair.com/news/2016/06/mark-zuckerberg-terrible-password-revealed-in-hack">Mark Zuckerberg</a>, <a href="http://arstechnica.com/security/2016/06/teamviewer-users-are-being-hacked-in-bulk-and-we-still-dont-know-how/">TeamViewer</a>, <a href="https://www.itgovernance.co.uk/blog/uk-national-lottery-password-reuse-attacks-what-are-the-chances/">U.K. National Lottery</a>. Further, Patrick Heim, Dropbox’s Head of Trust &amp; Security said in September that <a href="http://www.cso.com.au/article/606531/99-compromised-user-accounts-come-from-password-reuse-cso-heavy-hitters-reveal/">“99% of compromised user accounts come from password reuse.”</a> Seems more like wishful thinking / “it’s-2016-how-is-this-still-happening?!” than a prediction.</p>

<hr />

<h2 id="conclusion">Conclusion</h2>

<p>I’ve already started reading some of the 2017 predictions — being diplomatic, I’ll say I’m looking forward to seeing how the next year unfolds. I saw one list described as not containing any “wows,” which I think accurately pinpoints the problem with these lists in general.</p>

<p>It’s more an exercise in taking a provocative stance to raise attention and be able to tell customers how your product addresses this important upcoming issue, rather than a probability-weighted list of the actual threats enterprises will face in the upcoming year — users will keep clicking on things they shouldn’t, injection vulnerabilities are still in all the things, and not enough people use 2FA or encrypt their users’ credentials.</p>

<p>In conclusion, my ultimate 2017 prediction is that I’ll have plenty of content for an equivalent of this post next year.</p>
]]></content>
        </item>
        
        <item>
            <title>My 2016 Reading List</title>
            <link>//swagitda.com/blog/posts/2016-reading-list/</link>
            <pubDate>Mon, 26 Dec 2016 21:03:57 -0400</pubDate>
            
            <guid>//swagitda.com/blog/posts/2016-reading-list/</guid>
            <description>Two years ago, I made my New Years Resolution to read one fiction and one non-fiction book each month, and I’ve (mostly) kept it up since then. But towards the end of 2015, it suddenly occurred to me that the vast majority of the authors I read were men. So, I made my 2016 resolution to flip that ratio, while still sticking to the genres to which I gravitate, namely science fiction and popular science (with a bit of history thrown in).</description>
            <content type="html"><![CDATA[

<p>Two years ago, I made my New Years Resolution to read one fiction and one non-fiction book each month, and I’ve (mostly) kept it up since then. But towards the end of 2015, it suddenly occurred to me that the vast majority of the authors I read were men. So, I made my 2016 resolution to flip that ratio, while still sticking to the genres to which I gravitate, namely science fiction and popular science (with a bit of history thrown in).</p>

<p>Before I get to the list, I feel like these posts typically have a “what I learned from it” component. The only real difference I found was that there were far more female characters involved or female scientists highlighted — and for their research, not their sex.</p>

<p>I certainly didn’t lack for subject variety — I read about gravitational waves, extinction theory, Antarctica, parasites, machine learning and the women who were the first “computers” on the pop-sci side, to the absolutely-bonkers 14th century in Europe, the Great Migration, and the U.S. criminal justice system on the non-science history side. All the sci-fi novels I read were entertaining, thought-provoking, and full of rich world-building like any other good sci-fi — just with greater representation of women in the action.</p>

<p>I realize some people might think this is a useless exercise, or “reverse sexism.” I’d say it’s more in the model of Ruth Bader Ginsburg’s response to “When will there be enough women on the Supreme Court?” — when all nine justices are women. I went a year reading books that were ~90% by men (realistically, many more years than that), without it even registering. The goal should be to get to the point where it isn’t considered weird or “SJW” to read books 90% by women, as well.</p>

<p>The key point to me is visibility — the reality is that women, in many fields, struggle to gain visibility for their accomplishments. I’d assume for genres like popular science or science fiction that it’s similar to Hollywood, in that backing work by women is seen as a “gamble.” I’m aware that my individual Kindle purchase doesn’t amount to much, but if more people adopt a similar strategy and recommend these books to their avid-reader friends, then it starts to amount to something of significance.</p>

<p>Going forward, I’ll aim for closer to a <sup>50</sup>&frasl;<sub>50</sub> ratio and diversify my picks along other lines, such as ethnicity, religion, and gender identity. As anyone who reads a lot can likely attest, it’s always such a challenge to narrow down your next book selection from the thousands of options available. So to all who might potentially hate on this strategy, just view my method as a particularly socially-conscious selection engine.</p>

<p>Without further ado, here’s my reading list from this past year, including links to each book’s Amazon page so you can learn more — I make no illusions about being a masterful book reviewer, but just assume all of these books get a hearty thumbs up from me.</p>

<h2 id="non-fiction">Non-Fiction</h2>

<p><a href="https://www.amazon.com/gp/product/B004R1Q296">A Distant Mirror: The Calamitous 14th Century</a> by Barbara W. Tuchman</p>

<p><a href="https://www.amazon.com/gp/product/B006R8PHW0">Antarctica: An Intimate Portrait of a Mysterious Continent</a> by Gabrielle Walker</p>

<p><a href="https://www.amazon.com/gp/product/B017QLQLQW">Black Hole Blues and Other Songs from Outer Space</a> by Janna Levin</p>

<p><a href="https://www.amazon.com/gp/product/B00T3CU1ZK/">Dark Matter and the Dinosaurs: The Astounding Interconnectedness of the Universe</a> by Lisa Randall</p>

<p><a href="https://www.amazon.com/gp/product/B0067NCQVU">The New Jim Crow</a> by Michelle Alexander</p>

<p><a href="https://www.amazon.com/gp/product/B013CATQPY">Rise of the Rocket Girls: The Women Who Propelled Us, from Missiles to the Moon to Mars</a> by Nathalia Holt</p>

<p><a href="https://www.amazon.com/gp/product/B00EGJE4G2">The Sixth Extinction: An Unnatural History</a> by Elizabeth Kolbert</p>

<p><a href="https://www.amazon.com/gp/product/B011H55MY0">This Is Your Brain on Parasites: How Tiny Creatures Manipulate Our Behavior and Shape Society</a> by Kathleen McAuliffe</p>

<p><a href="https://www.amazon.com/gp/product/B003EY7JGM">The Warmth of Other Suns: The Epic Story of America’s Great Migration</a> by Isabelle Wilkerson</p>

<p><a href="https://www.amazon.com/gp/product/B019B6VCL">Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy</a> by Cathy O’Neil</p>

<h2 id="fiction">Fiction</h2>

<p><a href="https://www.amazon.com/gp/product/B00BAXFDLM">Ancillary Justice (Imperial Radch Book 1)</a> by Ann Leckie</p>

<p><a href="https://www.amazon.com/gp/product/B008HALO0U">Bloodchild: And Other Stories</a> by Octavia E. Butler</p>

<p><a href="https://www.amazon.com/gp/product/B005CRQ3MA/">Gravity’s Rainbow</a> by Thomas Pynchon (the token male author ;)</p>

<p><a href="https://www.amazon.com/gp/product/B003JFJHTS">The Handmaid’s Tale</a> by Margaret Atwood</p>

<p><a href="https://www.amazon.com/gp/product/B003JFJHTS">The Left Hand of Darkness</a> by Ursula Le Guin</p>

<p><a href="https://www.amazon.com/gp/product/B009LL3YRU">Ink</a> by Sabrina Vourvoulias</p>

<p><a href="https://www.amazon.com/gp/product/B00GU38B4S">Synners</a> by Pat Cadigan</p>

<p><a href="https://www.amazon.com/gp/product/B006VXFGJU">The Waves</a> by Virginia Woolf</p>
]]></content>
        </item>
        
        <item>
            <title>3 questions on cybersecurity that should be asked in the debates</title>
            <link>//swagitda.com/blog/posts/3-questions-cybersecurity-debates/</link>
            <pubDate>Fri, 30 Sep 2016 20:41:38 -0400</pubDate>
            
            <guid>//swagitda.com/blog/posts/3-questions-cybersecurity-debates/</guid>
            <description>A citizen’s guide (or to sound smart at cocktail parties) While the first U.S. presidential debate included an open-ended, broad question about each candidate’s stance on cybersecurity, it accomplished little in helping citizens understand the candidates’ actual policy positions — nor why cybersecurity policies are relevant at all on the national scale. Saying “cybersecurity is important” for the U.S. today is like saying “having a military is important.”
But, it was also clear from the responses, such as bringing up Daesh’s use of the internet for recruitment or using the term “the cyber,” that there’s a lack of mainstream understanding of what cybersecurity actually means in a policy context (the argument against the term “cyber” and its derivatives is left for another day).</description>
            <content type="html"><![CDATA[

<p><em>A citizen’s guide (or to sound smart at cocktail parties)</em>
<img src="/blog/img/code-murica-flag.jpg" alt="Murica flag, cyber edition" /></p>

<p>While the first U.S. presidential debate included an open-ended, broad question about each candidate’s stance on cybersecurity, it accomplished little in helping citizens understand the candidates’ actual policy positions — nor why cybersecurity policies are relevant at all on the national scale. Saying “cybersecurity is important” for the U.S. today is like saying “having a military is important.”</p>

<p>But, it was also clear from the responses, such as bringing up Daesh’s use of the internet for recruitment or using the term “the cyber,” that there’s a lack of mainstream understanding of what cybersecurity actually means in a policy context (the argument against the term “cyber” and its derivatives is left for another day).</p>

<p>Here’s my best attempt at a definition as far as most citizens are concerned: cybersecurity at the national level includes 1) methods and resources to conduct geopolitical, intelligence-gathering, or offensive operations, and 2) methods and resources to defend against digital attacks that might threaten national security, individual liberties, or our economic viability.</p>

<p>Simply put, cybersecurity is the newest domain both for warfare and way of life, and thus has policy implications at a national scale. It would be a grave mistake to underestimate the importance of cybersecurity in geopolitical strategy and how much our “life, liberty and the pursuit of happiness” depends on it today.</p>

<p>So, without further ado, here are the three questions I think are worthy of being asked at the next two debates, as well as why you, as a citizen, should care about their answers. If you think a question is worth asking, vote for it by clicking the relevant link below. (Author note: after the conclusion of the debates, the question site was disabled &ndash; these now link to the relevant sections in the post).</p>

<ol>
<li><a href="#crypto-backdoors">Do you support federally mandated encryption backdoors?</a></li>
<li><a href="#critical-infra">How would you improve protection of critical infrastructure from cyber attacks?</a></li>
<li><a href="#deterrence-offense">What balance would you strike between cyber deterrence and offensive cyber operations on other nations?</a></li>
</ol>

<hr />

<p><img src="blog/img/bad-cyberart-05.jpg" alt="Code dripping over the White House" /></p>

<h2 id="a-name-crypto-backdoors-a-do-you-support-federally-mandated-encryption-back-doors"><a name="crypto-backdoors"></a>Do you support federally mandated encryption back doors?</h2>

<h3 id="what-we-d-learn">What we’d learn</h3>

<p>The way the candidates answer this question primarily will show to what degree they are aligned to constitutional rights plus the needs and desires of citizens vs. what “the powers that be” (primarily the FBI) say is necessary. Secondarily, it will show how open they are to listening to expert opinions in a particular area, as the overwhelming majority of cybersecurity professionals are vehemently and publicly against encryption backdoors.</p>

<h3 id="the-context">The context</h3>

<p>There have been multiple encryption debates throughout the years, but the most recent focuses on encryption backdoors. Let’s start with a basic definition of encryption: it’s a “process of ciphering information in such a way that only authorized parties can read it.” It’s not hyperbole to say encryption is part of everything you use online — from online banking, online shopping, email, electronic medical records to Facebook chat. It is a fundamental part of what makes the internet economy as we know work by adding in a layer of trust.</p>

<p>Now, what’s a backdoor? A backdoor is an intentionally-placed method of bypassing a security mechanism in software, and is most often used to gain unauthorized access to something. In the context of encryption backdoors, it is specifically to obtain the “plaintext” or raw data. For example, encrypted data might look like “IUFdjxi/FI8+2zv/WbEUq=M+b…” while the plaintext says “I like pizza.”</p>

<p>By way of analogy, an encryption backdoor is similar to designing a physical lock with a master key that can always open the lock if needed. It’d be naive to assume that only the designated owner of the master key (for example, the government) could unlock the lock. Someone else could examine the way the lock is designed, deduce how the master key looks, and create one on their own.</p>

<p>The implications for an encryption backdoor are even worse than that analogy — at least in the physical lock case, there’s a slight barrier in that physical proximity is still needed to use the master key. In the digital case, a hacker doesn’t even have to move in order to use the master key across a bunch of different digital “locks” in any location in the world.</p>

<p>The FBI has been the most notable proponent of encryption backdoors, as highlighted in their <a href="/blog/posts/apple-vs-fbi-privacy-inequality/">battle with Apple earlier this year</a>. Further, in 2007, a <a href="http://www.nytimes.com/2013/09/06/us/nsa-foils-much-internet-encryption.html">backdoor was discovered in the encryption algorithm supported by the NSA</a>, which would have meant that companies who adopted the NSA’s recommend encryption algorithm would have developed software susceptible to attack or data interception. The argument in favor of encryption backdoors generally rests on the use of encryption by criminals or other bad actors, and the worry that encryption allows them to “go dark” (i.e. make it harder for someone to intercept or access their data).</p>

<p>However, the overwhelming majority of cybersecurity experts are against backdoors, primarily because there’s absolutely no way to ensure that these “trap doors” aren’t discovered by hackers, criminals or combative nation-states and used against American citizens, corporations, banks, utilities, troops or the government itself. It cannot be stressed enough that the <a href="http://docs.house.gov/meetings/IF/IF02/20160419/104812/HHRG-114-IF02-Wstate-BlazeM-20160419-U3.pdf">“harsh technical realities make a [lawful access only] solution effectively impossible.”</a></p>

<p>Requiring encryption backdoors also would place a huge financial and resource burden on private enterprises by requiring software developers to design systems in a way that allows law enforcement to gain access as needed — or desired. Further, no matter how you decide who is granted access to the “master key” for these backdoors, they immediately become an attractive and lucrative target for cyberattack — potentially pouring many millions of dollars of extra risk onto the shoulders of private enterprises.</p>

<h3 id="why-you-should-care">Why you should care</h3>

<p>If you’d include yourself among people who care about the following, <em>you should be strongly against requiring — or the even existence of — encryption backdoors:</em></p>

<ul>
<li>The First Amendment, free speech and freedom of the press</li>
<li>The Second Amendment (encryption software has historically been classified as a munition — a military weapon — by the government, which means that citizens arguably have the right to use encryption to defend their personal data, without it being rendered ineffective due to a backdoor)</li>
<li>The Fourth Amendment (keeping data safe in event of an “unreasonable search and seizure”)</li>
<li>Criminal justice reform</li>
<li>Eliminating discrimination</li>
<li>Stopping people from stealing your personal data or assets</li>
<li>Stopping people from stealing corporate data or assets</li>
<li>Protecting critical infrastructure</li>
<li>Protecting hospital systems and medical devices</li>
<li>Keeping our troops safe</li>
<li>and many other things, but I have a tendency to ramble as-is</li>
</ul>

<p>As you can recognize from the list, this isn’t a partisan issue.</p>

<p>Many people use the “I have nothing to hide” argument when first hearing about the encryption debate. That also happens to be irrelevant — given the prevalence of digital communications in our modern lives, encryption is essential in preserving our constitutional rights.</p>

<p>But it’s also way beyond that. As I mentioned above, encryption is used in nearly everything you do online these days, and not just your communications. Purposefully backdooring encryption leaves an open hole for hackers to get your healthcare data, personal pictures of your kids, drain your bank account, run up your credit card, or steal your identity. <strong>The vibrant, useful, trillion-dollar internet economy as we know it would not and could not exist without encryption.</strong></p>

<p>As Matt Blaze, a leading expert on encryption, said in <a href="http://docs.house.gov/meetings/IF/IF02/20160419/104812/HHRG-114-IF02-Wstate-BlazeM-20160419-U3.pdf">his recent testimony before Congress</a>:</p>

<blockquote>
<p>This is not simply a matter of weighing the desires for personal privacy and for safeguards against government abuse against the need for improved law enforcement… [Backdoors] will provide rich, attractive targets not only for relatively petty criminals such as identity thieves, but also for organized crime, terrorists, and hostile intelligence services. It is not an exaggeration to understand these risks as a significant threat to our economy and to national security.</p>
</blockquote>

<hr />

<p><img src="blog/img/bad-cyberart-06.jpg" alt="Power lines, but with code on them for some reason" /></p>

<h2 id="a-name-critical-infra-a-how-would-you-improve-protection-of-critical-infrastructure-from-cyber-attacks"><a name="critical-infra"></a>How would you improve protection of critical infrastructure from cyber attacks?</h2>

<p><em>With the follow-up: “Would you include election and electronic voting systems under the definition of critical infrastructure?”</em></p>

<h3 id="what-we-d-learn-1">What we&rsquo;d learn</h3>

<p>Each candidate would outline their plans for for the federal government’s role in protecting critical infrastructure. Additionally, we’d hear each candidate’s proposals for addressing and solving some of the key challenges in protecting critical infrastructure in order to judge how much they recognize the threat and how effective they’d be in preserving our national security, economy and way of life.</p>

<h3 id="the-context-1">The context</h3>

<p>Critical infrastructure, as per the Patriot Act, is defined as:</p>

<blockquote>
<p>systems and assets, whether physical or virtual, so vital to the U.S. that the incapacity or destruction of such systems or assets would have a debilitating impact on security, national economic security, national public health or safety, or any combination of those matters</p>
</blockquote>

<p>The NIST Cybersecurity Framework suggests a host of industries fall under this label, including agriculture, water, public health, emergency services, government, defense, information &amp; telecommunications, energy, transportation &amp; shipping, banking &amp; finance, chemicals &amp; hazardous materials, post, national monuments &amp; icons and critical manufacturing.</p>

<p>Many would argue that the list should also include election and electronic voting systems, as they are a vital component of maintaining democratic elections (and I personally would agree). Particularly in light of the recent revelations that Russian actors <a href="https://www.wired.com/2016/07/heres-know-russia-dnc-hack/">hacked the DNC</a> as well as the <a href="https://www.washingtonpost.com/world/national-security/fbi-is-investigating-foreign-hacks-of-state-election-systems/2016/08/29/6e758ff4-6e00-11e6-8365-b19e428a975e_story.html">Illinois and Arizona election systems</a> (and <a href="http://abcnews.go.com/US/russian-hackers-targeted-half-states-voter-registration-systems/story?id=42435822&amp;cid=abcn_tco">attempted to hack many more</a>), the plea by information security experts to have the security of voting systems be taken more seriously is steadily gaining legitimacy.</p>

<p>The reason why federal-level protection of critical infrastructure from cyber attacks is up for debate is that, currently, the onus is primarily on the private sector to defend itself. However, the same isn’t true for physical threats such as potential terrorist attacks — should the owner of the World Trade Center have conducted their own anti-terrorism operations and had fighter jets ready to escort a hijacked plane? Of course not.</p>

<p>If the federal government is in charge of protecting national security, then it’s logical to suggest that they should also take the lead on <strong>all</strong> national security, including securing national infrastructure. However, given the complexities of our physical and virtual infrastructure, and consequently the large number of industries that fall under the critical infrastructure label, there is disagreement over the extent to which the federal government should help bolster their cybersecurity. We, as citizens, should hear what the candidates respective positions are on this important issue.</p>

<p>Additionally, the <em>how</em> to do it is potentially more subjective and could include anything from recommending minimal security standards (which is the default, albeit ineffective, strategy) to imposing fines on software vendors for vulnerabilities or conducting cyber deterrence (which I dig into more in the last question). I, for one, would like to know the candidates ideas on the “how to” as well.</p>

<h3 id="why-you-should-care-1">Why you should care</h3>

<p>If you care about our national security, you should care about this question. It’s safe to say it’s a bipartisan desire for the local power plant not to blow up, to avoid a food crisis or not have our financial system come to a standstill.</p>

<p>The reason why you should care about each candidate’s specific answer to the question is because the level of cybersecurity in critical infrastructure is, in general, alarmingly poor, increasing the likelihood and severity of devastation of a cyber attack on critical infrastructure at any time. While I don’t condone <a href="https://en.wikipedia.org/wiki/Fear,_uncertainty_and_doubt">FUD</a> (fear, uncertainty and doubt as a media strategy), I will say that it’s far better to act now to reduce the probability of a calamitous digital attack on our critical infrastructure than keep our fingers crossed that it won’t happen.</p>

<p>There are a few cybersecurity challenges faced by these industries, though. First, there’s a massive shortage of talent, and those who are practitioners generally go to industries who can pay the most (like tech and financial services). Perhaps workers in declining industries could be given incentives to retrain with cybersecurity skills. Second, critical infrastructure systems usually are complex and a lot of infrastructure software is old, and it’s difficult to install or integrate security measures after the fact.</p>

<p>Third, a single private entity won’t have the same level of information about potential digital threats as the federal government, nor the resources to prevent against every possible scenario. By leveraging the U.S. intelligence community’s data, private entities in critical industries could be given a “heads up” on potential threats and guidance on the tactics, techniques and procedures of groups likely to target them in a cyberattack.</p>

<hr />

<p><img src="blog/img/bad-cyberart-07.jpg" alt="AFB Central Control facility" /></p>

<h2 id="a-name-deterrence-offense-a-what-balance-would-you-strike-between-cyber-deterrence-and-offensive-cyber-operations-on-other-nations"><a name="deterrence-offense"></a>What balance would you strike between cyber deterrence and offensive cyber operations on other nations?</h2>

<p><em>With the follow-up: “What role do you think cyber deterrence plays in cyberwarfare?”</em></p>

<h3 id="what-we-d-learn-2">What we&rsquo;d learn</h3>

<p>The candidates’ answers to this question should reveal:</p>

<ol>
<li>How they’d invest in and preserve our advantage in the “cyberwar” arena — from technology to human capital</li>
<li>How they’d use offensive cyber operations — would it be covert geopolitical influencing or upfront display of capability? Would we be the first-movers on offense or focused on attacking back?</li>
</ol>

<h3 id="the-context-2">The context</h3>

<p>The domains of warfare were traditionally Land, Sea, Air and Space, but Information Operations (i.e. the digital domain) became the fifth dimension for the U.S. Military in 1995. Since then, information operations, or “cyberwarfare” as dubbed by the media, has become a crucial component of military strategy due to the proliferation of digital systems globally and their importance in all areas of modern life.</p>

<p>The two main types of cyberwarfare are espionage and sabotage. Espionage is used for spying purposes to gain intelligence; for example, the <a href="http://arstechnica.com/security/2015/06/epic-fail-how-opm-hackers-tapped-the-mother-lode-of-espionage-data/">hack of the Office of Personnel Management</a> (presumably by China) was to gain intelligence on people who work for various U.S. government agencies. Sabotage is used to disrupt adversaries’ systems for geopolitical or military gain. For example, rather than conducting some sort of strike on Iran’s nuclear facilities, the U.S. leveraged its offensive cybersecurity capabilities to covertly disrupt Iran’s nuclear program in an attack later dubbed <a href="https://en.wikipedia.org/wiki/Stuxnet">“Stuxnet.”</a></p>

<p>Cyberwarfare is particularly reliant on intelligence (part of why the NSA has expanded so much over the past two decades), and thus most operations tend to fly under the radar. It would reduce a government’s advantage to reveal capabilities or methods, since then adversaries could better thwart attacks or repackage the attack for their own use.</p>

<p>This highlights the difficulty of cyber deterrence. Being able to attribute cyber attacks to a specific nation-state requires revealing, in part, how you were able to figure out who did it. If you don’t present evidence, it can be dismissed as a baseless accusation, which isn’t great for geopolitical maneuvering. Even then, <a href="https://medium.com/@thegrugq/idle-thoughts-on-cyber-82170b2b7280#.k2d7gfrsr">attribution is notoriously difficult</a> since attackers can attempt to mask their digital tracks, including by making it appear that their attack originated from a different location or by using a different language than their own.</p>

<p>In any case, to dissuade adversaries from attacking us, the U.S. has to make it clear that the intelligence community will figure out who is behind any attacks against us, retaliate swiftly and inflict significant damage…all without revealing the extent of our capabilities.</p>

<h3 id="why-you-should-care-2">Why you should care</h3>

<p>It is evident that the U.S. currently has a decisive advantage in the nation-state cybersecurity arena — anyone suggesting otherwise, as seen in this election, is misinformed. We began preparing for, and conducting, offensive cyber operations about a decade before others, giving us a significant head start.</p>

<p>Further, the dominance we have over global digital infrastructure is extremely difficult to replicate and that fact makes our cyber operations smoother to conduct. For example, <a href="http://www.theatlantic.com/international/archive/2013/07/the-creepy-long-standing-practice-of-undersea-cable-tapping/277855/">as revealed by the Snowden leaks</a>, the U.S. taps into undersea fiber optic cables that serve as the fundamental communication rails of the internet — giving access to any data that is transmitted over these cables.</p>

<p>To be clear, Russia, China and Iran all have highly intelligent and capable cybersecurity teams (to varying degrees of size and sophistication). But we can conduct offensive cybersecurity operations on a bigger scale. We not only can perform equally as sophisticated attacks, but we also possess a formidable information advantage to better craft attacks and anticipate attacks against us.</p>

<p>This doesn’t mean we’ll never be attacked, due to the aforementioned abilities of our adversaries — though the cyber deterrence strategy is meant to dissuade others from attacking us by showing our muscle. While we currently have superior offensive cybersecurity capabilities that give us a geopolitical advantage, this does not make us invulnerable to the potentially devastating effects of cyberattack against us by a capable nation-state.</p>

<p>On the other hand, an offensive operation presents the risk of being caught, which might be viewed as a declaration of war — thus leading to retaliation against us (which isn’t ideal). So, we have to be judicious in how we leverage our offensive cybersecurity capabilities to balance optimizing our foreign policy goals while protecting our own national security.</p>

<hr />

<p><img src="blog/img/bad-cyberart-08.jpg" alt="Map of the USA but with code on it" /></p>

<h2 id="conclusion">Conclusion</h2>

<p>Do I think these questions will be asked at the debates? No (but fingers crossed). I don’t think there’s a sufficient public understanding of the multifarious policy issues presented by cybersecurity — largely because the media coverage of cybersecurity is notoriously terrible.</p>

<p>However, raising voter awareness of these issues still is critically important. Real change won’t happen if it’s just the information security or privacy community who is concerned…and it really shouldn’t just be them, since cybersecurity issues affect all citizens.</p>

<p>Cybersecurity’s importance in our nation’s ecosystem only will grow, so starting the discussion of these issues now means there’ll be a deeper consciousness of them among voters in the next election — and a greater ability of “the people” to ensure their rights and security are preserved as we march past the point of no return into digital dependence.</p>
]]></content>
        </item>
        
        <item>
            <title>Behavioral Models of InfoSec: Prospect Theory</title>
            <link>//swagitda.com/blog/posts/behavioral-models-infosec-prospect-theory/</link>
            <pubDate>Mon, 01 Aug 2016 20:20:59 -0400</pubDate>
            
            <guid>//swagitda.com/blog/posts/behavioral-models-infosec-prospect-theory/</guid>
            <description>To those in the information security / cyber security industry, it’s an accepted truth that there exists a pernicious incentive structure that overwhelmingly puts the odds in the attacker’s favor. The consistent narrative is that defenders make irrational decisions and focus on the wrong problems while vendors peddle FUD and snake oil that not just fails to bolster the defensive cause, but inflicts ongoing harm.
But, I’ve seen less in the way of seeking to understand defenders’ irrational decision making patterns and why the industry is the way it currently is…and even less about how to fix this toxic feedback loop.</description>
            <content type="html"><![CDATA[

<p>To those in the information security / cyber security industry, it’s an accepted truth that there exists a pernicious incentive structure that overwhelmingly puts the odds in the attacker’s favor. The consistent narrative is that defenders make irrational decisions and focus on the wrong problems while vendors peddle FUD and snake oil that not just fails to bolster the defensive cause, but inflicts ongoing harm.</p>

<p>But, I’ve seen less in the way of seeking to understand defenders’ irrational decision making patterns and why the industry is the way it currently is…and even less about how to fix this toxic feedback loop. So, armed with my modest background in behavioral economics from undergrad, I’ve decided to take a stab at examining the “why” and proposing some ways to twist these incentives in the defense’s favor.</p>

<p>My hope is that this kicks off a series where I examine different theories within behavioral economics against evidence within infosec. The tl;dr background on behavioral econ is that traditional economics views people as rational decision-making machines (i.e. “Homo economicus”) that can perfectly perform cost benefit analyses and choose an objectively optimal outcome.</p>

<p>Behavioral econ, in contrast, recognizes that our brains are wired in a way that has been optimal for evolution, so it measures how people actually behave vs. how they optimally behave. We have quirks in our thinking that result in us making “irrational” decisions, but for understandable reasons.</p>

<p>This post will cover the O.G. theory in behavioral econ, Prospect Theory, as the first of many (potential) theories to help explain some of the dynamics of the infosec market.</p>

<h3 id="table-of-contents">Table of Contents:</h3>

<ol>
<li><a href="#what-is-prospect-theory">What is Prospect Theory?</a></li>
<li><a href="#defense-vs-offense">Defense vs. Offense</a></li>
<li><a href="#infosec-ref-points">InfoSec Reference Points</a></li>
<li><a href="#infosec-examples">Empirical Examples from InfoSec</a></li>
<li><a href="#infosec-incentives">Incentives in InfoSec</a></li>
<li><a href="#fixing-incentives">Fixing Incentives</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ol>

<h2 id="a-name-what-is-prospect-theory-a-what-is-prospect-theory"><a name="what-is-prospect-theory"></a>What is Prospect Theory?</h2>

<p><a href="https://en.wikipedia.org/wiki/Prospect_theory">Prospect Theory</a> is a theory in behavioral econ that helps explain how people make decisions between options that bear certain probabilities and risk. The main thesis in Prospect Theory is that people make decisions by evaluating potential gains and losses through the lens of probability, rather than looking at the final, “objective” outcome. This relies on the decision-maker setting a reference point against which they measure outcomes.</p>

<p>Let’s consider a simple example to get a better sense of what this means in practice, using data from the original paper on Prospect Theory:</p>

<p>Decision #1: A) 100% chance of receiving $3,000 vs. B) 80% chance of receiving $4,000, but a 20% chance of receiving nothing</p>

<p>A’s expected outcome is $3,000 while B’s is $3,200…but 80% of subjects choose option A because it represents a guaranteed gain. Homo Economicus would scoff at these silly people and choose B.</p>

<p>Decision #2: C) 100% chance of losing $3,000 vs. D) 80% chance of losing $4,000, but a 20% chance of losing nothing</p>

<p>C’s expected outcome is losing $3,000 while D’s is losing $3,200. Homo Economicus naturally chooses C, but turns out 92% of people choose D for having the small chance of losing nothing.</p>

<p><img src="/blog/img/prospect-theory-01.jpg" alt="A standard Prospect Theory graph" /><em>A standard Prospect Theory graph</em></p>

<p>People are inconsistent in their choices based on whether decisions result in a loss or gain, as well as how the decisions are framed. There are four key tenets resulting from Prospect Theory that I’ll examine with the lens of infosec:</p>

<ol>
<li><strong>Reference dependence</strong>: decision makers use a reference point to measure relative gains and losses</li>
<li><strong>Loss aversion</strong>: people really don’t like experiencing losses, and losses hurt 2.25x more than gains feel good</li>
<li><strong>Non-linear probability weighting</strong>: people tend to overweight small probabilities and underweight big ones, and they also like certainty</li>
<li><strong>Diminishing sensitivity</strong>: the farther an outcome is above or below the reference point, the less its marginal effect</li>
</ol>

<h2 id="a-name-defense-vs-offense-a-defense-vs-offense"><a name="defense-vs-offense"></a>Defense vs. Offense</h2>

<p><img src="/blog/img/bad-cyberart-01.jpg" alt="Toy soliders fighting on a keyboard" /><em>Get ready for more terrible cyber art throughout the post</em></p>

<p>Through the lens of Prospect Theory, my own theory is that defenders operate in the “realm of losses” while attackers operate in the “realm of gains.” As shown above, people in the domain of losses tend to be more risk-seeking, while those in the gain domain tend to be risk averse. In fact, losses felt by those in the gain domain are overvalued by 3:1 relative to those in the loss domain. The further defenders get away from their reference point, the more they’ll opt for small probabilities of a big leap closer to it instead of more certain, incremental improvements — that is, become more risk-seeking and pay more attention to potential payoffs rather than probabilistic outcomes.</p>

<p>Defenders take awhile to readjust their point of reference to match the status quo, which can really screw up their decision-making process; if potential outcomes are computed relative to the reference point, an outdated reference point will reinforce risky decision-making as defenders keep trying to jump back up to it. Attackers, on the other hand, will quickly update their reference point to the status quo. Given their predilection towards risk aversion and emphasis on weighing the probability of different outcomes, attackers need a technical and informational advantage to feel confident in their decision.</p>

<h2 id="a-name-infosec-ref-points-a-infosec-reference-points"><a name="infosec-ref-points"></a>InfoSec Reference Points</h2>

<p><img src="/blog/img/bad-cyberart-02.jpg" alt="Arrows that look oh-so-cyber" /><em>Look, they&rsquo;re pointing</em></p>

<p>In order to figure out the behavioral predilections of defenders and attackers within the infosec arena, we need to determine the reference points that guide their behavior. My theory on infosec reference points is the following:</p>

<ul>
<li><strong>Defenders’</strong> reference point is a security posture in which they can only withstand set Z of attacks, do not experience any materially significant breaches (e.g. those requiring disclosure), and spend $X on products to meet minimum compliance standards: Domain of Losses</li>
<li><strong>Attackers’</strong> reference point is successfully compromising a target for $X cost without being caught before achieving their goal with value $Y: Domain of Gains</li>
</ul>

<p>Therefore, we have the following conclusions on losses and gains for each party:</p>

<ul>
<li><strong>Defenders</strong> feel a loss when they are breached with set Z of attacks, experience a significant breach, or spend more on security products than the minimum needed to meet compliance requirements. The gain from spending less than $X to meet compliance standards is realistically trivial. The non-trivial gain is from successfully stopping attacks that are not included in set Z (i.e. those they assume they can’t withstand); for example, an advanced remote code execution exploit involving a sandbox escape, kernel privilege escalation and a payload that disables endpoint protection products.</li>
<li><strong>Attackers</strong> feel a loss whenever they are caught or when their cost of $X is greater than their outcome of $Y, and feel a gain if they either spend less than $X on an attack or have a greater outcome than $Y. Note, a gain here would include exploits that work across multiple platforms or malware that can be repackaged easily, since it’s reducing the marginal cost of $X for crafting each attack and is thus a superior use of the attacker’s development time. For example, an exploit for a design flaw, architectural weakness, or logic-based vulnerability is usually cross-platform, reliable (vs. memory corruption) and very likely will take longer to fix — all of which means it has a larger payoff for the time invested in its development.</li>
</ul>

<h2 id="a-name-infosec-examples-a-empirical-examples-from-infosec"><a name="infosec-examples"></a>Empirical Examples from InfoSec</h2>

<p><img src="/blog/img/threatbutt-map.PNG" alt="Threatbutt's Cyber Attribution Map" /><em>ThreatButt: the #1 must-have, best-of-breed, military-grade enterprise cyber defense-in-derpth platform</em></p>

<p>It’s important to highlight some examples of “irrational” behavior within infosec as a frame of reference for general theory, specifically focusing on differences in adoption (and hype) of various defensive security products. Irrational can be a subjective term, so I mean it in both the “counter to one’s own benefit” way and the “most outside observers think this is illogical” way.</p>

<p>Let’s start with EMET, Microsoft’s Enhanced Mitigation Experience Toolkit, a free tool that helps prevent software exploitation on Windows. Installing it and configuring commonly used applications with ASLR, DEP and other countermeasures significantly increases the difficulty of successfully compromising an application. While there are no official statistics, it’s widely accepted that EMET adoption rates are very low, despite it being free and well-tested.</p>

<p>In the years following the initial release of EMET, some of its features and functionality slowly crept into mainstream operating system releases, where their efficacy forced attackers to move to Office macros — a decision that involved attackers accepting the risk of savvy users who wouldn’t enable the macros rather than investing time in developing and retooling exploits to work in a post-EMET world. This is a good example of attacker risk aversion; they prefer to go for the fluffier target that requires less fancy exploitation, but still has a wide impact. Similarly, Java historically made a fantastic target for attackers because of its uniformity. Attackers could simply write their attack once and reuse it, which made it appealing from a ROI perspective.</p>

<p>Two-factor authentication (2FA) is another example of a solution that isn’t “sexy” per se but should receive greater hype relative to its defensive impact. It’s a low cost solution that’s easily deployed (particularly relative to most security products), and meaningfully bolsters account security beyond just passwords. Yet, it’s taken 7 years to get to the point where it is being widely acknowledged as a standard tool to have in the security arsenal — and adoption still isn’t ubiquitous among the largest consumer-facing firms, despite how inexpensive and simple it is.</p>

<p>Just take a look at <a href="https://twofactorauth.org/">the list of the firms who do and don’t have 2FA</a> to see how many notable companies don’t have it yet. And, among the financial services firms who don’t, it’s a somewhat solid bet that they do have a FireEye box, Bromium or some other anti-APT tech which is vastly more expensive and helps against much lower-probability attacks.</p>

<p>The rise of ransomware and how little has been done to preemptively stop its growth and potency is also perplexing. According to PhishMe, 93% of all phishing emails now contain ransomware. McAfee says there were nearly 1.2 million new ransomware kits in Q1 2016 alone, the total nearing 6 million. It’s an unsophisticated attack that can easily be conducted by the 13 year old in Romania using basic malware kits, presenting a high ROI to the attacker. But given the prevalence and impact of ransomware, it seems irrational that companies are not doing more to protect against it.</p>

<p>Part of this is cleverness by the attacker in making the ransom’s cost low enough to not cause their targets to take drastic measures, but high enough that over a big enough target base, it results in lots of cash against a one-time upfront cost they can amortize over the lifetime of the attack. However, it’s more likely an element of defense being slow to update their reference points; companies could still be adopting relatively low-cost solutions and strategies to better defend themselves against ransomware, such as email protection, filesystem canaries, or even just a better backup process. All three of those solutions would benefit any organization beyond just becoming more resilient against ransomware, and yet they remain some of the most “boring,” underlooked categories.</p>

<p>Canaries in general, in fact, are a smart idea. Yet at only 4-figures per box, they are criminally under-adopted relative to 6-figure anti-APT boxes. It’s pretty straightforward: set up something that looks like a juicy target for an attacker, and get alerted when there’s suspicious activity. It helps give you early breach detection, inform your threat model and better understand attacker behaviors, all for a reasonable price. But adoption is very far from ubiquitous. Unfortunately it doesn’t have hand-wavy technology that “stops” advanced attacks — it comes across more like a mouse trap with cheese than a sexy elaborate laser tripwire maze.</p>

<p>As a final example, application whitelisting is a highly effective, albeit mundane technology. Plenty of organizations are still being compromised with new executables running, something easily thwarted by whitelisting. However, there’s a lower probability of catching an “elite” attack, given it’s likely to exploit an application directly. Critics will say that whitelisting reduces flexibility and bears a non-trivial amount of upfront setup, which is fair until you consider how difficult “sexy” tech, commonly using kernel-level modules, is to implement.</p>

<h2 id="a-name-infosec-incentives-a-incentives-in-infosec"><a name="infosec-incentives"></a>Incentives in InfoSec</h2>

<p><img src="/blog/img/bad-cyberart-03.jpg" alt="It's a compass whose needle is pointing to &quot;security&quot;" /><em>Helps determine the direction of a cyber object from the observer</em></p>

<p>With the above as a reference, I’m going to walk through each of the four key tenets and examine their likely implications in infosec, and how they can explain the “irrational” decision making that many bemoan.</p>

<h3 id="reference-dependence">Reference Dependence</h3>

<p>While it’s (mostly) simple accounting for defenders to know how much is spent on compliance, it’s a lot harder to know your organization’s security posture. Attackers can rely on (mostly) simple accounting to tally their cost and probably guesstimate the value of a successful attack, particularly if it’s selling personal data for $X per user vs. a nation state calculating how much crippling an enemy’s nuclear facility is worth to national security. Defenders, in contrast, can’t tally their costs as they go.</p>

<p>Figuring out your security posture is complicated for a few reasons. First, there are no sufficient industry benchmarks for security health against which organizations can compare themselves. Second, it’s highly unlikely that organizations will have full situational awareness to know which attacks are working against them and which they’re successfully thwarting. Third, defenders aren’t always sure what the “spoils of war” are, i.e. what value an attacker gains from hacking them, from customer data, intellectual property even to something like carbon credits. When it’s difficult to know what’s at risk, it’s difficult to weigh risk.</p>

<p>And, updating the reference point is a slow process for defenders. If their reference point is their perception of their security posture from 2014, it’s now outdated by two years at the minimum, during which attackers assuredly developed new techniques. Even once the reference point is updated to the status quo, the uncertainty in measuring organizational security risk and health means the new reference point will be equally as fuzzy. Just think about the ransomware example; if the reference point were based on today’s most probable threats, adopting technologies to prevent it should be a top budget priority.</p>

<p>Attackers, however, are quickly updating their reference points and evolving their methods based on the true status quo rather than their prior perception. Because the reference point serves as the foundation for decision making under prospect theory, the fact that attackers have more timely and accurate reference points gives them a decisive advantage at stage 1 over defenders.</p>

<h3 id="loss-aversion">Loss Aversion</h3>

<p>We know that losses hurt 2.25x more than gains, and that attackers weigh losses 3x as much as defenders; to be a bit simplistic, the attacker’s “exchange rate” for gains and losses is therefore 1: 6.75. Defenders “just” need to make sure that for each additional dollar attackers spend towards breaching them, they’re getting less than $6.75 in additional value (I’ll discuss how defenders can do so in the last section).</p>

<p>As mentioned in the EMET example, attackers were probably inclined to switch to less arduous targets once it was released just based on the assumption that organizations would have adopted it, <em>even though there wasn’t yet evidence of adoption</em>. As a free tool, adopting it couldn’t present an easier, cost-effective opportunity for defenders to play into attacker’s loss aversion.</p>

<h3 id="non-linear-probability-weighting">Non-linear Probability Weighting</h3>

<p>Both sides overweight small probabilities and underweight large ones. Defenders are predisposed towards following small probabilities of a better outcome (risk-seeking) while attackers will care more about certainty and shun options that have smaller probabilities of worse outcomes (risk-averse).</p>

<p>To feel confident in their abilities to pwn their target, attackers need a strong reference point and the ability to calculate the probabilities of different outcomes. The more information the attacker has about the target, the better they can predict probability, and the greater their technical abilities, the better they can minimize the probability of being caught. Consequently, playing with attackers’ sense of certainty is another tactic defenders can use.</p>

<p>In defensive decision making, it’s crucial to understand the impact and probability of an attack on your organization. There’s a reason why there’s been a collection of attempts to come up with a framework for information security risk-weighting — it’s vital, but an arguably unattainable goal. The variables are prohibitively multifarious, from the company’s industry, technology stack, business model, brand power, etc. to attacker motives, current malware landscape, or even geopolitical statuses.</p>

<p>It’s safe to assume that it’s an impossible task to enumerate all attacks and calculate each of their probabilities and impacts. Industry data is pragmatic since it provides a reasonable reflection on what attacks are most likely. There’s also some data to provide historical precedents on impacts; for example, there’s minimal impact to stock prices, but potentially longer-term impact to sales that ends up affecting stock prices (like in Target’s case). This still leaves the defender left to determine whether they’re robust enough to withstand these different types of attacks .</p>

<p>Now, remember that loss domain-ers will overweight small probabilities and my hypothesis that the only “gain” a defender can really have is stopping attacks that they did not think they could. This can easily support why information security is saturated with products that stop APT or “advanced” attacks, while companies are still getting popped with “basic” methods like phishing, simplistic web app vulnerabilities and outdated, repackaged malware. The tools I mentioned above, such as 2FA, canaries and whitelisting help stop the large-probability, quotidian attacks and thus don’t present an opportunity for a “gain.”</p>

<p>Such a limited potential for a gain facilitates greater emotional basis for action as well, such as Clausewitz’s “passionate hatred for the enemy.” It’s no wonder, then, that attribution is so popular while being functionally useless — at least defenders can have some respite that the culprits were found. But I believe it’s more than that; giving a “face” to the attackers provides a greater sense of certainty, however false that feeling might be. And if I’m generous, nicely bound reports on threat group “[clever noun describing the target group] + [noun of Chinese-associated thing]” detailing <a href="https://en.wikipedia.org/wiki/Terrorist_Tactics,_Techniques,_and_Procedures">TTPs</a> might actually help defenders improve their probability weighting of what attacks they’re likely to incur.</p>

<h3 id="diminishing-sensitivity">Diminishing Sensitivity</h3>

<p>As defenders experience losses, they experience less “pain” for each additional instance. A big, acutely painful breach will more likely lead to action (of the risk-seeking kind) rather than death by a thousand paper cuts, which fully plays into diminishing sensitivity — each time a defender is hacked via a “stupid” bug, they’ll care less and less, so they’ll be less inclined to adopt security products that stop the repeated, lesser attacks (such as 2FA or canaries).</p>

<p>Another issue is that the outcome for defenders is often all or nothing. For example, if an attacker bypasses ASLR, the yield is 100% of the app; there’s no gray area where only part of the app is compromised, meaning from the defender’s point of view, it’s either a total loss or no loss. By this I mean, if an attacker has a <sup>1</sup>&frasl;<sub>10</sub> chance of guessing an address layout, the app is not 90% protected, and if an attacker guesses correctly, the app is 100% compromised. Thus, the impact of this 100% loss is the initial hit, and any subsequent hits don’t feel nearly as severe in comparison.</p>

<p>This disparity between losses is why incident response is such a lucrative business; when defenders are violently thrust deeper into the loss domain, they’re much more willing to spend whatever money necessary to get closer to their reference point again. This takes the form of expensive services or products that the IR providers say will help avoid this big, nasty pain they’re feeling…although this dynamic is often decried as predatory.</p>

<p>On the attacker side, achieving increasingly awe-inspiring levels of leetness loses its splendor after awhile; that is, there’s less motivation to strive for either an extra level of cost reduction or getting more value out of the attack. However, the initial gain leap can still be appealing, and is where I’d argue a lot of innovation happens…it’s just that there isn’t much incentive to continue to innovate.</p>

<p>This explains a few observations. First, that you commonly see the same attack being repackaged rather than completely new methods being used during a campaign. Second, wildly innovative, “great leap forward” vulnerability research is more common once some sort of new protection is developed and deployed (like ROP being used to work around a non-executable stack/heap), and less common when the status quo attacks can do just fine (like users plugging in shiny USBs they find in the parking lot).</p>

<p>What this also means, combined with attackers being more risk averse, is that reaching the next gain level will decreasingly justify the risk tradeoff. This is yet another benefit to the defense, since it can help deter ongoing campaigns even after an initial compromise — if you can up the cost of persistence, then developing tools for retaining system access on the target system will feel too risky relative to the lower gain payoff.</p>

<h2 id="a-name-fixing-incentives-a-fixing-incentives"><a name="fixing-incentives"></a>Fixing Incentives</h2>

<p><img src="/img/infosux.png" alt="Infosux comic" /></p>

<p>Now that I’ve tried explaining the <em>why</em>, it’s time to discuss how the balance of decision-making power can be shifted in favor of defense and some examples of tech that makes more sense to adopt. Clearly, defense is naturally predisposed to misjudging their real threat model, misallocating resources and miscalculating strategies, resulting in our current industry dystopia of a comically privileged offense, FUD marketing tactics, focus on thwarting sexier “advanced” attacks and a noxious romance with attribution.</p>

<p>Understanding you have a problem, what the problem is, and why you keep having it is step one. I’m not alone in using knowledge of behavioral econ to counter my human instincts towards suboptimal behavior (e.g. <a href="http://waitbutwhy.com/2013/10/why-procrastinators-procrastinate.html">instant gratification monkey</a>). So, I fully believe that defenders can leverage the knowledge of their weaknesses to correct their missteps and start leveraging their adversaries’ weaknesses against them.</p>

<p>If you’ve spoken to anyone in infosec with offensive experience, they’ll agree that “raising the cost of attack” is one of the most effective means of deterrence. I think re-framing it as “raising the stakes of attack” is more descriptive than cost, since it includes the notion of risk. The fact that attackers only care about their own outcome relative to the reference point, are extremely loss averse, prioritize certainty over a more valuable outcome and get less benefit out of successive gains all supports the idea of raising the stakes.</p>

<p>Defenders should prioritize efficiency when raising the stakes. Rather than focusing on less probable attacks, they should think about the commonalities between the technical and informational advantages that the spectrum of attackers possess. For example, a platform like Drawbridge Networks lets you detect and control lateral movement in an internal network, which could limit an attack’s impact in both more advanced attacks and common malware. Defenders often believe that cyber security products focused on countering “advanced” threats also counter more basic attacks, but that’s not always the case. It’s far simpler to raise the level of the lowest common denominator than try to stop each type of “sophisticated” method.</p>

<p>Eroding the informational advantage is the wisest move, since tackling the technical advantage is more of a cat-and-mouse game. “Silent” monitoring tech that gives visibility without informing the attacker can give defense the ability to respond quickly without the attacker realizing that they’ve been caught, so defenders can watch the attacker’s methods and gain valuable threat intelligence (the real kind). In contrast, technologies that use blocking are giving data to attackers that they can use to craft a better attack.</p>

<p>An effective technique that’s gaining some popularity is ensuring that the organization’s infrastructure isn’t static; attackers will have a substantially more difficult time attacking something that is constantly changing. Even more simplistically, setting up honey pots and other types of deception, like <a href="https://canary.tools/">Thinkst’s Canary</a>, can serve to foster uncertainty in the attacker as well as give defense the heads up that something nefarious is happening.</p>

<p>Defenders should also reduce their adversaries’ potential payoff in conjunction with raising the stakes. Having strict access control rules and a more segmented network means that a compromise of an individual machine doesn’t have much value, and attackers will have to expend more resources to get a bigger payoff. For example, deploying Duo Sec’s 2FA to end users reduces the value of their credentials by adding an extra hoop through which attackers must jump to illicitly access accounts.</p>

<p>But to counter their own weaknesses, defenders should take a data-driven approach — although data can have its flaws, it helps provide rational evidence of what the reference point should be. Having an ongoing picture of the “true” threat model may also encourage defenders to update their reference point more quickly, though it will still require some introspection to be aware of their bias towards being slow to change their views. One tech solution with this approach is Signal Sciences, which uses a data-driven approach to web app security by providing a continuously updated reference point of security posture in that area.</p>

<p>There also needs to be a better understanding in defense of how they define a loss. As I theorized earlier, right now it’s mostly “being breached,” and that may indeed have an immediate impact on the security practitioner&rsquo;s job security. However, it’s probable that a nation state attacker will breach a company, exfiltrate some data for espionage purposes, and there will be no real effects felt by the company (particularly short-term). Enhancing the equation of probability * outcome with an improved understanding of the real impact different types of attackers has on the organization would meaningfully improve prioritization of what solutions to adopt from lens of what is bad from the organizational point of view vs. what is bad from an “objective” security point of view.</p>

<h2 id="a-name-conclusion-a-conclusion"><a name="conclusion"></a>Conclusion</h2>

<p><img src="/blog/img/bad-cyberart-04.gif" alt="A very stupid looking gif of &quot;hacking&quot;" /><em>Real hackers use this leet virtual reality module for their attacks</em></p>

<p>I really believe understanding the motivations behind this “irrational” defensive behavior is empowering. While even I tend to veer towards hyperbole when describing offense’s advantages, the offense isn’t invisible nor is their decision-making flawless, and I think Prospect Theory helps identify those vulnerabilities — so now the challenge is for defense to start exploiting them.</p>

<p>My hope is that rather than telling infosec defenders that they’re being stupid or irrational or that they’re totally crappy at their jobs, the industry can take a more empathetic approach and suggest strategies towards ameliorating counterproductive incentives. I don’t think that will eradicate FUD marketing tactics or snake oil products, but it probably can give solutions that actually help a fighting chance to make a difference.</p>

<p>In conclusion, let’s try to be more of a community and practice some collective mindfulness, and just maybe we can start fixing things.</p>
]]></content>
        </item>
        
        <item>
            <title>WTFunding: Bioinformatics &amp; Genetic Data</title>
            <link>//swagitda.com/blog/posts/wtfunding-bioinformatics-genetic-data/</link>
            <pubDate>Tue, 17 May 2016 19:35:10 -0400</pubDate>
            
            <guid>//swagitda.com/blog/posts/wtfunding-bioinformatics-genetic-data/</guid>
            <description>WTFunding is one of my “spare time” projects to delve into tech sectors attracting VC funding that pique my curiosity. I like connecting dots between disparate things, it’s also pretty useful.
Table of Contents:  So what is bioinformatics? What are the applications? What&amp;rsquo;s hindering adoption? Who cares? What are the risks? What&amp;rsquo;s the current scene? Conclusion  So what is bioinformatics? Simply put, bioinformatics is software used to understand biological data, including (but not limited to) genomic sequences.</description>
            <content type="html"><![CDATA[

<p><img src="/blog/img/bioinformatics-02.jpg" alt="stylized pic of a double helix" /></p>

<p><em>WTFunding is one of my “spare time” projects to delve into tech sectors attracting VC funding that pique my curiosity. I like connecting dots between disparate things, it’s also pretty useful.</em></p>

<h3 id="table-of-contents">Table of Contents:</h3>

<ol>
<li><a href="#so-what-is">So what is bioinformatics?</a></li>
<li><a href="#applications">What are the applications?</a></li>
<li><a href="#adoption">What&rsquo;s hindering adoption?</a></li>
<li><a href="#who-cares">Who cares?</a></li>
<li><a href="#risks">What are the risks?</a></li>
<li><a href="#current-scene">What&rsquo;s the current scene?</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ol>

<h2 id="a-name-so-what-is-a-so-what-is-bioinformatics"><a name="so-what-is"></a>So what is bioinformatics?</h2>

<p>Simply put, bioinformatics is software used to understand biological data, including (but not limited to) genomic sequences. What’s particularly cool about the field is that it brings in a bunch of expertise and methods from other areas, like statistics and computer science in order to perform analysis and glean insights from this bio data.</p>

<p>If you only care about the business stuff and don’t really care about how it works (and / or hate science), then skip ahead to the <a href="#applications">“What are the Applications?”</a> section.</p>

<h3 id="how-are-living-things-made">How are living things made?</h3>

<p><img src="/blog/img/bioinformatics-01.gif" alt="Mr. DNA from Jurassic Park gif" /><em>The <a href="https://www.youtube.com/watch?v=qUaFYzFFbBU">reference</a>, for those who have had the misfortune of never watching “Jurassic Park.”</em></p>

<p>Let’s start with what constitutes life. The simplest unit of a living thing is a cell; some living things just have one cell, whereas others, like humans, have something like 37.2 trillion cells. Each cell has a nucleus, which is home to most of the genetic material within cells. Within the nucleus lies chromosomes, which are made of protein and DNA.</p>

<p>At a lower level you can think of these cells as being made of molecules, which are atoms grouped together by chemical bonds. Cells depend on three macromolecules, or molecules with upwards of 1,000 atoms, to determine the cell’s’ basic functioning and structure: DNA, RNA and proteins. The relationship between them is that DNA makes RNA, and RNA makes proteins (this is the central dogma of molecular biology). For the computer nerds reading, think of DNA like persistent storage, RNA like volatile memory and proteins like executables.</p>

<p>Zooming further in, genes are regions of DNA that is the recipe book, so to speak, for biological function. They’re what gives a living thing its biological traits — for example, I have the “blue eye” gene. For a sense of scale, there are 23 pairs of chromosomes within the human body that include over 20,000 individual genes that are made up of over 3 billion DNA base pairs.</p>

<p>So what are base pairs? The building blocks of each strand within the famous double helix of DNA are called nucleotides. Each of these nucleotide building blocks is made of either guanine, cytosine, adenine or thymine — which are shortened to C, G, A or T. But, these building blocks pair off to form base pairs, and only like the same partners: G always goes with C and A always goes with T (G-C and A-T). So, if you know the blocks in one strand of the double helix, you also know the blocks in the other.</p>

<p>As I said before, DNA makes RNA and RNA makes proteins, but how? DNA contains exons, which get converted to messenger RNA (mRNA), which delivers the message of which proteins are needed in a cell. The official way to describe exons is as “coding” for proteins, since like a computer program, they end up telling the cell what proteins to have.</p>

<p>Introns, in contrast, don’t code for proteins. Their relevance to this discussion is that they happen to also show where individual genes are separated. That isn’t to say that they are useless otherwise — they’re quite important for regulating how proteins show up in cells, but that’s not important for what I’ll be covering.</p>

<p>This is a good illustration of how all these things link:
<img src="/blog/img/bioinformatics-03.png" alt="Reference pic about chromosomes, DNA, nucleosome, etc." /></p>

<p>When you put all the genes within a living thing together, you get its genome. You might be familiar with the Human Genome Project, which is an international research body launched in 1990 dedicated towards mapping out the “human genome.” I have “human genome” in quotes because what they mapped out is an amalgamation of genomes — each human being has a unique genome with special variants in genes. They accomplished their goal in 2003, making it a warm and fuzzy example of the world banding together towards the common good.</p>

<p>The project also spurred a lot of innovation in the field. The way they were going about mapping the human genome was through DNA sequencing, the method for which at the time hadn’t evolved since the 1970s, other than getting a boost as computers became more powerful. But, concurrent with the start of the Human Genome Project, new sequencing methods, novelly called “next generation sequencing” methods, or NGS for short, began being developed and had gained a market foothold by the time the project finished. So, what exactly does DNA sequencing entail?</p>

<h3 id="dna-sequencing-process">DNA Sequencing Process</h3>

<p><img src="/blog/img/bioinformatics-04.png" alt="Pic of DNA encoding" /></p>

<p>Before you can figure out the purpose of specific genes, the DNA in question must first be sequenced. For simplicity’s sake I’m sticking with DNA sequencing in this section vs. other types of sequencing, because an already lengthy post would have to become a novel.</p>

<p>DNA sequencing just means determining the exact order of the building blocks (nucleotides) within the DNA, and more specifically the order of its bases — the Gs, Cs, As and Ts. I’ll walk you through a next-generation sequencing (NGS) process, since that’s been the catalyst for rapidly decreasing costs of sequencing, although there’s also the Sanger method, which is the “old school” method.</p>

<h4 id="library-preparation">Library Preparation</h4>

<h5 id="1-dna-samples">1. DNA Samples</h5>

<p>First you have to start with some sort of sample from which to extract DNA. It can be from blood, fossils, saliva or tissues, although saliva is now the go-to for collecting samples from humans given its ease of procurement.</p>

<p><img src="/blog/img/bioinformatics-05.jpg" alt="DNA purification" /></p>

<h5 id="2-starting-material">2. Starting Material</h5>

<p>But, there’s also a lot of other stuff in saliva (or any of the samples) that needs to be weeded out. Isolating DNA is done by “purifying” it from a sample through lysis, which disrupts the cell in order to release and separate its biological contents. There are physical and chemical methods for performing cell lysis, the coolest of which is probably sonication — harnessing ultrasonic sound energy to fragment the cell.</p>

<p>Once the cell is broken open, the non-DNA contents are removed via chemical methods, like adding detergents to remove the cell’s surface materials and enzymes that break down proteins and RNA. But those chemicals also have to be separated, so some extra chemistry magic happens (or a wash / spin cycle in a centrifuge) to get the DNA to bind together. Now the purified DNA can be extracted as starting material. As with many things in life, the more starting material, the better.</p>

<h5 id="3-fragmentation">3. Fragmentation</h5>

<p>Now, if we’re talking about human DNA, the genome is going to be really long and would thus take a really long time to sequence. So, the next step is cutting the DNA into smaller pieces for sequencing to help speed up the process. These fragments are officially termed “reads,” which I’ll use going forward. And fittingly, the collection of DNA fragments you generate is called a “library.”</p>

<p>But how do you determine how big you want these fragments to be? It largely depends on how the fragments will be sequenced and for what they’re being sequenced. Most NGS methods sequence up to 400 base pairs (“bp”) during one sequence cycle (called a “run”); the old-school Sanger method typically sequences up to 900bp.</p>

<p>The two typical approaches for this are either physical or enzymatic. One example physical method is acoustic shearing, which may be even cooler than sonication; the DNA sample is placed into a glass vial which is then subjected to acoustic energy that continually creates and collapses microbubbles. The process of growing these bubbles and causing them to implode creates shockwaves that have sufficient power to break down the sample DNA into random fragments. The power of the microbubbles fragments DNA pretty quickly with little loss of the DNA sample, and creates fragments ranging from 100bp to 5,000bp.</p>

<p>The other common physical method is nebulization, involving nebulizer devices which use compressed air to convert liquids into a fine mist. DNA is pushed through a small hole in the nebulizer, creating a mist which is then collected. The resulting fragments are typically 500bp to 3000bp, depending on how quickly the DNA is forced through the hole. This method is pretty quick, but can cause DNA to be lost in the process.</p>

<p>Enzymes capable of degrading DNA can also be used to fragment it into smaller pieces. Although it’s more consistent than physical methods, it also has the opportunity to alter the fragment by insertion or deletion. So, your method of choice really depends on your end goal.</p>

<h5 id="4-repairs-adapters">4. Repairs &amp; Adapters</h5>

<p>First, let’s take a look at how a DNA molecule looks, broken apart by its ribbons (or strands):</p>

<p><img src="/blog/img/dna-molecule.png" alt="Diagram of a DNA molecule broken apart by its ribbons / strands, by Kelly Shortridge" /></p>

<p>Each arrow represents one strand of DNA; I’ve laid this out linearly, but as you very likely know (and if the abundance of pictures in this post hasn’t reinforced it), the strands twist to form a double helix. The 3’ end of one of the DNA strands aligns with the 5’ end of its partner strand.The way the DNA molecule is “read” is from 5’ to 5’. So, if the 5’ ends of both strands stick out farther than each other, the strands will be “repaired” by combining them together and filling in the gaps (A with T and C with G).</p>

<p>This is crucial for the next step, which is putting “adapters” onto the DNA strands. Adapters are short DNA molecules that have been synthesized specifically to help in the sequencing process. Adapters are generally provided in a kit, and the adapter sequences therein will be added to the 5’ and 3’ ends of every fragment within one library.</p>

<p>Some may help put “primers” into their correct place, while others may help the DNA fragment stick to a surface in what’s called “immobilization.” The primers have to match the beginning and the end of the DNA fragment, since they’ll serve as the guide for what DNA fragment is amplified in the next step. The final step in the library preparation, immobilization, just means that each single DNA molecule is made up by a bead, which is then anchored to some solid surface, like a glass plate.</p>

<h4 id="amplification">Amplification</h4>

<p>Polymerase chain reactions (PCR) is used effectively as a copy machine for a DNA sequence, or what’s known as “amplification.” This thinking is actually somewhat similar to the methods for improving image quality within satellite imagery that <a href="/blog/posts/wtfunding-space-data-satellite-imagery/">I discussed in my prior post</a>. If you think of the target DNA as an object to be captured within a landscape, it’s far easier to take a bunch of quick pictures of it than take one big, detailed picture of the landscape and try to extract the object from it.</p>

<p>This process follows an exponential curve, too, since you’re making copies of the copies. After just 6 cycles, you have 64 copies of the target gene. After ten, you have 1,024. These copies then make up a “DNA colony” to be sequenced.</p>

<h4 id="sequence-reaction">Sequence Reaction</h4>

<p><img src="/blog/img/bioinformatics-06.gif" alt="DNA sequencing gif" /><em>Note: this is a sequencing UI from a TV show and thus should be disregarded entirely as resembling reality</em></p>

<p>NGS is also known as high-throughput sequencing, which just means that there’s tons more data that comes out of the sequencing process, and it’s due to parallelization. There are a few different sub-methods of NGS, the most common of which seems to be Illumina’s sequencing by synthesis (SBS) — though the others also depend on the sort of library preparation and amplification discussed above.</p>

<p>In SBS, the immobilized DNA is “washed” with one of the four nucleotide bases (either T, A, C or G) at a time to see which gets incorporated. For example, if the fragment’s template is GCGAATCG, and your wash is “A”, both of the A’s in the middle will incorporate the wash and the others will be washed away. Then, using the power of lasers and fluorescence, a machine can record which part of the fragment incorporated the base, showing _ _ _ A A _ _ _.Then, the “A” dye would be chemically removed so the next cycle can start, a “G” wash would be used, the laser magic happens, and the machine would record G _ G A A _ _ G, and so forth one nucleotide at a time until the sequence is complete.</p>

<p>For frame of reference, these machines are really, really expensive. At the low end, Illumina’s Miniseq “benchtop” sequencer is about $50,000 and produces an output of up to 7.5GB in just 24 hours. But at the high end, its heavy-duty sequencer, the HiSeqX, is sold for $10 million and only in units of 10, producing an output of up to 1.8TB in less than three days. Even despite their hefty price-tag, it’s still far cheaper and faster than Sanger sequencing (the “old school” method).</p>

<h4 id="sequence-assembly">Sequence Assembly</h4>

<p>The process above generates sequencing “reads,” but not the genome itself — which is where sequence assembly comes in, and is also where data science starts to get involved. De novo sequence assembly, which seems to be the most commonly used, reconstructs an organism’s likely genome sequence based on the reads. The word “likely” here is important, since there’s no guarantee that it’s correct, but instead is approximating the source of these fragments.</p>

<p>As you might guess, this leaves some room for error, which I’ll touch on in the “what’s hindering adoption” section. Errors can be in the form of false negatives, such as thinking a fragment is a mistake or just a repeat, or the fragments could just be linked up together in the wrong spots…plus there’s no guarantee that there weren’t errors in the sequencing process itself. To better illustrate why this process is so tricky, Wikipedia has a great analogy:</p>

<blockquote>
<p>The problem of sequence assembly can be compared to taking many copies of a book, passing each of them through a shredder with a different cutter, and piecing the text of the book back together just by looking at the shredded pieces. Besides the obvious difficulty of this task, there are some extra practical issues: the original may have many repeated paragraphs, and some shreds may be modified during shredding to have typos. Excerpts from another book may also be added in, and some shreds may be completely unrecognizable.</p>
</blockquote>

<p>One example algorithm used in de novo assembly is overlap-layout-consensus (OLC), which leverages a graph to represent the reads. First, overlaps between fragments are computed, and then each fragment is connected by an edge if there are indeed overlaps. The computing problem then becomes how to navigate through the graph in a way that still contains each fragment, which involves a lot of graph theory.</p>

<p>The other primary assembly algorithm is the de Bruijn graph (DBG), which also is based on graph theory. First, the reads are broken into smaller pieces of the same size (called k-mers). Then, the graph is created by ordering from the pieces that overlap at the beginning of the sequence to those that overlap at the end. This sounds linear, but isn’t.</p>

<p>The image below should help visualize each algorithm and compare how they differ:
<img src="/blog/img/overlap-consensus-de-bruijn-graph.png" alt="Visualizing the de Bruijn graph and overlap-layout-consensus algorithms, by Kelly Shortridge" /></p>

<p>To move on, let’s assume for now you’ve done a pretty good job sequencing and assembling the DNA in question. It’s not just there to be admired, but to gain something useful out of it, which leads into sequence analysis.</p>

<p><img src="/blog/img/velociraptor-dna.jpg" alt="Raptor with DNA overlayed" /><em>An example of a DNA sequence and foreshadowing of my “what are the risks” section</em></p>

<h3 id="sequence-analysis">Sequence Analysis</h3>

<p>Sequence analysis is where the startup software solutions addressing bioinformatics begin to appear. This is the process of taking the sequenced DNA and applying meaning to it — determining and reporting what the genes are and what is their purpose.</p>

<h4 id="homology-search">Homology Search</h4>

<p>“Homology searching” is a fancy way of saying looking for similarity between sequences, and is step one (at least to try) towards getting to the end goal of understanding the genes you’ve just sequenced. “Homology” itself means shared ancestry between genes in different species — after all, all species evolved from the same ancestors. So, when sequences between two distinct species are similar, you can argue that there’s homology.</p>

<p>If you are able to search a database of known sequences to see where they might be similar to your sequence, then you can much more easily identify certain genes and their functions, as well as speculate on evolutionary relationships. In an ideal scenario, if the results of your homology searches cover all the genes you need identified, and they already describe what the genes are for, then you’ve arrived at your end-result. You’ve also saved yourself a lot of other potential steps, which are described right after this.</p>

<p>So how do researchers compare sequences? It involves a lot of data science and also necessitates having a rigorous dataset with which to compare. The more genes are identified on other species, such as mice, the greater ability there is to search human DNA sequences for similar genes.</p>

<p>The most commonly used tool for homology search is BLAST, for “Basic Local Alignment Search Tool,” which is favored due to its speed, at the expense of some accuracy. For those familiar with statistics, it’s a heuristic algorithm. The way it compares a query sequence to a sequence database is by finding “high-scoring pairs,” or HSPs for short, which represent overlaps that seem statistically significant. The speed comes from creating “words” within a sequence, which are generally 11 letters long in DNA sequences, and running the search on each word, narrowing down which HSPs fit as the search moves through the letters. This constitutes a substitution matrix, and would look something like this in BLAST:</p>

<p><img src="/blog/img/substitution-matrix-dna.png" alt="An illustration of a substitution matrix for DNA sequence analysis, by Kelly Shortridge" /></p>

<p>While BLAST is the current market leader, improving upon it is clearly an area of interest within academia. Like in many data science projects, making an algorithm both faster and more accurate is the holy grail. For projects with lots of sequence data, BLAST becomes computationally expensive — and given the explosion of sequence data in recent years, researchers have developed and proposed alternatives that potentially offer greater efficiency and ameliorate computational cost concerns. I could publish an academic journal on all the approaches proposed, but I’ll explain one I find particularly neat and badassly-named, GHOSTX.</p>

<p>GHOSTX’s speed advantage is by analyzing both length and match score — the algorithm will only ingest sequences that are both close to the appropriate length of the target sequence and deemed to be statistically similar content-wise. To oversimplify a bit, it’s able to do this by using suffix arrays, an example of which is below:</p>

<p><img src="/blog/img/suffix-array-dna.png" alt="An illustration of GHOSTX and suffix arrays for DNA sequence analysis, by Kelly Shortridge" /></p>

<p>This is much more efficient for searching, since now you just have an array with pointers to text snippets (suffixes) that are sorted in alphabetical order. So, what’s being stored isn’t “A$,” “ATA$” and so forth, but “7” or “5”, that is, what’s stored is only the code for where in the list the suffix lies. In other words, you now have an index for all the different suffixes, making it easy to find where your sequence matches others in the database.</p>

<p>Using these various methods, even if you find a matching result for a gene, there still may be insufficient data on what the gene does (i.e. lacking gene annotation, which I’ll describe in a bit). In those cases, and others in which homology search doesn’t complete the puzzle, you’ll have to pursue the following steps in addition.</p>

<h4 id="gene-identification">Gene Identification</h4>

<p>If the homology search fails, or for whatever reason you just don’t want to perform a homology search, gene identification is the next step. What you’ll be looking for is the region of DNA that specifically codes for its function, i.e. codes for protein. These are called open reading frames (ORF), which you can think of as triplets of base nucleotides (again: A, T, C or G), and researchers use them to initially identify regions that probably show what the gene is.</p>

<p>Fancy data science can be involved in this step, too, as gene prediction (which is just the predictive form of gene identification). There are three primary methods of doing so, which include statistical, comparative or empirical methods (i.e. homology, which I went through above).</p>

<p>As an example of a statistical method, gene prediction programs using neural networks can be trained on the organism whose DNA is in question and then applied to the target sequence. Hidden Markov Models (HMMs) also use training data to return the most likely structure of a gene, which is useful if there’s no gene in a sequence or only a partial gene.</p>

<p>Comparative methods use DNA from another species to help predict gene function, since there’s a lot of overlap in how our genes work. The common example is using mouse DNA to determine genes in human DNA; after all, we’re a lot more like a mouse than we are a plant. In fact, 95% of genes that actually code for a biological function match one for one between mice and humans. The theory behind this is that evolutionary pressures led to a general, optimized blueprint for how mammals should function.</p>

<p>However, both these methods still require experimentation to verify that the gene prediction is accurate…at least for now, until these methods (particularly statistical) become more accurate.</p>

<h4 id="translation-database-search">Translation &amp; Database Search</h4>

<p>Once you have identified your gene, whether through homology search or gene identification, you need to translate that gene into its proteins — the stuff that brings to life whatever the genes have blueprinted. These translators are pretty easily found on various academic websites. For example, the sequence I’ve been using in my examples throughout, “CATCATATCAGTCATAGT,” by pure luck happens to have an ORF (the region that actually codes for proteins). It’s in “ATGACTGATATGATG,” which is in the “opposite” DNA strand of my sequence (specifically, the opposite of “CATCATATCAGTCATAGT”).</p>

<p>Once you have your ORFs, you can search them against protein databases to see if there’s already data on them. If there is, it’ll help enormously in the next and crucial step, gene annotation, since you’ll have a head start on knowing what purpose the gene serves.</p>

<h4 id="gene-annotation">Gene Annotation</h4>

<p>Gene annotation is the process in which a specific gene is assigned a specific functional or physical feature. This ideally is largely automated and just enhanced with human expertise, because it would take a very long time for one person to manually annotate a DNA sequence.The functional features generally are drawn from a specific vocabulary to form an ontology, or a formal method of naming and defining things.</p>

<p>There are two components of annotation: the gene’s structure and its function. Structure mostly just determines which regions code for proteins, while functional annotation records the gene’s biological and biochemical functions. Its function just means what proteins are there, or what proteins are hypothetically there (such as predicted by gene finding programs), which engenders how the gene influences an organism’s operations. The gene is also given a name, whether unknown or known based on being identical to a previously documented gene.</p>

<p>Automated annotation uses data science in order to help predict the relevant gene annotations, like its functions. Bayesian networks and similar statistical methods are used by learning from prior annotations, but doesn’t help with finding new gene functions. Automated annotation certainly helps speed up the process, but it’s basically impossible to be accurate on the whole genome, so manual annotation is still needed to clean up any errors or fill in the gaps.</p>

<p>Annotation is one of those things that sounds really easy to do (“just write down what the gene does!”), but is actually really tricky in process. The quality of annotation is largely dependent on the quality of the process before it. If the gene prediction was faulty, or if the sequencing shoddy, it will result in greater difficulty and / or error in annotation.</p>

<h4 id="upload-stop-collaborate-and-listen">Upload &amp; Stop, Collaborate and Listen</h4>

<p><img src="/blog/img/cold-storage-dino-dna.jpg" alt="Cold storage of dino DNA" /><em>This is not the sort of DNA data storage I mean.</em></p>

<p>Once the DNA has been sequenced and annotated, it has to be stored…which sounds simple enough, but isn’t quite. In a brief crossover to my industry, security and compliance is actually pretty important when considering biological storage; after all, someone’s genome is extremely private information (though admittedly a mouse probably doesn’t care if their genomic data gets hacked).</p>

<p>Adding to the complexity of storage, there are various types of databases for genomic data — some that contain empirical genomic data, predicted genomic data or structural data. It might also have just the raw sequencing data, or data that’s been cleaned up a bit. Some are publicly available, but some aren’t. The databases also have to be able to facilitate search across sequences, too.</p>

<p>While databases may be a bit passé in other areas of tech, this is actually a reasonably hot area within bioinformatics. Making it easy for customers to search and combine different types of genomic data, such as via an API, is hugely useful in bioinformatics applications, but even helping them manage their data is a plus as well, such as through versioning control (since this sort of data is always being updated). Some startups are also focused on unifying this data with other sorts of medical data, such as patient outcomes, which helps streamline their customer’s processes considerably by not requiring them to visit lots of different databases and tools.</p>

<p>Collaboration is another key part of the bioinformatics end-stage. For example, when designing a pharmaceutical drug, the R&amp;D team needs to collaborate with the clinical trials team, who needs to collaborate with the clinical team in order to maintain a complete feedback loop. So, bioinformatics software is also about optimizing workflows to produce more robust results — which may sound ridiculously outmoded to techies at the cutting edge, but, well, there it is.</p>

<p><img src="/blog/img/ian-malcolm-there-it-is.gif" alt="Ian Malcolm gif saying &quot;Well, there it is&quot;" /><em>The O.G. thought leader.</em></p>

<p>Now with these query-able databases of annotated genes, i.e. you have your sequenced DNA handy and know more or less what the random letters mean in a biological sense, you can now get to work on using them towards a variety of use cases.</p>

<hr />

<h2 id="a-name-applications-a-what-are-the-applications"><a name="applications"></a>What are the applications?</h2>

<p>While genes really shouldn’t be thought of based on what diseases they cause, the most obvious applications of bioinformatics revolve around discerning better treatments of diseases. The ability to map out genes and relevant mutations of complex and pervasive diseases such as cancer, diabetes or those that cause infertility could help research in which drugs or other forms of therapy might alleviate — or even eliminate — them.</p>

<p>With that lens, there are three major areas that can benefit the most from bioinformatics: pharmaceuticals, personalized medicine and genetic testing.</p>

<h3 id="pharmaceuticals">Pharmaceuticals</h3>

<p><img src="/blog/img/bioinformatics-07.JPG" alt="Pharmaceutical person doing something" /></p>

<p>Pharmaceutical companies can leverage bioinformatics for a variety of applications, from target discovery and drug design to improving the efficacy of existing drugs.</p>

<p>Pharmacogenomics studies how people’s responses to drugs are affected by their genes towards a more tailored approach to pharmaceuticals. You also might hear the term “pharmacogenetics,” which is typically used interchangeably with pharmacogenomics but does have a slight difference. Pharmacogenomics involves looking for differences between people at the genetic level that can explain drug response, while pharmacogenetics involves looking for a genetic reason for why there is a specific drug response. So think of pharmacogenomics as a whole-genome approach while pharmacogenetics deals with just one interaction between a drug and genes.</p>

<p>Right now most medications take a “one size fits all” approach, when in reality a person’s genetic makeup may determine how beneficial a drug is or what side effects they experience. In the future, this means doctors could analyze your genome against a variety of drugs for a specific condition to optimize your therapy — part of personalized medicine, which I’ll discuss in greater detail below.</p>

<p>This type of “predictive” approach would be a big benefit to patients, as they would theoretically no longer have to suffer through a trial and error period that can sometimes come with harmful reactions. Everything from the type of drugs to the dose amount could be tailored based on your predicted reaction to the medication. And, when the data on your response based on this predicted approach is recorded, it would help improve prescription algorithms, resulting in a huge data-network-effect.</p>

<p>For example, at least one in ten Americans takes antidepressants. Which antidepressant and dosage works for some people vs. others currently is more art than science. Further, these medications can often come with adverse side effects that reduce the patient’s quality of life — from insomnia to emotional numbness — or even make their depression worse. If instead their genome could be used to accurately predict which drug and how much of it would improve their depression the most, or at least cause them to suffer the least, their outcomes would be far more quickly and painlessly reached.</p>

<p>So how would pharmaceutical companies reach this stage? Genetic data could be used to isolate a particular protein related to a disease and conduct research towards finding a drug effective against this protein. Or, they could use genetic data to better understand receptors (such as a protein) so they know which would be the best target for their drug molecule. There’s also plenty of room for improvement with existing drugs; oftentimes drugs are effective against conditions for reasons that aren’t well understood, and bioinformatics could help determine why that is.</p>

<p>Ultimately, the real dream for the future of pharmaceuticals is a combination of bioinformatics-informed drug development with a feedback loop of patient outcomes to continually inform and improve drug design. This would likely serve as the cornerstone of personalized medicine, which leads us to…</p>

<h3 id="precision-personalized-medicine">Precision / Personalized Medicine</h3>

<p><img src="/blog/img/bioinformatics-08.jpg" alt="Doctor with an adorable puppy" /><em>In 2030, scientists will discover the key to all health issues is giving someone a puppy.</em></p>

<p>“Personalized” or “precision” medicine is all about tailoring treatment and prevention based on an individual’s unique characteristics, including their genes. It’s unrealistic (or so my 2016 brain believes) to assume that in 2100 AD there will be a unique drug for each person and their conditions, but it is realistic to assume, even in the nearish future, that populations can be segmented in a way to substantially improve treatment. This isn’t just about drugs, either — it extends to medical devices along with prevention of diseases, too.</p>

<p>Part of this nearish future is improving the clinical trials process, the experiments performed with human participants to test the efficacy and safety of treatment methods. As discussed in the pharmaceutical example above, having a record of genetic compositions of potential trial participants can help improve the decision-making behind who should be included in the trial. With the clinical trials market estimated to reach $72 billion by 2020, you can see why having the ability to more carefully select trial participants in order to optimize clinical trial outcomes is worthwhile.</p>

<p>In a somewhat extreme case, Estonia is using a government-driven approach for furthering along these initiatives — it’s collecting DNA of all its citizens to create a genetic biobank. While right now the use case is primarily genomic research, farther down the line, having a strong sense of the average genetic “posture,” so to speak, of the Estonian population could help inform public health policy and population-wide personalized medicine.</p>

<p>Here in the United States, Obama created the Precision Medicine Initiative (PMI) in January 2015 within the Health and Human Services (HHS) department, primarily led by the National Institutes of Health (NIH). Its goal is simply to do away with the aforementioned “one size fits all” approach to improve medical outcomes for patients. It’s still in its early innings and has a somewhat slow timeline; in February of this year, NIH announced that it will study the genomes of one million volunteers by 2019.</p>

<p>The hope is the PMI will help kickstart precision medicine both by collecting a large swath of data as well as through developing improved analysis capabilities. It’s likely that everyday citizens won’t see the fruits of personalized medicine until quite a bit after 2019, as personalized treatments will require research, development and testing time, even once the genetic data collection and analysis methods improve. The presumable safety regulations around personalized medicine also may cause snags and delays, which I’ll discuss a bit more in the next section.</p>

<h3 id="genetic-testing">Genetic Testing</h3>

<p><img src="/blog/img/bioinformatics-09.jpg" alt="Newborn baby's foot" /></p>

<p>Genetic testing arguably must serve as the basis for any personalized medicine initiative, as only by knowing an individual’s genes can any treatment plans be tailored. A genetic test is simply a test that identifies either the person’s genes or changes in their genes (including proteins). There are currently a few uses for genetic tests, or at least uses cases leveraged in marketing.</p>

<p>Parents or couples considering becoming parents can use genetic testing to evaluate what sort of genes they will pass along to their offspring, generally with the goal of identifying genetic disorders. For couples pursuing in vitro fertilization (IVF), genetic testing can also find defects within the embryos created via IVF before they are implanted into the carrier, which increases the the embryo’s viability.</p>

<p>Individuals can test their own susceptibility to disorders and see what sort of mutated genes they have, or use it as a diagnostic test if they have a currently unidentified ailment. Or, more humorously, as I heard from a friend, being able to tell a sibling that you have less Neanderthal DNA than they have. This, in theory, helps individuals take preventative actions to stave off diseases or illnesses later in life.</p>

<p>While most genetic tests are currently based on DNA, longer-term there will likely be tests involving RNA and proteins. As discussed in the first section, DNA is the blueprint; this means it’s potentially a decent predictor of genetic risks, but not great at measuring the body’s current state for diagnostic purposes.</p>

<h3 id="genome-editing">Genome Editing</h3>

<p>While this isn’t a major application (yet), it’s pretty cool and a company doing it (Intellia) just IPO’d, so I want to touch on it briefly. Turns out you can also leverage DNA sequences and knowledge of genes’ functions in order to modify an organism’s DNA sequence, such as a virus.</p>

<p>The key tech enabler here has been CRISPR, or “clustered regularly interspaced short palindromic repeats,” which actually is part of a bacterial immune system. To use a reverse analogy from my industry, it’s a bit like most anti-virus software, where “signatures” of viruses are kept around so the system knows how to defend against it in the future.</p>

<p>The next part is using CRISPR-associated proteins, or “Cas,” that will actually chop virus DNA in half like a pair of scissors upon confrontation, the result being that the virus can no longer replicate.</p>

<p><img src="/blog/img/crispr.gif" alt="Gif of how Crispr works" /></p>

<p>This is pretty cool stuff, and while most of the research leveraging this technology is towards curing diseases, this could make the “designer baby” fears a reality, or even facilitate genetically engineered bioweapons (more on the latter in the “risks” section).</p>

<hr />

<h2 id="a-name-adoption-a-what-s-hindering-adoption"><a name="adoption"></a>What&rsquo;s hindering adoption?</h2>

<p><img src="/blog/img/bioinformatics-12.jpg" alt="A faceless person in a latex suit with DNA printed on them" /><em>In the future, apparently we’ll be featureless and wear latex suits with our genomes printed on them.</em></p>

<p>Now that sequencing costs have decreased, what are the other barriers to progress? A common theme that’s emerging from these WTFunding pieces it that being able to manage and analyze a lot of complex data turns out to be really hard…and bioinformatics is not different.</p>

<p>While it’s now far cheaper to generate genomic data, there’s still no guarantee that it’s accurate. Since gene annotation is still performed by humans, at least to some degree, there’s still plenty of room for human error — not to mention oftentimes the annotations are simply incomplete. You can play a “fun” game if you want and keep backing up the chain I walked you through earlier to spot the numerous chances for error to be incorporated, sequence assembly and sequencing itself being the most likely candidates.</p>

<p>You might have also caught while reading the “how it works” part that the algorithms for various processes require a lot of computational resources. Obviously the past decade has seen enormous strides in the availability of computing power, but it’s still reasonably time and cost prohibitive given the sheer size of genomic data. Someone else has already done the math on the size of the human genome in digital terms, and it’s <a href="https://medium.com/precision-medicine/how-big-is-the-human-genome-e90caa3409b0#.fstnfweh1">roughly 200GB of raw sequencing data</a> because, as explained before, you need a bunch of “reads” of the sequence.</p>

<p>On the data management side, a big question is how do you make genomic data easily accessible and combinable with other types of health data (such as patient outcomes), since that’s a key to unlocking a larger addressable market. There are startups addressing precisely this issue, in figuring out how to sensible combine different data types and link them up in an intuitive manner, but it’s safe to say that data harmony hasn’t yet been reached industry-wide.</p>

<p>The analysis portion specifically presents two big challenges. First, in a fragmented market for tools and algorithms and other analysis methods, how do you figure out what to use? Second, how do you know you’re not just seeing patterns in data that aren’t actually there? Many analysis tools are written with a single application in mind, and from what I gather, they aren’t very well-written, either. So organizations that could potentially benefit from bioinformatics may not be willing to dip their toe in the water until a discernable market with user-friendly software emerges.</p>

<p>And on the genetic testing side, the main questions are: Who pays for the tests? Who advises individuals on their test results? How does the data easily get to health care providers? While I’ll get into the incentive problems within genetic testing later, part of the problem currently is that genetic tests are still somewhat expensive for individuals. Will the government? With health insurance providers? It presents a bit of a chicken and egg problem, given patients need to be genetically tested in order to gain benefits from tailored pharmaceuticals or personalized medicine, but neither of those benefits are yet in a mature enough stage to incentivize people taking the tests.</p>

<hr />

<h2 id="a-name-who-cares-a-who-cares"><a name="who-cares"></a>Who cares?</h2>

<p><img src="/blog/img/bioinformatics-11.jpg" alt="Image of pills over twenty dollar bills" /><em>Pillz on ya billz</em></p>

<p>VCs love numbers, so I’m going to present the big, high-level ones first:</p>

<ul>
<li>Global pharmaceutical industry = ~$1 trillion annually</li>
<li>U.S. pharma industry = ~$400 billion annually</li>
<li>Global healthcare industry = ~$3 trillion annually</li>
</ul>

<p>So combined, looking at a $4 trillion global market, which should assuage any VC’s fears of insufficient market size since grabbing 1% of the market gets you $40 billion in revenue. For reference, Pfizer has just about 5% of pharma market share and has a market capitalization of just over $200 billion.</p>

<p>But, the clever VC says, the initial markets will just be those with the highest pain points, not the entire industry! Those top therapy classes would be oncology (cancer), diabetes and mental health. So here are more numbers:</p>

<ul>
<li>Cancer drug spending in the U.S. = ~$42 billion annually (in 2014)</li>
<li>Direct medical costs of diagnosed diabetes in the U.S. = ~$176 billion annually (in 2013)</li>
<li>Direct cost of mental illness in the U.S. = ~$147 billion annually (in 2009)</li>
</ul>

<p>VCs should still be quite happy, since combined (and given inflation), this is likely nearing a $400 billion market, and just in the U.S. alone. With the 1% market share goal, you’re still looking at $4 billion in revenue today.</p>

<p>Obviously, it isn’t just VCs who care about the big, shiny numbers. The aforementioned pharma companies likely get cartoon-dollar-sign-eyes contemplating the ability to charge more for genomic-data-driven drugs tackling these illnesses. Medical providers could also pad their pockets by leveraging personalized medicine to differentiate from the competition and thus create the ability to markup prices.</p>

<p>But on a less cynical note, there’s the real chance that genome-based personalized treatments won’t be solely a marketing shtick and actually result in significantly better outcomes for patients. So anyone suffering from an illness could potentially see benefits from personalized medicine. Further, there are many illnesses where the current understanding of what causes them can be summed up by ¯\_(ツ)_/¯, and bioinformatics could potentially lead to breakthroughs that would give hope of proper treatment and prevention to those afflicted.</p>

<hr />

<h2 id="a-name-risks-a-what-are-the-risks"><a name="risks"></a>What are the risks?</h2>

<p><img src="/blog/img/ian-malcolm-warning.gif" alt="Ian Malcolm saying &quot;yeah, yeah, but your scientists were so preoccupied with whether or not they could that they didn't stop to think if they should&quot;" /><em>And we all know how well Jurassic Park turned out</em></p>

<p>There’s a pretty clear incentive problem on the pharmaceutical side. Pharma companies’ goal is to have more people buy their drugs. One way to incentivize more people to buy your drugs is by recommending certain drugs based on a person’s genetic profile. In a world in which genetic testing was performed by an altruistic, neutral third party, people might only be prescribed the drugs they need. In a world in which pharma companies have massive budgets, particularly for marketing, it isn’t difficult to imagine that they might “sponsor” genetic testing recommendations to ensure that their drugs are recommended.</p>

<p>As with any industry with a growing reliance on data-driven and increasingly automated approaches, there’s the potential for error as well. Relying too much on data models could mean that there is less caution than today towards prescribing treatments, which could result in disastrous “black swan” type events. Though, as I mentioned, it’s far more probable that healthcare providers will instead commit the error of seeing patterns that don’t actually exist, the implications of which could range from unnecessary and costly treatments to wrong and costly treatments that set back the patient’s progress, or even harm them.</p>

<p>Taking a look at another risk angle, the recent <a href="http://www.wsj.com/articles/theranos-is-subject-of-criminal-probe-by-u-s-1461019055">Theranos debacle</a> is now a cautionary tale for medical startups. Many of the startups in bioinformatics are just in the software part of the space, so have less potential of scientific blundering like Theranos, since they’re instead helping with the data management, analysis and workflow portion.</p>

<p>But startups in the genetic testing area should definitely be heeding the lamentable tale of the “blood unicorn.” As I probably have made abundantly clear by this point, there’s room for error at nearly every step of the process, so there’s a non-trivial chance that end users might get inaccurate results that could have pernicious impacts on their health.</p>

<p>Let’s also not forget the cautionary tale of Jurassic Park, in which dinosaurs (which, spoiler alert, are extinct) are brought back to life through the power of DNA for the purposes of creating a theme park. Then everything goes to shit and people die. I’m pretty skeptical that anyone will be so silly as to bring back dinosaurs, but the general lesson of considering long-term implications is an important one. While the current research focus may be on disease prevention, there could be a future in which you can “enhance” yourself (or your offspring, like the “designer babies” I mentioned earlier) through genetic modification. It sounds really cool, but it could also be really, really disastrous and have far-reaching consequences not just health-wise, but societally (for anyone who has played Bioshock, think of ADAM).</p>

<p>On a geopolitical note, improvements in understanding the human genome also open up nefarious applications. At the “mildly evil” end of the evil scale, there’s genetic modification to win sporting events like the Olympics; at the “really evil” end of the evil scale, there’s creating highly targeted biological weapons to conduct warfare against a specific subpopulation, i.e. biological genocide. In fact, this year the U.S. Intelligence Community added gene editing to its <a href="https://www.dni.gov/files/documents/SASC_Unclassified_2016_ATA_SFR_FINAL.pdf">“Worldwide Threat Assessment”</a> report.</p>

<p><img src="/blog/img/bioinformatics-10.jpg" alt="Snake with needles for fangs, it's pretty weird" /><em>Implanting needles in snakes’ mouths as fangs is clearly the most efficient bioweapon distribution mechanism…</em></p>

<p>Heretofore, the scale needed for biological weapons has limited it due to the rigorous research and development process that synthetic pathogens require. But thanks to improvements in sequencing technology and analysis, it’s far easier to sequence viruses, which means it’s potentially easier to create new viruses, too!</p>

<p>While it’s probably an exaggeration to say that most people could now make a bioweapon in their backyard, as the availability of genomic databases and computing power increases, and the cost of equipment continues to decrease, it will certainly be accessible to well-funded terrorist organizations (if not already).</p>

<p>I definitely don’t think it’s too tin-foil-y to suggest that most of the current world powers have synthetic bioweapons at their disposal already. However, it may be a bit more tin-foily to say that the population’s increasing reliance on antibiotics really doesn’t help proactively defend against some sort of biological attack.</p>

<p>Hopefully these bioweapons, like nuclear weapons, aren’t ever deployed in-the-wild and are instead more about deterrence. In any case, this sort of subject could make for thrilling TV.</p>

<hr />

<h2 id="a-name-current-scene-a-what-s-the-current-scene"><a name="current-scene"></a>What&rsquo;s the current scene?</h2>

<p>There are a number of startups scattered about the various areas within the bioinformatics chain, most of which seem to have early leaders emerging within them. As far as incumbents, there’s Illumina and Qiagen in the NGS part, CLC bio (owned by Qiagen) in bioinformatics analysis, Bina Technologies (owned by Roche) and Genomic Health in data management and most recently, Intellia in gene editing. I’ve segmented out the startups based on the different sub-areas below:</p>

<h3 id="data-management">Data Management</h3>

<ul>
<li>DNAnexus</li>
<li>Genome Compiler</li>
<li>Genospace</li>
<li>MediSapiens</li>
<li>SolveBio</li>
<li>Benchling</li>
<li>Omicia</li>
</ul>

<h3 id="genomic-analysis">Genomic Analysis</h3>

<ul>
<li>Biomatters</li>
<li>BioNano Genomics</li>
<li>CancerIQ</li>
<li>Maverix Biomics</li>
<li>One Codex</li>
<li>Onramp BioInformatics</li>
<li>Spiral Genetics</li>
<li>Syapse</li>
<li>Tute Genomics</li>
</ul>

<h3 id="genome-editing-1">Genome Editing</h3>

<ul>
<li>Caribou</li>
<li>CRISPR Therapeutics</li>
<li>Desktop Genetics</li>
<li>Homology Medicines</li>
</ul>

<h3 id="genetic-testing-1">Genetic Testing</h3>

<ul>
<li>23andMe</li>
<li>Cofactor Genomics</li>
<li>Color Genomics</li>
<li>Counsyl</li>
<li>NextGxDX</li>
<li>Recombine</li>
</ul>

<p>There isn’t one major investor that’s all over this space, though in the lead is definitely Google Ventures. Below is the list of the major institutional and corporate VCs that have played in this space in some form:</p>

<ul>
<li>Andreessen Horowitz</li>
<li>Data Collective</li>
<li>Felicis Ventures</li>
<li>First Round</li>
<li>Formation 8</li>
<li>Google Ventures</li>
<li>Johnson &amp; Johnson</li>
<li>Khosla Ventures</li>
<li>Mohr Davidow Ventures</li>
<li>New Enterprise Associates</li>
<li>Novartis Venture Fund</li>
<li>SV Angel</li>
<li>WuXi Healthcare Ventures</li>
<li>Y Combinator</li>
</ul>

<hr />

<h2 id="a-name-conclusion-a-conclusion"><a name="conclusion"></a>Conclusion</h2>

<p><img src="/blog/img/life-finds-a-way.gif" alt="Ian Malcolm saying &quot;Life finds a way&quot;" /><em>Bioinformatics startups will find a way</em></p>

<p>The implications of a more mature bioinformatics industry are pretty radical — from curing cancer to having vastly improved patient outcomes resulting from personalized medicine. I think it’s evident from the simplicity of some current solutions, such as basic data management and collaboration tools, that there’s a long way to go until this area becomes sufficiently sophisticated and automated.</p>

<p>While the error problem throughout the bioinformatics chain will certainly hinder adoption of end-applications, I think the truly massive dollar size amounts behind those applications will spur along innovation sooner rather than later. Perhaps even some of the washed out founders and engineers who are victims of the current market correction will apply their computer and data science skills towards this area — it seems a more easily accessible industry than others, with collaborative incumbents.</p>

<p>I think the biggest long-term opportunities are in what is some of the traditionally “boring” application infrastructure software — data management, collaboration, workflow management, etc. — both in pharmaceutical research and for healthcare providers. If I were a healthcare data platform like Flatiron Health or Health Catalyst, I might be looking to scoop up some of these companies early with the long-term vision of an integrated genomic and clinical data platform.</p>

<p>In any case, the companies that create this sort of middleware can use those data-driven underpinnings to very easily reach up into the sexier analytics and visualization plays. I honestly wouldn’t be shocked if some of these data management software startups become the SAP or Oracle of Bioinformatics.</p>

<p>In contrast, I think genetic testing will eventually become a commodity, which in my opinion doesn’t justify the level of risk that startups face with it today. Admittedly, it’s a crucial part of unlocking the potential of personalized medicine, but my gut feeling is that there will just end up being a Quest Diagnostics and Laboratory Corp. of genetic testing one day. For comparison, Quest has a market cap of ~$10 billion and Laboratory has one of ~$13 billion, while SAP has one of ~$95 billion and Oracle one of ~$163 billion (not to mention all of their competitors). So I think on the whole, the software bet is the smarter one given the risk / reward trade off.</p>

<p>Finally, I think we’re not at all yet prepared for the ethical challenges that will arise from bioinformatics applications…but that’s another long-read post for a different time.</p>
]]></content>
        </item>
        
        <item>
            <title>Apple vs. FBI: Privacy &amp; Inequality</title>
            <link>//swagitda.com/blog/posts/apple-vs-fbi-privacy-inequality/</link>
            <pubDate>Thu, 17 Mar 2016 19:29:13 -0400</pubDate>
            
            <guid>//swagitda.com/blog/posts/apple-vs-fbi-privacy-inequality/</guid>
            <description>In recent news, there’s been a fiery public relations battle between the FBI and Apple. While there has been vocal protest by the technology community, and particularly the information security community, against the FBI’s request, it has largely been rooted in (very valid) concerns over the degradation of software security, national security and privacy in a general sense. However, there’s a larger societal concern that cannot be ignored.
By way of brief background, the gist of the legal fight is that the FBI wants Apple to create a custom version of its operating system for iPhones, iOS, that would allow it to circumvent the safety measure of reducing the frequency with which the device’s passcode can be guessed when locked.</description>
            <content type="html"><![CDATA[<p><img src="/blog/img/apple-privacy-scales.png" alt="Scales of justice, with an iPhone on one side" /></p>

<p>In recent news, there’s been a fiery public relations battle between the FBI and Apple. While there has been vocal protest by the technology community, and particularly the information security community, against the FBI’s request, it has largely been rooted in (very valid) concerns over the degradation of software security, national security and privacy in a general sense. However, there’s a larger societal concern that cannot be ignored.</p>

<p>By way of brief background, the gist of the legal fight is that the FBI wants Apple to create a custom version of its operating system for iPhones, iOS, that would allow it to circumvent the safety measure of reducing the frequency with which the device’s passcode can be guessed when locked. The FBI wants this in order to access the contents of the San Bernardino shooter’s personal device, although this software, if created, would in fact allow them to circumvent the safety measure of any iPhone. I recommend viewing <a href="https://www.youtube.com/watch?v=zsjZ2r9Ygzw">John Oliver’s segment</a> on the debate for an accessible, but more comprehensive overview.</p>

<p>The issue itself, I fear, is representative of the early stages of a dichotomy between the “haves” and “have nots,” but for privacy. This is more important than I believe many realize as far as deepening inequality and injustice. This isn’t just about protecting photos of your children, or keeping secret the absurd questions you type into Google.</p>

<p>This is about the fact that devices are now an integral part of our digital identities; that, as dictated by the Constitution, all citizens have a right to privacy; and that, while not necessarily the norm, there are very real abuses of power within the law enforcement and legal system and bias against underprivileged groups.</p>

<p>Society and technology are now in an entrenched dependency; tech inequality will progressively manifest as socioeconomic inequality. The stratification of tech knowledge between higher and lower socioeconomic groups is leading to the latter’s vicissitudes in education and employment opportunities, but I also argue that the Apple vs. FBI case is the start of the age in which it manifests in legal rights as well.</p>

<p>Even if Apple wins, they will continue their invigorated efforts towards ensuring that any requests for customer data by law enforcement simply cannot be fulfilled. This will be a fantastic result — for those who can afford an iPhone. The reality is that owning an Android is a less expensive option, but does not come with these protections and is unlikely to in the near future.</p>

<p>Apple can enforce this level of security because it manufactures both the device and develops its operating system. Google could do the same with its own line of Nexus devices, but otherwise there would need to be a collaboration across all the various manufacturers of Android devices in order for it to happen. Even if such a collaboration does happen, it will take time and likely still command a price premium in the market for these efforts.</p>

<p>While there are a number of security measures individual Android users can take to reproduce most of Apple’s protections, they require a higher degree of tech ability than most consumers, not to mention lower socioeconomic groups, possess. So even though it is technologically feasible, the nature of socio-tech inequality as it relates to education still creates a substantial hurdle towards an accessible method of ensuring privacy.</p>

<p>Less affluent consumers are thus left without these protections, the protections that protect our private data on our mobile devices from law enforcement and enforce the Fourth Amendment. These less affluent consumers, particularly those belonging to racial minorities, are also much more likely to be accused of criminal activity.</p>

<p>There are two key disadvantages in this arena that underprivileged consumers subject to discrimination face, which I’ll illustrate by way of analogy (which may seem superficial at first blush). In Jay-Z’s song <a href="https://www.youtube.com/watch?v=32Xh9L-AqA8">“99 Problems,”</a> he raps that, when asked by a police officer to search his car, he retorts, <a href="http://genius.com/17560/Jay-z-99-problems/Well-my-glove-compartment-is-locked-so-is-the-trunk-in-the-back-and-i-know-my-rights-so-you-gon-need-a-warrant-for-that">“Well, my glove compartment is locked, so is the trunk in the back / And I know my rights, so you gon’ need a warrant for that.”</a></p>

<p>First, underprivileged people may not have the ability to “lock” the digital versions of their glove compartments and trunks, because they cannot afford the luxury of privacy via strong security. Second, Jay-Z is aware of his rights and the need for the officer to have a warrant, and such knowledge is invaluable in maintaining his privacy — more importantly, his innocence — in the face of accusatory and biased law enforcement. However, the knowledge and understanding of digital privacy rights, and how to protect them, is barely existent among even affluent, highly-educated groups, so consumers with greater resource constraints are even less likely to have satisfactory awareness.</p>

<p>With many highly public instances of abuse of power by law enforcement, it would be naive to assume, should the FBI win this case, that the power that having full access to the contents of a mobile device brings would not have the same potential to be similarly abused. When you consider that divulging passwords is protected by the Fifth Amendment as “knowledge,” having an easy technology option — simply automatically updating the device with law enforcement’s proprietary version of the operating system, thereby removing passcode protections — is understandably seductive.</p>

<p><a href="https://en.wikipedia.org/wiki/Parallel_construction">Parallel construction</a> is a very real concern among groups subject to routine discrimination, and it isn’t difficult to imagine law enforcement illegally unlocking phones, searching for the evidence they want and retroactively claiming that this data was the cause for the suspicion. This includes liberal interpretations of incriminating evidence — jokes could potentially be purposefully misconstrued as evidence, with the defendants facing racial or socioeconomic bias that would make it difficult for them to contest.</p>

<p>The combination of pervasive smartphone cameras and the viral nature of social media has helped foment the recent outrage against transgressions by law enforcement. Knowing that data recorded on smartphones now has the potential to highlight unprofessional and illegal conduct, it also not difficult to imagine that law enforcement would desire the ability to easily circumvent security protections to prevent such ignominy.</p>

<p>Even with these examples, it’s essential that we not forget that privacy is granted to us as a fundamental right, and that even the potential for its violation is worthy of outrage, regardless of cause. Rights are meant to be for all citizens, but in a FBI-wins world, only those who can afford the efforts of Apple’s R&amp;D department will be able to buy their rights back in a depressingly simoniacal manner.</p>

<p>Though it may appear hyperbolic, this case may be one that has a resounding impact in shaping our society going forward. Even though many will still be unable to afford the luxury of privacy, at least in the short-term, even if Apple wins, their prospects are far more dire if Apple loses. Should the FBI win, we are unequivocally cementing this inequality and further disenfranchising those who cannot afford the luxury of privacy.</p>

<p>I like to think that the United States, at its best, can serve as a role model for innovation to the rest of the world. Is the sort of society in which only those who are well-off can realize their right to privacy the one we want to build?</p>
]]></content>
        </item>
        
        <item>
            <title>WTFunding: Space Data (Satellite Imagery)</title>
            <link>//swagitda.com/blog/posts/wtfunding-space-data-satellite-imagery/</link>
            <pubDate>Mon, 04 Jan 2016 18:49:24 -0400</pubDate>
            
            <guid>//swagitda.com/blog/posts/wtfunding-space-data-satellite-imagery/</guid>
            <description>WTFunding is one of my “spare time” projects to delve into tech sectors attracting VC funding that pique my curiosity. I like connecting dots between disparate things, it’s also pretty useful.
Table of Contents:  So what is &amp;ldquo;space data&amp;rdquo; and &amp;ldquo;satellite imagery&amp;rdquo;? What are the applications? What&amp;rsquo;s hindering adoption? Who cares? What are the risks? What&amp;rsquo;s the current scene? Conclusion  So what is &amp;ldquo;space data&amp;rdquo; and &amp;ldquo;satellite imagery&amp;rdquo;?</description>
            <content type="html"><![CDATA[

<p><img src="/blog/img/satellite-01.jpg" alt="Satellite observing Earth" /></p>

<p><em>WTFunding is one of my “spare time” projects to delve into tech sectors attracting VC funding that pique my curiosity. I like connecting dots between disparate things, it’s also pretty useful.</em></p>

<h3 id="table-of-contents">Table of Contents:</h3>

<ol>
<li><a href="#so-what-is">So what is &ldquo;space data&rdquo; and &ldquo;satellite imagery&rdquo;?</a></li>
<li><a href="#applications">What are the applications?</a></li>
<li><a href="#adoption">What&rsquo;s hindering adoption?</a></li>
<li><a href="#who-cares">Who cares?</a></li>
<li><a href="#risks">What are the risks?</a></li>
<li><a href="#current-scene">What&rsquo;s the current scene?</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ol>

<hr />

<h2 id="a-name-so-what-is-a-so-what-is-space-data-and-satellite-imagery"><a name="so-what-is"></a>So what is &ldquo;space data&rdquo; and &ldquo;satellite imagery&rdquo;?</h2>

<p>I derive a lot of pleasure by calling it “space data,” but it is more accurately termed satellite or geospatial imagery, and the analysis thereof. This is all about image data collected by satellites about the Earth’s surface and the software that helps make these images useful to humans.</p>

<p>To get to the final, useful image, there are three main steps: launching a satellite into orbit, collecting the images of the Earth’s surface, then processing and analyzing the images. While image processing &amp; analysis, with the goal of gleaning intelligence from satellite imagery, is drawing the most VC funding at the moment, it’s important to understand each step to get both the opportunities and risks startups face within this sub-sector.</p>

<p>If you are more of a <a href="https://en.wikipedia.org/wiki/Wikipedia:Too_long;_didn%27t_read">tl;dr</a> person, you might want to skip down the page a bit to the next section, <a href="#applications">“What are the Applications?”</a> and continue from there.</p>

<h3 id="step-1-launch-to-orbit">Step 1: Launch to Orbit</h3>

<p><img src="/blog/img/satellite-02.jpg" alt="Landsat 8, an Earth Observation satellite operated by NASA" /><em>Landsat 8, an Earth Observation satellite operated by NASA.</em></p>

<p>I’ll be focusing on remote sensing satellites, but there are also communications and weather satellites and reconnaissance (aka spy) satellites and maybe even satellites that can shoot lasers out of the sky (<a href="https://commons.wikimedia.org/wiki/File:Nothing_is_beyond_our_reach.svg">nothing is beyond their reach</a>, after all). First, let’s cover some basics about satellites.</p>

<p>Rockets are the things who take satellites on their journey into Earth’s orbit. Earth’s gravity is what keeps satellites in orbit, just like how gravity causes the Moon to orbit us. Satellites are fitted with gyroscopes, spinning discs that leverage the Earth’s magnetic field to keep the satellites on course. Solar panels power the satellites once they’re in their proper orbit, since they have nice access to the Sun’s rays.</p>

<p>There are three different types of orbit: geostationary, geosynchronous and polar (or sun synchronous). Geostationary means that the satellite is stationary relative to the Earth’s surface. Communication and weather satellites use this orbit, staying way above the Earth at a specific point on the Equator (way above = 30k+ km). Polar (aka sun synchronous) means the satellite orbits at an altitude at which it will consistently pass over a given location at the same local time. This is what remote sensing satellites use, and are typically closer to the Earth’s surface than the communications satellites (only hundreds of km above).</p>

<h3 id="step-2-data-collection">Step 2: Data Collection</h3>

<p>Objects on Earth reflect energy from the Sun. This energy is in different “bands,” typically visible, infrared and water vapor. Each object reflects a different amount of energy, giving them a “spectral signature.” “Remote sensing” is how satellites collects these reflections — and just means sensing the energy bands from a remote place (like the Earth’s orbit).</p>

<p><img src="/blog/img/energy-reflection.png" alt="How remote sensing works, diagram by Kelly Shortridge" /><em>How remote sensing works (no animals were harmed in the making of this image).</em></p>

<p>Satellites carry sensors that allow them to do this remote sensing. Passive sensors just collect the reflections (radiation) that are emitted from Earth, using the Sun’s energy as its source of electromagnetic radiation. Active sensing systems carry their own source of electromagnetic radiation, which is directed to the Earth’s surface. Just like the passive sensing system, it then measures the amount of energy that is reflected back.</p>

<p>Different sensors are designed to measure different parts of the electromagnetic spectrum. For example, the 450nm - 495nm wavelength band would measure the visible color blue. The 570nm - 590nm wavelength band would measure the visible color yellow. And near-infrared is the ~700nm - 1mm wavelength band.</p>

<p><img src="/blog/img/EM-spectrum.png" alt="The Electromagnetic Spectrum" /><em>The Electromagnetic Spectrum</em></p>

<p>Luckily, the electromagnetic spectrum is pretty handy for identifying objects. The way objects reflect back energy is consistent and tends to be unique. Water doesn’t reflect much visible or infrared energy, but vegetation strongly reflects infrared.</p>

<p>Satellites can even be used for things like detecting gravitational pull on the Earth’s surface. If you have two satellites, one leading and one following, when the lead passes over an area with high gravitational pull, it’ll start speeding up (and slow down over areas with weaker gravitational pull). NASA launched twin satellites in 2002 to do just that (GRACE mission). Being able to detect gravity helps map out areas below the Earth’s surface, from tunnels to potential oil wells.</p>

<p>Capturing lots of energy/wavelength bands creates what are called “multispectral images.” This means that the final image contains a few layers of images that each capture a different energy band. For example, one layer might capture the infrared band while another might capture the green band. You can even have superspectral and hyperspectral, but that just means there are lots and lots of layers with different energy bands. The more bands, the more opportunity for differentiation of objects on the ground. On the opposite side of the spectrum (pun intended), panchromatic images capture a single band of energy in black and white (and shades of grey).</p>

<p>As the satellite orbits around the Earth, it looks at a small chunk of the planet at a time. Even if it goes around the Earth every few hours, it is only capturing as little as less than 1% of the Earth each time. This small chunk is represented by a pixel, just like on your TV or computer monitor. For example, one pixel could represent 50x50m on Earth. Different types of sensors allow for different pixel sizes, typically from 5m to 1km — though some have pixel sizes as small as the micrometer scale (and tend to capture a smaller portion of the Earth as a result).</p>

<p>As the satellite orbits, it has to capture as many pixels as it can. However, even if pixel size is the same among different sensors, this doesn’t mean the resolution is as well. Higher resolution is indicated with a smaller physical measurement; for example, an image with a pixel size of 5m will look much clearer with a resolution of 5m vs. 20m.</p>

<p>According to one news report, in 2013 a spy satellite was launched in California capable of “snapping pictures detailed enough to distinguish the make and model of an automobile hundreds of miles below.” Given the length of a car is approximately 5m, let’s assume that the resolution needs to be 10% of that in order to see the necessary level of detail about the car, so 0.5m or smaller. And we’d likely assume that it isn’t just panchromatic since the government likely also wants to know the color of the car. This makes it quite a bit more powerful than most commercially available multispectral resolutions. As a frame of reference, DigitalGlobe’s satellite scheduled to be launched in September 2016, the WorldView-4, will have a multispectral resolution of 1.2m.</p>

<p>When reading about a satellite’s imagery capabilities, it’s crucial to understand that pixels and resolution are not the same thing. The difference between a pixel and resolution is that resolution is the size of the smallest detectable feature captured whereas a pixel is the size of the smallest physical area captured (or in other words, the smallest unit of the image).</p>

<p>There are also different types of resolution. Spatial resolution depends on what’s called the sensor’s “Instantaneous Field of View” (IFOV), which just means the visibility the sensor has at a particular altitude at a particular moment in time. Spectral resolution just means which parts of the electromagnetic spectrum the sensor can capture; higher spectral resolution indicates a narrower band of wavelengths captured. Radiometric resolution measures how many shades of grey there are between black and white, measured in bits; an 8-bit radiometric resolution means the sensor can measure 256 unique shades of gray. And temporal resolution is the length of time it takes for the satellite to complete one orbit; for example, some satellites may capture the same area every 5 days, while for others it will be every 15 days.</p>

<p>Another important thing to keep in mind is that satellites are not necessarily taking photographs; these sensors are not cameras (though those spy satellites sometimes do use long-focus lenses). The mechanism is through sensing energy bands. Hence the industry lingo is remote sensing. Each image represents a lot of pixels arranged in rows and columns — think of it like putting a fragment of an image into 1,000 rows by 1,000 columns in Excel, and then zooming way out to see the full picture.</p>

<p><img src="/blog/img/pixel.png" alt="One pixel example" /></p>

<p>To continue that analogy, that Excel file would then represent what’s called a “scene.” Scenes can represent over 2,000km on each side. These scenes are what most people buy since they want the “big picture,” not a tiny snippet of whatever is being observed.</p>

<p><img src="/blog/img/pixel-to-scene.png" alt="Showing how one pixel becomes a scene" /></p>

<p>Again, this scene represents a physical object on the Earth’s surface. In our cat’s case, let’s say it’s just over a foot tall sitting upright, which is about 0.33m. In the scene, we can see the entire physical object (the cat) and features as small as its nose, so using its nose as a proxy for the smallest detectable feature, our resolution is approximately 0.01m — meaning we’re likely using a super secret spy satellite to spot Mr. Whiskers.</p>

<p><img src="/blog/img/pixel-and-resolution.png" alt="Showing how pixel and resolution relate" /></p>

<p>Multispectral scenes can represent a large amount of data, often with tens of millions of bytes (10MB+) per scene. This is because the intensity of each pixels is stored as a single byte (an 8-bit digital number), and there are typically millions of pixels in any given scene. Recorded video can hog even more data, and high quality video is a more newly available product — and typically no longer than two minutes. How video can be used to track objects on the ground is a longer discussion that mostly talks about error correction.</p>

<h3 id="step-3-image-processing-analysis">Step 3: Image Processing &amp; Analysis</h3>

<p>This is just where the fun begins! Now the imagery needs to be processed, in what’s novelly called “image processing.” This just means a human uses a computer to work with the images. Having higher spatial resolution and more data doesn’t mean you’ll necessarily get more information out of the images — and that’s why image processing is so important.</p>

<p>Why is there a human required, you might ask? While it’d be fantastic to have automated image analysis software that can intelligently scan images and highlight relevant objects or issues, that’s not the current state of things at all. Humans still sadly need to be involved to help with filtering and classification based on their project goals.</p>

<p><img src="/blog/img/satellite-03.png" alt="Satellite imagery of wildfires in Idaho" /><em>Satellite imagery of wildfires in Idaho</em></p>

<p>Processing is part of image analysis, which requires special types of statistics and analytical methods that are tailored for spatial data — though primarily anchored around interpolation (predicting new, unknown data points within a group of some known data points). There’s an emphasis in geostatistics in being able to estimate how much the gaps that are filled in via prediction might be wrong; things like elevation for 3D modeling are particularly tricky to determine aerially, so there is ripe opportunity for error. One example method is kriging, which helps predict the appropriate values in unobserved locations by using a weighted average of surrounding areas and estimating its accuracy.</p>

<p>For satellite imagery, there are software tools that have been developed with the specific use case of geospatial imagery processing and analysis in mind. These tools are called geographical information systems (GIS for short). The usual aim of image processing is to create an image that makes sense to a human — in essence, make the energy band sensing look more like a photograph. Or more simply put, “what the heck is in this spot?”</p>

<p>At a higher level, the end goal from all of this is information extraction. Even with basic mapping (like Google Maps), the point of the image is to gain some insight about stuff here on the ground. The type of information that is desired can be different between applications (which I’ll discuss later), and very rarely is available with just the basic image taken. Thus, humans need tools to help extract information from the images.</p>

<p>The typical chain of image processing is data import, image restoration &amp; rectification, image enhancement and information extraction. The methods for performing this processing chain have been shifting somewhat in recent years. Now, satellite data can be retrieved via APIs and similarly are various tools for processing made available via APIs. But, to understand some of the challenges, it’s important to walk through these steps.</p>

<p><img src="/blog/img/image-processing-chain.png" alt="Showing the steps in satellite image processing" /></p>

<h4 id="data-import">Data Import</h4>

<p>While spy satellites might take actual film, eject it and have it intercepted by military aircraft, the process to transmit imagery back to Earth isn’t quite as cool for commercial satellites. As the satellites orbit around the Earth, they send data down (“downlinked”) via directional antennae to receiving stations on Earth. They can also receive instructions on what to capture, which are sent up from Earth (“uplinked”). These communications are conducted over the X-band, a specific frequency range.</p>

<p>The images the satellites take are generally compressed on-board, with their total storage nearing a terabyte. Some can even perform image fusion (which is discussed shortly) before transmission to help improve the image’s resolution.</p>

<p>There are various GIS data formats and different types of data that can be imported. Much like <a href="/blog/posts/wtfunding-industrial-manufacturing-analytics/">in my prior post</a>, a number of different vendors have their own data formats, and they are often proprietary, making a lot of big data analysis stuff a lot harder. There are also unique data formats for specific parts of imagery &ndash; such as the feature geometry, feature attributes, topology, etc.</p>

<p>Satellite imagery is commonly stored in digital numbers (DN), which means each pixel gets a value in 8-16 bits for a physical value (like color or temperature). This helps minimizing the storage volume.</p>

<p>There are a number of geospatial-specific database management systems (DBMS), which is software used to store GIS data to allow it to be later retrieved and modified &ndash; essentially the tool that helps organize data. This is actually an important part of image processing, since it allows for querying and comparisons across different pieces of data.</p>

<p>Most of the time, this data is stored in distributed systems, which just means it isn’t all in one database. Distributed databases optimize scalability, which is why they are used in this use case, due to geospatial data’s typically large file sizes.</p>

<p>The relational database model is the one that is primarily used for GIS data. I’ll skip an explanation and discussion of the various database systems and types for now, but if you aren’t familiar with them, <a href="https://en.wikipedia.org/wiki/Database">here’s a link to the Wiki</a>.</p>

<p>Analysis techniques are used to create and define models of relationships between different data sets. The traditionally used technique is the entity-relationship model. In this, there are specific entities, like buildings, forests, and agricultural land, which have different attributes and “members” of the entities (like government, residential or religious buildings). The relationship part of entity-relationship just means members of these entities are compared; for example, you could compare different residential buildings in New York City to determine what attributes they have in common (perhaps they have rooftop gardens, are thinner, etc.).</p>

<p><img style="float: right;" src="/blog/img/relational-model.png" alt="relational database model"/></p>

<p>The relational model works well for this sort of data, since it allows for different datasets to be compared. The key (pun intended) here is that there are keys for each entry in the data sets that link the data sets between each other.</p>

<p>Using a relational model for databases allows the humans to make queries with the Structured Query Language (known best by its acronym, SQL). You can make a query such as: select all buildings with rooftops within 100km of Union Square in Manhattan. Each building will have a specific coordinate (i.e. spatial relation), and (which we’ll cover in a few paragraph), can be marked as having a rooftop. Geographic search is the most important query out of these, and a large part of why there are geospatial-specific DBMS.</p>

<p>There are two types of data stored in these systems: spatial or attribute data. Spatial meaning “georeference” (i.e. location on Earth), and attribute meaning feature-related data (normally stored in tables). From the example above, the building coordinates are within the spatial data, and whether they have rooftops is within attribute data.</p>

<p>Spatial data has two further subcategories: vector and raster. Vector data deals with geometry in a few different ways. Think 2D, like boundaries of forests, or 1D (lines), like following the path of a river. You can also have specific spatial points that represent a particular item, and they are technically 0D. Raster data represents surfaces, but not just as far as 3D things like elevation; it can also show things like population density or temperature.</p>

<p>Raster data is where our spectral data comes in, along with the actual imagery and topography. Each cell (remember, this means pixel) gets assigned a specific value based on its primary feature. For example, one cell may have vegetation assigned to it, while another may have water.</p>

<p>GIS software will link all this data and create these models with a specific structure. Typically, it will start with geometrical information, then add topological and finally thematic (the raster data). This order is logical since geometrical represents the physical form and position of objects, topological is about relational position (intersections of objects), and thematic adds in the detail about objects (typically in layers).</p>

<p>There’s some trickiness in importing and managing this data, however. The data needs to be both accurate, meaning the map needs to match real world values, and precise, meaning described as exactly as possible. The distinction is important; you can have a map perfectly overlaid on real coordinates (accurate), but showing “this area is green” rather than a breakdown by type of vegetation (imprecise). Similarly, you could have detail down to different types of weeds (precise), but your coordinates are a mile off (inaccurate).</p>

<p>I’ll get into more of the challenges later, but it’s important to touch on how these errors happen, most of which are during processing. Formatting data can cause scaling to change, the data can be outdated, or there could even be an errant sensor. There are issues even with positional accuracy as far as non-land things go — it’s a lot easier to accurately determine the boundaries of a lake than the boundaries of population density.</p>

<p>There will also be labeling errors, whether by humans or automated processing. If I saw an image showing there was a Magnolia tree forest in some region of China, I’m unlikely to know if that is the correct type of tree in the forest or not.</p>

<p>These errors can snowball and ultimately make the entire analysis worthless, which is why they are such a big deal. A mining company starting a drilling project for gold in a specific spot will be none too pleased when the imagery they used actually was kilometers off, or showing the wrong type of mineral.</p>

<p>Some can be mitigated by supervised classification, in which a human selects an area of land they do know a lot about so that the software can then classify other areas accordingly. But that’s an inefficient and time-consuming process that requires domain expertise — so, far from ideal.</p>

<p>There are new sorts of structures being developed to improve object retrieval from satellite imagery databases. Some involve automatically extracting objects from imagery, then encoding their descriptors into much smaller (&lt;1% of original imagery) sizes. This allows for very fast retrieval by object shapes, which seems like a nice improvement.</p>

<h4 id="image-restoration-rectification">Image Restoration &amp; Rectification</h4>

<p>Processing might start with getting true color on the images (i.e. making it look like a photograph, with blue water, green forests, etc.). You might be familiar with RGB values, (255, 255, 255 being white), which represent an 8-bit image color range. Sometimes it’s better to have false color, which just means using unnatural colors to help highlight differences between energy bands. If your goal is to see levels of vegetation, then you may prefer a false-color composite (FCC) that lets the infrared bands really pop.</p>

<p><img src="/blog/img/satellite-04.png" alt="False color satellite imagery" /><em>False color imagery</em></p>

<p>After that, you might want to make sure your image is accurate, or what is called “georectification.” You can use what are called ground control points (GCP), which just means using the coordinates of known locations on a map in order to make sure the image’s coordinates map the real physical location. There’s also “orthorectification,” which removes issues of scale by accounting for different tilts and terrains — the more diverse the Earth’s surface is, the more likely there will be distortions in the image.</p>

<p>There are a few different ways images can be distorted or have irregularities, and thus different techniques to help restore them. One such technique is resampling, in which a pixel gets assigned a DN based on the DN’s of its neighbor pixels. Another is radiometric pre-processing, in which corrections are made to handle noise or irregularities generated from the sensors so that only the actual reflected radiation shows on the image.</p>

<p>Since satellites can’t control the weather (and weather modification is banned), there’s often the need for atmospheric correction, such as cloud removal. Since remote sensing is based off of how the sun reflects off objects, there can also be variations in the angle of the sun that need to be taken into account.</p>

<h4 id="image-enhancement">Image Enhancement</h4>

<p>The image enhancement phase is to improve the quality of the imagery. Some enhancements are familiar due to their use in photography, such as contrast enhancement to help highlight the differences within an image. Spatial filtering involves directly manipulating pixels for some effect. If you’ve ever played around with filters in Photoshop, or even on your phone, you get the picture. These can include image sharpening or softening, embossing, etc.</p>

<p><img src="/blog/img/satellite-imagery-fusion.png" alt="Satellite image fusion diagram by Kelly Shortridge" /></p>

<p>A common technique for enhancement is image fusion, which seeks to create a single, more detailed image out of multiple images. It’s also one of the ways that the tradeoff between spatial and spectral resolution can be solved.</p>

<p>There are a few different levels at which image fusion can be performed. First is at the pixel level, comparing pixels in different images to figure out how to pack in more detail into a pixel. Second is at the feature level, comparing sizes, lengths, shapes, etc. of the same geographic area and using statistics to combine the highest-intensity features out of different images. Third is object-level, the highest-level type of image fusion, in which images are processed separately and then combined using fancy algorithms to help maximize intensity.</p>

<p>There are limitations of image fusion, such as color distortion and poor quality when dealing with high resolution images, but apparently these problems are expected to be alleviated as technology improves.</p>

<h4 id="information-extraction">Information Extraction</h4>

<p>Image classification is the largest part of information extraction, and means each pixel (or, more recently, object) in an image is categorized. This is important to distinguish different objects within an image and ultimately extract information from the image. For example, if you are measuring how quickly a city is expanding, you’ll want to be able to classify buildings, or even particular types of buildings.</p>

<p>Classification typically involves the computer (or software tool) automatically distinguishing different types of objects — like water, grass, urban areas, forests, etc. These tools aren’t perfect, not only as a function of pixel size but also that the bands of energy that objects emit may be too similar to properly distinguish them.</p>

<p>For pixels, there’s unsupervised and supervised classification — if you’re familiar with machine learning, you’ll already get what that means. The shortest difference is that unsupervised classification involves examining unknown pixels in an image, while supervised means examining known pixels.</p>

<p>Unsupervised classification will compare the unknown data with reference data as a way to figure out the category of the unknown sub-area. It’s a manual process, with the user having to choose how many clusters, or groups with similar properties, to be generated, and then match clusters with classes. It’s arguably more accurate than supervised classification, but it’s also more tedious due to its more manual nature.</p>

<p>Supervised classification will take the known data in an image, compare it with reference data and use it to extrapolate categories for the unknown parts of the image. The process is typically “training” the classification engine on sample imagery, selecting specific features, applying the right algorithm, then determining how well it worked or not.</p>

<p>Some issues with classification are similar to those in machine learning — you need reliable comparison data and strong sampling data in order for it to work, which is why unsupervised is often preferred.</p>

<p>There’s also object-oriented image classification, or “multi-resolution segmentation,” which is a non-traditional approach (meaning it’s only come into use in the past decade or so). As the name suggests, it creates objects by grouping pixels rather than classifying individual pixels. The resulting objects have different shapes and scales, and thus can be classified more flexibly using different image layers (e.g. population density, infrared, elevation, etc.). The user is still doing supervised classification using samples and fancy algorithms, but with more accuracy when dealing with objects vs. individual pixels.</p>

<p><img src="/blog/img/satellite-05.jpg" alt="Example of how object-based image analysis works." /><em>Example of how object-based image analysis works.</em></p>

<p>The general rule of thumb is that object-oriented classification is best for higher spatial resolution, since objects might consist of multiple pixels, and the other methods work fine for lower resolution (in which objects are just a pixel). Of course, as spatial resolution improves, this means that object-oriented classification might be increasingly adopted in kind.</p>

<p>The type of algorithm matters, too. For example, a highly tailored algorithm might eliminate any false classifications due to shadows by incorporating into its model the position of the sun and relevant ground elevations in the area based on the image’s location and time.</p>

<p>At the forefront of research are different automation techniques to help extract features. Methods leveraging machine vision are one example, as well as methodologies that allow for more variables for classification while maintaining a high level of accuracy (90%+). It’ll likely take a few years for commercially available products to catch up to the research (along with bugs that come out when scaling to product-level use), but highly accurate automation within 5 years doesn’t seem preposterous.</p>

<p>Once features are classified, information can be extracted for its desired purpose. Which leads to the various applications of geospatial analysis.</p>

<hr />

<h2 id="a-name-applications-a-what-are-the-applications"><a name="applications"></a>What are the applications?</h2>

<p>There are a bunch of industries that benefit from using satellite-based imagery — particularly for anything in which physical trends over time are needed or they want to see stuff below the Earth’s surface. The number of applications is expanding as imaging capability improves, since higher resolution images provide a more granular view of what’s happening on Earth.</p>

<p>Even though purchasing multispectral imagery can be high in absolute dollar terms, relative to the cost of physical exploration, it is inexpensive. But for non-profit or applications without this high cost of physical capital on the line, the reward isn’t necessarily as high.</p>

<p>Also, assume for any of the following applications, traders can use similar information to inform their financial bets. For example, if satellite imagery suggests that the rate of construction in China is slowing down, they might short construction materials firms or commodities as a result. Of course, this has some intriguing implications for the efficient-market hypothesis, if investors have information on a company’s operations that even the company itself might not possess.</p>

<p>The government has a variety of applications for geospatial imagery, and has been leveraging it as a source of intelligence for half a century. But, I’ll just be focusing on applications within commercial industries.</p>

<h3 id="current-applications">Current Applications</h3>

<h4 id="agriculture">Agriculture</h4>

<p>It can be hard to measure agricultural trends on the ground, so satellite imagery is immensely helpful in assessing crop health and yields, environmental changes and trends pertaining to livestock. Even when planning and maintaining agricultural sites, this imagery can map irrigation and analyze soil — even showing variations in soil’s organic matter.</p>

<p><img src="/blog/img/satellite-06.jpg" alt="Imagery highlighting irrigation" /><em>Imagery highlighting irrigation.</em></p>

<p>Aside from optimizing costs and boosting productivity at large agricultural companies, there’s a general global need for improved agricultural production and better utilization of resources. Having a better sense of what and where these resources are to improve their management has significant benefits on a macro scale.</p>

<h4 id="engineering-construction">Engineering &amp; Construction</h4>

<p>Along with companies in the mining and oil &amp; gas industries, engineering &amp; construction companies have high capital costs relating to physical projects. So, geospatial imagery can help these companies visualize their projects, not just for evaluating and planning construction sites, but also for maintaining them. This helps reduce construction costs and also minimize environmental impact.</p>

<p><img src="/blog/img/satellite-07.jpg" alt="Digital elevation model of a construction site" /><em>Digital elevation model of a construction site.</em></p>

<p>Being able to model construction sites in 3D is crucial for planning purposes, but also ensuring ongoing safety. And for certain project types, like airstrips, dams, power plants and sewers, you need data beyond just the visual. For example, when building an airport, not only do you need to make sure the terrain is appropriate for an airstrip, but also have 3D models for flight simulation to make sure pilots aren’t going to run into recurring issues.</p>

<h4 id="environmental-monitoring">Environmental Monitoring</h4>

<p>On the “save the world,” side of things, environmental monitoring helps assess damage from natural disasters as well as help manage natural resources. Governments can use satellite imagery to help develop disaster response plans, as well as improve environmental planning and conservation.</p>

<p><img src="/blog/img/satellite-08.jpg" alt="Imagery highlighting deforestation" /><em>Imagery highlighting deforestation.</em></p>

<p>Being able to see high-level trends, like deforestation, is helpful to monitor local environmental health but even more so to evaluate potential long-term impacts. After all, trees don’t grow back overnight, so excessive “forest farming” can have devastating effects on future generations’ economic wellbeing. Not to mention being a harbinger of global climate change.</p>

<h4 id="logistics-shipping-maritime">Logistics (Shipping &amp; Maritime)</h4>

<p>Logistics and shipping companies, port operators, fishers, trade organizations and governments all have an interest in geospatial imagery relating to maritime and weather patterns. On the pure logistics side, being able to track ships in transit is highly useful, as tracking systems can fail when far enough away from ports. Weather patterns and other spatial data (like terrain mapping) can also help optimize shipping routes.</p>

<p><img src="/blog/img/satellite-09.png" alt="Search for MH370 by satellites" /><em>Search for MH370; odd given the number of global recon satellites that it’s still missing.</em></p>

<p>Being able to monitor trading, spot illegal fishing or piracy, and help with search and rescue missions are of particular importance from a global trade perspective. Even the “little guy” can win — local fishers and fisheries are often put out of business by illegal fishing, which is more widespread than you might think.</p>

<h4 id="mining">Mining</h4>

<p>Multispectral satellite imagery has the ability to differentiate between different types of rocks, vegetation and soil, which helps mining and geology projects in a few different ways.</p>

<p><img src="/blog/img/satellite-10.jpg" alt="Imagery optimized to show rare earth elements" /><em>Imagery optimized to show rare earth elements.</em></p>

<p>First and most obviously, this imagery can help identify clays, oxides and soils for mineral mapping and exploration. This is in contrast to most humans, who would walk to the location and say, “yep, that looks like ground.” All the different energy bands will show both different types of rocks and elements as well as structural aspects of the Earth’s surface that may influence ease of mining.</p>

<p>Second, it helps plan out mining projects. Digging into the ground isn’t the only challenge; mining companies also have to worry about how to get access to the mine and what infrastructure would be required to support the project. And, they also need to estimate what sort of impact the project will have on the surrounding area from a human and environmental perspective.</p>

<h4 id="oil-gas">Oil &amp; Gas</h4>

<p>Satellite imagery can help oil and gas companies reduce risk in oil exploration as well as monitor ongoing projects. The level of detail is pretty impressive, from generally detecting areas that are most productive down to even detecting seismic lines or offshore oil seepage.</p>

<p><img src="/blog/img/satellite-11.jpg" alt="Deepwater Horizon oil spill by satellite" /><em>The Deepwater Horizon spill being just a bit more than seepage.</em></p>

<p>But not only does it help find areas most likely to be rich with oil, but it also helps these companies assess the potential costs and pitfalls associated with drilling in a particular area. For example, satellite imagery shows which areas have rock formations, heavy forest coverage, unfavorable weather conditions and whether they are in more remote or developed locations.</p>

<h3 id="future-applications">Future Applications</h3>

<p>In the next section I’ll talk about some of the challenges that have hindered adoption to date, but if geospatial imagery becomes more widely available and easier to leverage for business and operational intelligence, other industries may become customers in addition to those above.</p>

<p>One potential area is physical retail. A super cool application might be looking at the surrounding area and weather patterns of store locations to see what types of goods might resonate best with local customers. For example, imagery could show the levels and types of vegetation in nearby residential areas to see if stocking more garden supplies makes sense. If imagery can be updated quickly enough, retail companies could see how many cars are at a given location in order to estimate growth or decline. They could also plan new locations based on factors like accessibility or even locations that have lots of cars parked at their competitors’ stores.</p>

<p>In that vein, real estate is another potential application area. Much like for construction projects, real estate developers can improve planning their projects by being able to optimize residential appeal — whether by accessibility, proximity to natural spaces or avoiding high-risk zones. And the same goes for city and urban planners.</p>

<p>The advertising industry could leverage different types of data towards better ad targeting. Someone like Facebook could use satellite imagery to generate a wealth of data about a user’s specific location, that they can then provide as part of their user targeting suite for their customers. This could include the example above of measuring vegetation in residential areas to advertise garden supplies, or knowing proximity to mountains and trails to advertise hiking gear or mountain bikes.</p>

<p>As I’ll discuss a bit later, there’s also the potential that space data startups generate and sell intelligence directly to end customers, which could open up an even wider set of potential applications.</p>

<hr />

<h2 id="a-name-adoption-a-what-s-hindering-adoption"><a name="adoption"></a>What&rsquo;s hindering adoption?</h2>

<p>There isn’t necessarily one thing hindering adoption of geospatial imagery and intelligence. It’s a combination of availability, costs, latency, quality and usability. All these issues in conjunction means there’s a barrier for many commercial enterprises to using geospatial data to their advantage.</p>

<p>Getting satellites into orbit so there is more imagery available is step one. The goal of many of these imagery companies is to have a constellation of satellites in orbit to allow for daily imaging of the whole planet. Launching these satellites into orbit is currently expensive, and ups the cost of the end imagery (which thereby reduces the potential customer set). So, a lot depends on SpaceX’s (and others’) ability to cut down on the cost of satellite launches. The recent successful Falcon 9 launch and landing will very likely pave the way for rocket reuse, which will help bring down these costs substantially.</p>

<p><img src="/blog/img/rocketlaunch.gif" alt="Cinemagraph of the American flag waving while a rocket launches" /><em>Obligatory cinemagraph in the name of ‘Murica.</em></p>

<p>The delivery of imagery is historically quite slow as well. Not only do satellites capture a small part of the Earth at a time, but there’s also the issue of sending down large file sizes over transmissions speeds that are just in the hundreds of MB per second range. Assuming there’s no pre-processing before the customer receives the image, the customer still has to download the image for themselves, which takes time…and any processing work needed only adds to that time. This is starting to change, as images are increasingly available online and some images are pre-processed, saving customers from having to do the image processing themselves.</p>

<p>Of course, images will only realistically be “near real-time,” given the transmission delay. But getting down to a matter of minutes, or even hours, is an improvement over the traditional daily or longer wait times. Faster transmission speeds could help improve the speed at which images are received as well.</p>

<p>Launching a satellite into space is no cheap feat, not to mention costs of ongoing operations, resulting in imagery pricing that is quite expensive. Pricing can range from $20 to $25 per square km, and there are often minimum order sizes of 25 square km a pop (meaning $500+).</p>

<p>On the satellite design side, more development is needed in the miniaturization of components. For example, Planet Labs’ satellites are cutely described as “baguette”-size, and that’s the general trend — 172 satellites weighing 100kg (~220lbs) or less were launched in 2014. There are also sensor-related challenges, most which can’t be remediated at the source, putting more onus on the image processing part of the chain. There are multiple tradeoffs within sensors that affect quality: spectral resolution vs. signal to noise ratio (SNR), radiometric resolution vs. SNR, data size vs. spatial resolution, and spatial resolution vs. spectral resolution.</p>

<p>So, there’s a long way to go with image processing software as well, particularly as it pertains to information extraction. Better automation seems to be the path forward towards improving this software, though that isn’t particularly easy, either. It isn’t surprising that automation is perhaps the biggest area of focus among many of the startups in the field. The automation is primarily in the pre-processing (rectification and restoration phase), but also through easier integration (API all the things).</p>

<p>While I wasn’t able to find these claims specifically, after looking at a bunch of traditional GIS software, it has the GUI sophistication of Minesweeper from Windows 95. While I’m sure for users familiar with these interfaces it makes sense and works fine, I can’t help but imagine that a more intuitive and “typical user”-friendly UX might allow for more widespread adoption.</p>

<hr />

<h2 id="a-name-who-cares-a-who-cares"><a name="who-cares"></a>Who cares?</h2>

<p>The government has cared a lot for a long time, and I’d have to assume they’d be a little nervous about a bunch of new satellites being sent into orbit that may risk having spy satellites uncovered. But, they would also be able to benefit from innovations, particularly on the software-side, that are spurred by greater commercial adoption. Though based on how homely most government-facing software looks, maybe government analysts would disprove of UI improvements.</p>

<p><img src="/blog/img/satellite-12.jpg" alt="Satellite imagery via the CIA of Osama bin Ladin’s compound." /><em>Satellite imagery via the CIA of Osama bin Ladin’s compound.</em></p>

<p>Any of the commercial industries from earlier might care, as it can help them cut costs, curtail risks and arguably even improve revenues. So, they care to the extent that better satellite imagery and analysis can help them optimize their business, but the degree to which it does may vary. I’d imagine it’s a “nice to have,” maybe even “would love to have,” but not a “necessary to have” in most of these cases.</p>

<p>As described above, there are a lot of “save the world” use cases that could legitimately help improve the environment and even potentially human rights. But generally those budgets are much thinner than for-profit industries.</p>

<p>On the darker side of things, there’s the potential for invasion of privacy. This currently pertains to sub-orbit, but high altitude aircraft (as far as we, the unknowing public, knows), but it certainly isn’t a stretch to imagine being able to detect individuals by thermal spectrum within specific buildings. Or, to watch their patterns of life via satellite — though that could more easily be done by gaining access to their phone’s GPS and location data.</p>

<p>With the <a href="https://www.washingtonpost.com/news/the-switch/wp/2015/05/22/the-house-just-passed-a-bill-about-space-mining-the-future-is-here/">recent bill passed to allow companies to retain profits from space mining activities</a>, improvements in these technologies could potentially help these companies scout asteroids and other celestial objects containing valuable elements. It might be tricky from the satellite positioning perspective, but would cut down on the exploration costs enormously if companies could make “sure bets.”</p>

<hr />

<h2 id="a-name-risks-a-what-are-the-risks"><a name="risks"></a>What are the risks?</h2>

<p><img src="/blog/img/satellite-13.jpg" alt="How a satellite constellation looks" /><em>How a satellite constellation looks.</em>
A lot depends on getting satellites into orbit, at least to make this a huge opportunity. The successful Falcon 9 launch and re-landing helps mitigate those risks a bit, but that happened only weeks ago. So, to get more satellites into orbit, thus increasing not only the amount of imagery, but quality of imagery, you have to hope that SpaceX really has their stuff together and in a hurry. You actually probably need to hope that more than just SpaceX does rocket reuse successfully.</p>

<p>Satellite imagery, at least as it stands today, also isn’t that big of an industry. The satellite industry as a whole is a hefty market ($200 billion), particularly because of consumer communications and entertainment. But right now the Earth Observation (EO) market, which includes the satellite imagery portion, is still quite small.</p>

<p>Specifically, the EO market size is just about $2 billion today, which doesn’t leave a lot of room for new players to make a killing. DigitalGlobe and Esri, arguably the largest satellite imagery providers, only made about $650mm and $950mm in revenue in 2014, respectively. Some of the estimates, like from Northern Sky research, put the EO market hitting $3.5 billion in 2020, and $4.5 billion by 2024.</p>

<p>An alternative is betting that even if the imagery part doesn’t grow that quickly, better software and analytics still has the opportunity for significant growth. After all, these tools would help companies get a better bang for their buck when purchasing satellite imagery. But is it a 10x better bang for the buck than it stands today? That’s up for debate, and largely depends on use case. But that’s not the sort of “sure bet” most VCs like.</p>

<p>On the other hand, if the monetization of satellite imagery isn’t via the imagery or software itself, but via the resulting data streams, then there’s arguably less risk. If you’re just selling what would essentially be business intelligence, but collected from Earth’s orbit, you’d undoubtedly find additional interested customers due to the more immediate value proposition. However, companies pursuing this would probably have to control the whole chain — satellites, imagery, processing, etc. — to have differentiated and high-quality data streams, which requires a ton of capital to pursue. So VCs would need to clutch their talismans and hope the all-in bet pays off.</p>

<hr />

<h2 id="a-name-current-scene-a-what-s-the-current-scene"><a name="current-scene"></a>What&rsquo;s the current scene?</h2>

<p>There are not too many startups specifically in the satellite-imagery arena, though there are a few more in the satellite and space category more generally (most notably SpaceX). The ones who are in what I’d call the “geospatial big data” arena are:</p>

<ul>
<li>Analyze</li>
<li>Aquila Space</li>
<li>BlackSky Global</li>
<li>CartoDB</li>
<li>Descartes Labs</li>
<li>Iceye</li>
<li>MapBox</li>
<li>Planet Labs</li>
<li>Orbital Insight</li>
<li>Skybox Imaging (acquired by Google)</li>
<li>Spire</li>
<li>TellusLabs</li>
<li>UrtheCast</li>
</ul>

<p>There are some sub-categories, like tracking weather and maritime conditions (Analyze, Spire), or mapping services (CartoDB, Mapbox). But for the most part there isn’t much overlap between the companies, other than at the highest level. You’ll see terms like “tracking,” “data streams,” and so forth, but they all self-describe quite differently.</p>

<p>The more notable VC funds that have funded some of these ventures are:</p>

<ul>
<li>Accel Partners</li>
<li>Draper Fisher Jurvetson</li>
<li>Earlybird Venture Capital</li>
<li>Felicis Ventures</li>
<li>Founders Fund</li>
<li>Foundry Group</li>
<li>Lux Capital</li>
<li>Promus Ventures</li>
<li>Razors Edge Ventures</li>
<li>Rothenberg Ventures</li>
<li>RRE Ventures</li>
</ul>

<p>There are also a few larger companies that do provide either satellite imagery, GIS software, or geospatial database management systems, including:</p>

<ul>
<li>Autodesk</li>
<li>Bentley Systems</li>
<li>DigitalEye</li>
<li>Esri</li>
<li>Exelis</li>
<li>Hexagon Geospatial</li>
<li>Teradata</li>
</ul>

<p>There are also a number of open source projects, from software to SDKs and libraries, that are released by non-profit organizations and universities. But they rarely have the same breadth of features, nor the number of capabilities, as the paid software.</p>

<hr />

<h2 id="a-name-conclusion-a-conclusion"><a name="conclusion"></a>Conclusion</h2>

<p><img src="/blog/img/satellite-14.jpg" alt="Satellite looking over Earth at night" />
There’s a reason why I like using the term “space data” — this is really cool stuff. But, there are huge capital costs involved for a market that as of yet isn’t very big at all. Or, for companies that are improving just the software part, there’s a lot of reliance on third parties to provide the actual imagery.</p>

<p>Automation does seem like the most legitimate opportunity for a 10x improvement on what is available today, so that companies don’t need GIS experts in-house to still glean intelligence from satellite imagery. It seems like this software vertical is particularly behind in many of the infrastructure developments made in the past decade, so there’s certainly room for disruption just in that regard.</p>

<p>But, what are companies’ ongoing needs for satellite imagery? Many of the applicable industries suggest a per-project need rather than the sort of continuous need best met via SaaS. At the very least, the government is likely willing to throw some money towards better software, but relying on that revenue is unlikely to produce a blockbuster VC return.</p>

<p>The most viable proposition in my eyes is in eliminating the need for companies to have to even touch satellite imagery and give them the information they need to know, i.e. the data stream approach. It feels like a truly modern way of approaching business and operational intelligence with a large potential audience. And hedge funds would probably eat it up.</p>

<p>My main hesitation here would be in vertical-specific needs, and to a lesser extent, in pricing. My gut feeling is that, at least in early days, the data received by customers would require a heavy level of customization based on their needs, making the business almost like a software and data-enabled consultancy (which is arguably working out for Palantir). And as a result, the pricing might still be prohibitive to many customers — not to mention the initial and ongoing costs of maintaining a constellation of satellites.</p>

<p>My prediction is that many of the software-only companies will remain quite small, while those pursuing the entire chain (like Planet Labs) have a good shot at a big long-term payoff (with bigger capital requirements, of course). DigitalGlobe itself only has a $1 billion market cap, with about $100mm cash, so they can’t just gobble up the new software companies. There’s always the chance a cash-rich tech giant like IBM or Facebook decides they’re interested in the space data game, too. Or perhaps I’m wrong and space mining comes sooner rather than later, with space data crucial for any level of success.</p>

<p><img src="/blog/img/satellite-15.jpg" alt="Rendering of asteroid mining" /><em>Asteroid mining — the not so distant future?</em></p>
]]></content>
        </item>
        
        <item>
            <title>WTFunding: Industrial / Manufacturing Analytics</title>
            <link>//swagitda.com/blog/posts/wtfunding-industrial-manufacturing-analytics/</link>
            <pubDate>Tue, 13 Oct 2015 17:24:07 -0400</pubDate>
            
            <guid>//swagitda.com/blog/posts/wtfunding-industrial-manufacturing-analytics/</guid>
            <description>WTFunding is one of my “spare time” projects to delve into tech sectors attracting VC funding that pique my curiosity. I like connecting dots between disparate things, it’s also pretty useful.
Table of Contents:  So what is &amp;ldquo;industrial / manufacturing analytics&amp;rdquo;? What are the applications? What&amp;rsquo;s hindering adoption? Who cares? What are the risks? What&amp;rsquo;s the current scene? Final thoughts  So what is “industrial / manufacturing analytics”? There doesn’t seem to be a great catch-all term for it yet, but there are a few different terms for and related to the sector I’m discussing: smart manufacturing, infrastructure analytics, manufacturing analytics, industrial analytics and, though it’s a broader term, IoT analytics.</description>
            <content type="html"><![CDATA[

<p><img src="/blog/img/sparks.jpg" alt="Image of Sparks" /></p>

<p><em>WTFunding is one of my “spare time” projects to delve into tech sectors attracting VC funding that pique my curiosity. I like connecting dots between disparate things, it’s also pretty useful.</em></p>

<h3 id="table-of-contents">Table of Contents:</h3>

<ol>
<li><a href="#so-what-is">So what is &ldquo;industrial / manufacturing analytics&rdquo;?</a></li>
<li><a href="#applications">What are the applications?</a></li>
<li><a href="#adoption">What&rsquo;s hindering adoption?</a></li>
<li><a href="#who-cares">Who cares?</a></li>
<li><a href="#risks">What are the risks?</a></li>
<li><a href="#current-scene">What&rsquo;s the current scene?</a></li>
<li><a href="#conclusion">Final thoughts</a></li>
</ol>

<hr />

<h2 id="a-name-so-what-is-a-so-what-is-industrial-manufacturing-analytics"><a name="so-what-is"></a>So what is “industrial / manufacturing analytics”?</h2>

<p>There doesn’t seem to be a great catch-all term for it yet, but there are a few different terms for and related to the sector I’m discussing: smart manufacturing, infrastructure analytics, manufacturing analytics, industrial analytics and, though it’s a broader term, IoT analytics.</p>

<p>My preference is towards “infrastructure analytics,” but when I use those terms interchangeably throughout, know I mean the same thing (and it’s also helpful to keep an eye out for that range of terms in your own reading). The one sub-sector I’ve seen broken out the most is predictive maintenance, but otherwise startups in this space use some selection of the terms above when self-describing.</p>

<p>Gartner’s definition says, “providers that help manufacturers support variable product content through the manufacturing process and improve the visibility and analysis of manufacturing performance.” Other research creates some beautiful imagery like, “removing the barrier between physical and information flows,” and, “translating the physical world into a model accessible by IT.” Or that, you know, this is all about “IoT orchestration” and “big-data-driven manufacturing.”</p>

<p>My <a href="https://www.urbandictionary.com/define.php?term=ELI5">ELI5</a> definition, that makes me want to stab my eyes out less, is, “providers that help manufacturers manufacture better, and for less money, using data from machines.” They listen to all the stuff the manufacturers’ machines have to say, figure out what part of the stuff shows that things are going wrong or things could be done better, and show the manufacturers pretty pictures and clear advice on how to fix machines or make the machines do things better.</p>

<p>Industry people refer to this data as generated “on the edge,” which just means at the sensor or machine level. As I’ll talk about farther down, there are lots of machines that provide data for solutions to leverage. To visualize the different types of machines, here are some pictures along with what they are:</p>

<table>
<thead>
<tr>
<th align="center"><img src="/blog/img/industrial-01.png" alt="machine vision systems" /></th>
<th align="center"><img src="/blog/img/industrial-02.png" alt="RFID/barcode scanners" /></th>
<th align="center"><img src="/blog/img/industrial-03.png" alt="welding machines" /></th>
</tr>
</thead>

<tbody>
<tr>
<td align="center">Machine vision systems</td>
<td align="center">RFID/barcode scanners</td>
<td align="center">Welding machines</td>
</tr>
</tbody>
</table>

<p>*</p>

<table>
<thead>
<tr>
<th align="center"><img src="/blog/img/industrial-04.jpeg" alt="PLCs" /></th>
<th align="center"><img src="/blog/img/industrial-05.jpeg" alt="Robots" /></th>
<th align="center"><img src="/blog/img/industrial-06.jpg" alt="Plasma cutters" /></th>
</tr>
</thead>

<tbody>
<tr>
<td align="center">PLCs</td>
<td align="center">Robots</td>
<td align="center">Plasma cutters</td>
</tr>
</tbody>
</table>

<p>Oh, and don’t forget “factory-floor software” like MES (manufacturing execution system) and ERP (enterprise resource planning).</p>

<p>Let’s walk through an example manufacturing process to get a sense of what data is generated; for example, putting together the chassis (the car’s frame) on an automobile.</p>

<p><img style="float: right; max-width:50%; padding: 5px" src="/blog/img/industrial-07.png"></p>

<p>Robots will put together the chassis by welding different parts together. Then, the robots will typically put in the engine, transmission and suspension. Building the rest of the body operates in much the same way — putting in parts and welding them together (some human interaction is still involved).</p>

<p>Tracking temperature abnormalities can help cut down on defects and, once enough data is analyzed, potentially speed up production time as well. But, what is arguably even more valuable is tracking the robots themselves. The data generated from their operations allows better forecasting of production time, maximizing uptime, performing predictive maintenance, spotting quality issues and generating insights into how processes might be improved.</p>

<p>So, this software mines information across the manufacturing floor from all those machines, sensors, devices, etc. But how is that done?</p>

<p>The data is typically either collected via a gateway device that’s located on the factory floor or via a local node that transfers the collected data to a central gateway device. Then, the data needs to be made usable, or “transformed” (as is true for traditional data analytics), so there needs to be cleansing, normalizing and organizing.</p>

<p>This transformed data is then analyzed, either on-prem or cloud-based, using industry-specific data models. These include those tailored towards discrete, batch and process manufacturing. Since VC’s ❤ KPI’s, some of the common ones from these analytics are performance, uptime, quality, cycle time, OEE (overall equipment effectiveness), along with reasons for defects and downtimes.</p>

<p>The results of this analysis and the KPI’s are then shown to the customer that is (ideally) highly understandable and conducive to informing decision-making and action. Which brings us to what sorts of decisions and actions can be informed by this analysis.</p>

<hr />

<h2 id="a-name-applications-a-what-are-the-applications"><a name="applications"></a>What are the applications?</h2>

<p>I’m not the first to suggest that there’s a big resource allocation problem across a number of industries due to prioritization of “collect all the things” (the NSA approach) rather than finding the “why” first and narrowing down what needs to be collected from there. Perhaps as a result of the jubilant headlines on how revolutionary big data is, it often seems that data collection serves primarily as a signal for organizations to show that they’re doing something about big data.</p>

<p>It’s a similar conundrum to my company’s industry, information security, in which most organizations care foremost about showing they care about security, and vendors are happy to be providing something that sounds really cool but doesn’t solve anything real. But companies showing that they care about security ends up being expensive not just financially, but also time, resource and user experience-wise (something my company is attempting to change) — and the same is largely true for companies attempting to “do something” with the big data they’ve now spent a lot of money to collect and store.</p>

<p>Going into researching this smart manufacturing area, I assumed there would exist roughly the same narrative. Tons of data is given off by sensors, which is being collected and now companies are searching for applications to show that their investments into collecting and storing all this data have not been fruitless.</p>

<p>However, to a large extent this is not the case. As I’ll get to in the next section, a big problem is actually in the data collection itself. And companies have some pretty crystalline use-cases in mind that would materially — perhaps even disruptively — enhance their operations, and the challenge is how to get the right data to do so in a non-cost-prohibitive manner.</p>

<p>At the highest level, the primary application is “optimizing operations.” For a tech startup, that can mean reducing downtime, maximizing processing speed, and so forth. For a manufacturing floor, the implications are enormous. I had the pleasure of seeing a manufacturing floor up-close when I was very young, and it’s one of the clearest of my early memories. But for those who may not have before, here are a few pictures as a guide (in addition to all the lovely machines from earlier):</p>

<table>
<thead>
<tr>
<th align="center"><img src="/blog/img/industrial-08.png" alt="PLCs" /></th>
<th align="center"><img src="/blog/img/industrial-09.png" alt="Robots" /></th>
<th align="center"><img src="/blog/img/industrial-10.png" alt="Plasma cutters" /></th>
</tr>
</thead>

<tbody>
</tbody>
</table>

<p>As you can see, there are lots of machines, and different types of them. This means that there’s a lot of opportunity for malfunctioning, misbehaving, slowing down, speeding up and so forth. Identifying and fixing these issues costs money, and before they’re fixed, there are also potential losses from downtime and quality issues as well.</p>

<p>As a result, there is, in fact, a long list of potential applications of being able to analyze data from these machines, including, but not limited to (and with some overlap):</p>

<ul>
<li>Improving quality</li>
<li>Identifying issues in real-time</li>
<li>Minimizing operational variability</li>
<li>Reducing waste of raw materials</li>
<li>Reducing unplanned downtime</li>
<li>Reducing scrap rates</li>
<li>Identifying opportunities for process improvements</li>
<li>Increasing production speed</li>
<li>Predicting supply needs</li>
<li>Synthesizing visibility of operations across plants</li>
<li>Reducing compliance costs</li>
<li>Improving auditability</li>
</ul>

<p>The impact of these applications is huge in an industry with slim margins. Being able to better predict, streamline and “smooth out” operations results in significant savings at scale.</p>

<hr />

<h2 id="a-name-adoption-a-what-s-hindering-adoption"><a name="adoption"></a>What&rsquo;s hindering adoption?</h2>

<p><img src="/blog/img/industrial-11.jpeg" alt="Image of a man watching a car fly through a screen" /><em>Maybe one day humans can just think of a bunch of dots and have it turn into a sportscar that shoots out of random screens, disrupting manufacturing entirely.</em></p>

<p>As foreshadowed above, one of the largest barriers to adoption of these technologies is in data collection. Though it can be treated as a distinct part of the big data chain, the “data cleanup” aspect is extremely challenging, as it is difficult to combine and centralize sensor data emanating from disparate machines and devices.</p>

<p>And, that’s not to mention the sheer amount of data causes some hefty storage requirements, and the pace at which data is generated means that solutions have to be able to keep up. There are also challenges like needing to make sure sensors and communications hardware can survive potentially rough environments, but it seems like people know more or less how to accomplish that.</p>

<p>The most-repeated challenge in realizing the “Fourth Industrial Revolution” is that there needs to be a standard data architecture across vendors. Lacking a global government to decree all vendors must adhere to a certain data architecture or perish, most of the people who cite this challenge have suggestions to ameliorate it along the lines of ¯\_(ツ)_/¯.</p>

<p>As a result, current capabilities are pretty okay at helping solve basic problems but not great at solving tricky, complex and big problems. Most of the time, being able to solve those hard problems comes with a big price tag and requires a lot of human input.</p>

<p>When you visualize the manufacturing floor, it becomes super easy to understand why this is the state of the world. Ignoring the fact that there are lots of different kinds of machines involved in any particular manufacturing process, individual machines themselves will have different types of sensors recording tons of data points on different things involved in its operation. Considering then that an individual machine will have thousands of data points in a single operation, an entire manufacturing floor will have millions — so just imagine the sheer scale of how much data is generated from ongoing operations.</p>

<p align="center"><img src="/blog/img/industrial-12.jpg"/ alt="Spock sobbing mathematically"></p>

<p>It’s a frustrating realization, and if you’re like me, then your mind begins to furtively think of how to fix it. Starting at a small scale, theoretically vendors could ensure that all sensor data generated from a single machine adhered to the same data architecture. Let’s be bold and optimistic and even say that a vendor could ensure that all their different machines adhered to the same data architecture, so if you had a floor with only their equipment, you could collect and combine all this juicy data with ease.</p>

<p>But in the real world, making sure all the sensor data has the same architecture is decidedly not trivial. What incentives does the vendor have to implement this extra process when designing their products? Do customers really care so much about the data that they’re willing to pay a premium for machines with a common data architecture?</p>

<p>This results in first-mover disadvantages both on the vendor and customer side. For vendors, they don’t want to be the dopes that were first to put all this effort into having this gorgeous, seamlessly combinable spring of sensor data from their machines that customers end up not buying because it’s more expensive than that of their Tower of Babel-esque competitors. For customers, they don’t want to be the dopes that were first to pay a premium for this theoretically awesome tech that turns out not to generate such actionable insights after all, or worse, isn’t as easily collected and synthesized as promised. Not to mention the customers will still need to buy or develop the analytics that leverage this data.</p>

<p>It’s going to take a risky bet on one side or the other. My personal bet is on the customer side, which will (hopefully) put pressure on vendors. Why? Because the customers (the manufacturers) may be able to sell some of the insights they gain to their own customers as part of a value-added offering. For example, they can differentiate from their competition by suggesting data-supported ways for their customers to improve the quality of their products or speed at which their products can get to market. While helping bottom-line by reducing their downtime, use of raw materials, etc. is all well and good, there’s nothing like a boost to the top-line to really cultivate interest.</p>

<p>This still doesn’t solve the problem of how standardization will actually be accomplished. It doesn’t do a ton of good if each vendor standardizes within itself but makes it even harder for manufacturers to combine data from machines across different vendors. Very few vendors will make all of the machines and devices manufacturers need to run their operations, so it is realistic to assume that broader standardization is essential.</p>

<p>There is some evidence of the government looking to help facilitate standardization, but I, for one, have little faith in how quickly any initiative might be accomplished in that manner. Realistically, the GE’s and other titan industrial firms will throw a lot of money at figuring this out and monetize it as something like IoTAaaS, but those will likely be in areas away from the manufacturing part of “industrial” and instead in use-cases where their machine or part can serve as a strong sole-source of data (e.g. sensors on jet engines to help optimize fuel efficiency).</p>

<p>What I’d love to see is an approach similar to what the founders of Flatiron Health performed in its earliest days. The lore is they traveled around the country speaking with cancer specialists and treatment centers to better understand the challenges they faced, what sort of data was relevant, how it was currently being shared, etc. From gathering those insights first-hand, they developed a solution that has been very sincerely described as “fighting cancer with big data.” There are meaty enough sub-sectors within the industrial space that I strongly believe something similar could be accomplished and help leap over the standardization hurdle to the phase where meaningful insights can be generated with ease.</p>

<p>But, I’m only half joking when I think that maybe this whole problem will be solved if Google open sources whatever software it develops as it builds its self-driving cars and/or AI-driven robot and drone army, <em>ahem</em>, logistics network, then executes its Order 66 to have the global leaders it’s funded and seeded mandate Google’s way as the standard.</p>

<hr />

<h2 id="a-name-who-cares-a-who-cares"><a name="who-cares"></a>Who cares?</h2>

<p>Luckily from a “who cares” perspective, the field of potential customers is actually enormous. Manufacturing spans behemoth industries, including pharmaceuticals, transportation, aerospace and defense, oil and gas, electronics and chemicals. And within each industry is a chunky supply chain full of many vendors, from raw materials to packaging. Being the go-to operational analytics solution provider for any one of these industries alone would very likely result in a hefty amount of revenue.</p>

<p>From the VC perspective, it’s pretty obvious why there’d be interest in investing in this area, particularly if you’re an investor for a corporate VC arm who happens to be a part of the manufacturing or industrial supply chain. It also heavily touches on IoT, which is an area still attracting a considerable amount of interest, but has crossover appeal to enterprise software investors who are familiar with more general analytics and business intelligence companies.</p>

<hr />

<h2 id="a-name-risks-a-what-are-the-risks"><a name="risks"></a>What are the risks?</h2>

<p><img src="/blog/img/more-sparks.jpg" alt="more sparks" /><em>Our lizard brains are programmed to like pictures of sparks.</em></p>

<p>Being early to the party is no fun, and even though the “Fourth Industrial Revolution” has had its cue music on for a while, it’s still extremely early and somewhat speculative. Nanotechnology is an example industry that’s had an excessively long drumroll, to the point that now legitimately exciting advances are met by many with indifference. And many of the early “plays” in it never took off or lived up to the hype.</p>

<p>To generalize, VC’s like taking “safe risks.” What this means is that they typically like to fund new companies in markets somewhere between totally unproven and super saturated. So where does the infrastructure analytics market fall?</p>

<p>On the one hand, it’s sort of a “no duh” that companies want this operational intelligence, for all the potential applications described earlier. On the other, will these companies buy these sorts of from startups? And can these startups prove that their solutions provide disruptive-level value early enough? Those are questions that are true for a lot of enterprise solution providers, but I feel are magnified given the higher-stakes of the physical realm.</p>

<p>There’s also the uncertainty of standardization. Can companies actually extract the value they promise in a world with that many disparate types of data? There’s potentially the question of whether their value is eroded a bit if standardization happens, and thus it becomes substantially easier to use existing data science methods to garner operational intelligence, but I personally think that’s a lesser risk (and definitely cart before horse).</p>

<p>As in my Flatiron Health analogy earlier, a startup that cataloged all the different machines and things involved in the processes of a specific sub-sector in industrial analytics might have the potential to do exceptionally well. That level of granularity may be necessary to truly crack the code on how to most efficiently and meaningfully collect, combine and analyze this data. But it would involve a longer time horizon and doing Paul Graham’s “things that don’t scale,” both which add execution risk that might be insurmountable in an investors’ eyes.</p>

<hr />

<h2 id="a-name-current-scene-a-what-s-the-current-scene"><a name="current-scene"></a>What&rsquo;s the current scene?</h2>

<p>There are a handful of startups that are already operating in this arena (and I have no doubt I’m missing some and quite a few are in stealth). They self-describe in roughly four buckets:</p>

<ul>
<li><strong>Manufacturing Analytics/Intelligence</strong> — Hai, Northwest Analytics, OFS Systems, Optimal+, SightMachine</li>
<li><strong>Predictive Maintenance</strong> — Augury, DecisionIQ, Maintenel, Predikto</li>
<li><strong>Industrial Analytics</strong> — Seeq</li>
<li><strong>IoT Analytics</strong> — Decisyon, mnubo</li>
</ul>

<p>Some of the more notable VC funds that have invested in these companies include:</p>

<ul>
<li>First Round</li>
<li>Formation 8</li>
<li>IA Ventures</li>
<li>Lerer Hippeau</li>
<li>Madrona Venture Group</li>
<li>O’Reilly Alpha Tech</li>
<li>Orfin Ventures</li>
<li>…and KKR</li>
</ul>

<p>There are a number of industrial conglomerates that have created data science wings, realizing the potential of the space, such as the list below. They tend to self describe as “industrial analytics” or “process analytics.”</p>

<ul>
<li>ABB</li>
<li>Dionex</li>
<li>GE (Predix)</li>
<li>Genpact</li>
<li>Honeywell</li>
<li>Siemens</li>
</ul>

<p>Along with analytics and consulting incumbents spreading out into industry-specific solutions, including the list below. In contrast to the industrial conglomerates above, they come up if you search for “manufacturing analytics.”</p>

<ul>
<li>Ayasdi</li>
<li>CSC</li>
<li>Datawatch</li>
<li>Deloitte</li>
<li>Oracle</li>
<li>ParStream</li>
<li>Predixion</li>
<li>Tableau</li>
<li>TIBCO</li>
<li>Wipro</li>
</ul>

<hr />

<h2 id="a-name-conclusion-a-final-thoughts"><a name="conclusion"></a>Final Thoughts</h2>

<p>I think the issue with the industrial analytics sector isn’t level of VC interest, but instead that the pipeline of companies is really small; so even if investors wanted to take a gamble, there hasn’t been much opportunity to do so. After all, the “right” founders will have come from working in the industrial sector, and if they have the level of data science expertise required to create a novel startup in this space, I’d suspect they’re really well paid (especially if in oil and gas). More typical SV data science founder-types may be apprehensive of tackling this problem space as well given the domain expertise required, which seems to be tricky to acquire without access to industrial equipment.</p>

<p>My prediction is we’ll see some of the incumbent analytics players creating industry-specific modules at first along with the big industrial companies creating analytics solutions. And from there, I hope (and I suspect VC’s hope as well) there are defectors that take the risk to create a pure-play company that takes the big leap forward rather than incremental shuffles.</p>
]]></content>
        </item>
        
    </channel>
</rss>
